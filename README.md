# ğŸ”æˆ‘æƒ³çœ‹çœ‹ Java åº“æºç 

æœ¬æ–‡æ¡£ä»¥ **å¤§å­¦è®¡ç®—æœºæœ¬ç§‘è¯¾ç¨‹** ä¸ºå‡ºå‘ç‚¹ï¼Œä¸€çª¥ Java åº“ä»£ç çš„å®ç°ï¼Œå°† **ç†è®ºä¸å¼€å‘å®è·µ** è¿›è¡Œç´§å¯†ç»“åˆã€‚æ–‡æ¡£ä¸­æºä»£ç æºäº **JDK-19**ã€‚

å¦å¤–ï¼Œæœ¬æ–‡æ¡£æ¶‰åŠ JVM çš„ä¸€äº›åº•å±‚å®ç°ï¼Œå¹¶å°½é‡åŸºäºæºç è¿½è¸ª (åŸºäºOpenJDK C++ æºç )çš„æ–¹å¼å»æŒ–æ˜å®ç°åŸç†ã€‚

â€‹																			â€”â€”**ç‹‚åˆ„æ˜Ÿç©º**

### ğŸ–Šçº¿æ€§è¡¨

#### ğŸ“åŠ¨æ€æ•°ç»„

##### æ’å…¥

å¯ä»¥çœ‹å‡ºï¼Œåœ¨ä¸­é—´æ’å…¥æ¶‰åŠåˆ°æ•°ç»„çš„æ‹·è´ç§»åŠ¨ã€‚å…¶ä¸­æ¶‰åŠçš„

> System.arraycopy

æ˜¯ä¸€ä¸ª native å‡½æ•°ï¼Œç”¨äºå°†æ•°ç»„çš„æŸä¸€ç´¢å¼•ä½ç½®å¼€å§‹çš„å…ƒç´ ç§»åŠ¨åˆ°å¦ä¸€ä¸ªå¼€å§‹ç´¢å¼•ã€‚

```java
/**
 * Inserts the specified element at the specified position in this
 * list. Shifts the element currently at that position (if any) and
 * any subsequent elements to the right (adds one to their indices).
 *
 * @param index index at which the specified element is to be inserted
 * @param element element to be inserted
 * @throws IndexOutOfBoundsException {@inheritDoc}
 */
public void add(int index, E element) {
    rangeCheckForAdd(index);
    modCount++;
    final int s;
    Object[] elementData;
    if ((s = size) == (elementData = this.elementData).length)
        elementData = grow();
    System.arraycopy(elementData, index,
                     elementData, index + 1,
                     s - index);
    elementData[index] = element;
    size = s + 1;
}
```

æ‰¹é‡æ’å…¥ï¼Œæ‰¹é‡æ’å…¥å¯ä»¥ä¸€æ¬¡æ€§å®Œæˆæ•°ç»„çš„æ‰©å®¹ä¸å…ƒç´ ç§»åŠ¨ï¼Œæ›´åŠ é«˜æ•ˆ

```java
/**
 * Appends all of the elements in the specified collection to the end of
 * this list, in the order that they are returned by the
 * specified collection's Iterator.  The behavior of this operation is
 * undefined if the specified collection is modified while the operation
 * is in progress.  (This implies that the behavior of this call is
 * undefined if the specified collection is this list, and this
 * list is nonempty.)
 *
 * @param c collection containing elements to be added to this list
 * @return {@code true} if this list changed as a result of the call
 * @throws NullPointerException if the specified collection is null
 */
public boolean addAll(Collection<? extends E> c) {
    Object[] a = c.toArray();
    modCount++;
    int numNew = a.length;
    if (numNew == 0)
        return false;
    Object[] elementData;
    final int s;
    if (numNew > (elementData = this.elementData).length - (s = size))
        elementData = grow(s + numNew);
    System.arraycopy(a, 0, elementData, s, numNew);
    size = s + numNew;
    return true;
}
```



##### åˆ é™¤

ä»ä¸‹åˆ—ä»£ç å¯ä»¥çœ‹å‡ºï¼Œåˆ é™¤ä¹Ÿæ¶‰åŠåˆ°æ•°ç»„å…ƒç´ çš„ç§»åŠ¨æ“ä½œã€‚

```java
/**
 * Removes the element at the specified position in this list.
 * Shifts any subsequent elements to the left (subtracts one from their
 * indices).
 *
 * @param index the index of the element to be removed
 * @return the element that was removed from the list
 * @throws IndexOutOfBoundsException {@inheritDoc}
 */
public E remove(int index) {
    Objects.checkIndex(index, size);
    final Object[] es = elementData;

    @SuppressWarnings("unchecked") E oldValue = (E) es[index];
    fastRemove(es, index);

    return oldValue;
}

/**
 * Private remove method that skips bounds checking and does not
 * return the value removed.
 */
private void fastRemove(Object[] es, int i) {
    modCount++;
    final int newSize;
    if ((newSize = size - 1) > i)
        System.arraycopy(es, i + 1, es, i, newSize - i);
    es[size = newSize] = null;
}
```

æ¸…ç©º

```java
public void clear() {
    modCount++;
    final Object[] es = elementData;
    for (int to = size, i = size = 0; i < to; i++)
        es[i] = null;
}
```



##### æŸ¥æ‰¾

```java
/**
 * Returns the element at the specified position in this list.
 *
 * @param  index index of the element to return
 * @return the element at the specified position in this list
 * @throws IndexOutOfBoundsException {@inheritDoc}
 */
public E get(int index) {
    Objects.checkIndex(index, size);
    return elementData(index);
}
```

##### ä¿®æ”¹

```java
/**
 * Replaces the element at the specified position in this list with
 * the specified element.
 *
 * @param index index of the element to replace
 * @param element element to be stored at the specified position
 * @return the element previously at the specified position
 * @throws IndexOutOfBoundsException {@inheritDoc}
 */
public E set(int index, E element) {
    Objects.checkIndex(index, size);
    E oldValue = elementData(index);
    elementData[index] = element;
    return oldValue;
}
```



##### æ‰©å®¹

è‹¥ä¸æŒ‡å®šï¼Œæ•°ç»„é»˜è®¤å®¹é‡ä¸º **10**

```java
public class ArrayList<E> extends AbstractList<E>
        implements List<E>, RandomAccess, Cloneable, java.io.Serializable
{
    @java.io.Serial
    private static final long serialVersionUID = 8683452581122892189L; // ç”¨äºåºåˆ—åŒ–

    /**
     * Default initial capacity.
     */
    private static final int DEFAULT_CAPACITY = 10;

    /**
     * Shared empty array instance used for empty instances.
     */
    private static final Object[] EMPTY_ELEMENTDATA = {};

    /**
     * Shared empty array instance used for default sized empty instances. We
     * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when
     * first element is added.
     */
    private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};

    /**
     * The array buffer into which the elements of the ArrayList are stored.
     * The capacity of the ArrayList is the length of this array buffer. Any
     * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA
     * will be expanded to DEFAULT_CAPACITY when the first element is added.
     */
    // ä½¿ç”¨ transient å…³é”®å­—ï¼Œä»¥å¿½ç•¥åºåˆ—åŒ–
    transient Object[] elementData; // non-private to simplify nested class access

    /**
     * The size of the ArrayList (the number of elements it contains).
     *
     * @serial
     */
    private int size;
```

é»˜è®¤æ‰©å®¹ä¸ºåŸæ¥çš„2å€ (æ‹·è´åˆ°ä¸€ä¸ª **æ–°æ•°ç»„** )

```java
/**
 * Increases the capacity to ensure that it can hold at least the
 * number of elements specified by the minimum capacity argument.
 *
 * @param minCapacity the desired minimum capacity
 * @throws OutOfMemoryError if minCapacity is less than zero
 */
private Object[] grow(int minCapacity) {
    int oldCapacity = elementData.length;
    if (oldCapacity > 0 || elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
        int newCapacity = ArraysSupport.newLength(oldCapacity,
                minCapacity - oldCapacity, /* minimum growth */
                oldCapacity >> 1           /* preferred growth */);
        return elementData = Arrays.copyOf(elementData, newCapacity);
    } else {
        return elementData = new Object[Math.max(DEFAULT_CAPACITY, minCapacity)];
    }
}
```



##### è£å‰ª

```java
/**
 * Trims the capacity of this {@code ArrayList} instance to be the
 * list's current size.  An application can use this operation to minimize
 * the storage of an {@code ArrayList} instance.
 */
public void trimToSize() {
    modCount++; // ç»Ÿè®¡æ­¤åˆ—è¡¨åœ¨ç»“æ„ä¸Šè¢«ä¿®æ”¹çš„æ¬¡æ•°
    if (size < elementData.length) {
        elementData = (size == 0)
          ? EMPTY_ELEMENTDATA
          : Arrays.copyOf(elementData, size);
    }
}
```

#### ğŸ“çº¿ç¨‹å®‰å…¨çš„åŠ¨æ€æ•°ç»„ â€”â€” Vector

ä¸»è¦ä½¿ç”¨ **synchronized** å…³é”®å­—ä¿è¯çº¿ç¨‹å®‰å…¨

```java
public synchronized void insertElementAt(E obj, int index) {
    if (index > elementCount) {
        throw new ArrayIndexOutOfBoundsException(index
                                                 \+ " > " + elementCount);
    }
    modCount++;
    final int s = elementCount;
    Object[] elementData = this.elementData;
    if (s == elementData.length)
        elementData = grow();
    System.*arraycopy*(elementData, index,
                     elementData, index + 1,
                     s - index);
    elementData[index] = obj;
    elementCount = s + 1;
}

public synchronized boolean removeElement(Object obj) {
    modCount++;
    int i = indexOf(obj);
    if (i >= 0) {
        removeElementAt(i);
        return true;
    }
    return false;
}

public synchronized E get(int index) {
    if (index >= elementCount)
        throw new ArrayIndexOutOfBoundsException(index);

    return elementData(index);
}

public synchronized E set(int index, E element) {
    if (index >= elementCount)
        throw new ArrayIndexOutOfBoundsException(index);

    E oldValue = elementData(index);
    elementData[index] = element;
    return oldValue;
}
```

#### ğŸ“åŸºäº Vector çš„ Stack

```java
public class Stack<E> extends Vector<E>
```

#### ğŸ“åŒå‘é“¾è¡¨

##### ç»“ç‚¹å°è£…

```java
public class LinkedList<E>
    extends AbstractSequentialList<E>
    implements List<E>, Deque<E>, Cloneable, java.io.Serializable
{
    transient int size = 0;

    /**
     * Pointer to first node.
     */
    transient Node<E> first;

    /**
     * Pointer to last node.
     */
    transient Node<E> last;
    
    //......
}

private static class Node<E> {
    E item;
    Node<E> next;
    Node<E> prev;

	Node(Node<E> prev, E element, Node<E> next) {
	    this.item = element;
	    this.next = next;
	    this.prev = prev;
	}
}

/**
 * Returns the (non-null) Node at the specified element index.
 */
Node<E> node(int index) {
    // assert isElementIndex(index);

    if (index < (size >> 1)) {
        Node<E> x = first;
        for (int i = 0; i < index; i++)
            x = x.next;
        return x;
    } else {
        Node<E> x = last;
        for (int i = size - 1; i > index; i--)
            x = x.prev;
        return x;
    }
}
```

##### æ’å…¥

```java
/**
 * Inserts the specified element at the specified position in this list.
 * Shifts the element currently at that position (if any) and any
 * subsequent elements to the right (adds one to their indices).
 *
 * @param index index at which the specified element is to be inserted
 * @param element element to be inserted
 * @throws IndexOutOfBoundsException {@inheritDoc}
 */
public void add(int index, E element) {
    checkPositionIndex(index);

    if (index == size)
        linkLast(element);
    else
        linkBefore(element, node(index));
}

/**
 * Inserts element e before non-null Node succ.
 */
void linkBefore(E e, Node<E> succ) {
    // assert succ != null;
    final Node<E> pred = succ.prev;
    final Node<E> newNode = new Node<>(pred, e, succ);
    succ.prev = newNode;
    if (pred == null)
        first = newNode;
    else
        pred.next = newNode;
    size++;
    modCount++;
}

/**
 * Links e as last element.
 */
void linkLast(E e) {
    final Node<E> l = last;
    final Node<E> newNode = new Node<>(l, e, null);
    last = newNode;
    if (l == null)
        first = newNode;
    else
        l.next = newNode;
    size++;
    modCount++;
}
```



##### åˆ é™¤

```java
/**
 * Removes the first occurrence of the specified element from this list,
 * if it is present.  If this list does not contain the element, it is
 * unchanged.  More formally, removes the element with the lowest index
 * {@code i} such that
 * {@code Objects.equals(o, get(i))}
 * (if such an element exists).  Returns {@code true} if this list
 * contained the specified element (or equivalently, if this list
 * changed as a result of the call).
 *
 * @param o element to be removed from this list, if present
 * @return {@code true} if this list contained the specified element
 */
public boolean remove(Object o) {
    if (o == null) {
        for (Node<E> x = first; x != null; x = x.next) {
            if (x.item == null) {
                unlink(x);
                return true;
            }
        }
    } else {
        for (Node<E> x = first; x != null; x = x.next) {
            if (o.equals(x.item)) {
                unlink(x);
                return true;
            }
        }
    }
    return false;
}

/**
 * Unlinks non-null node x.
 */
E unlink(Node<E> x) {
    // assert x != null;
    final E element = x.item;
    final Node<E> next = x.next;
    final Node<E> prev = x.prev;

    if (prev == null) {
        first = next;
    } else {
        prev.next = next;
        x.prev = null;
    }

    if (next == null) {
        last = prev;
    } else {
        next.prev = prev;
        x.next = null;
    }

    x.item = null; // é‡Šæ”¾å¼•ç”¨
    size--;
    modCount++;
    return element;
}

// å¤´éƒ¨ ä¸ å°¾éƒ¨ ä½œç‰¹æ®Šå¤„ç†
**
 * Unlinks non-null first node f.
 */
private E unlinkFirst(Node<E> f) {
    // assert f == first && f != null;
    final E element = f.item;
    final Node<E> next = f.next;
    f.item = null;
    f.next = null; // help GC
    first = next;
    if (next == null)
        last = null;
    else
        next.prev = null;
    size--;
    modCount++;
    return element;
}

/**
 * Unlinks non-null last node l.
 */
private E unlinkLast(Node<E> l) {
    // assert l == last && l != null;
    final E element = l.item;
    final Node<E> prev = l.prev;
    l.item = null;
    l.prev = null; // help GC
    last = prev;
    if (prev == null)
        first = null;
    else
        prev.next = null;
    size--;
    modCount++;
    return element;
}
```

##### æŸ¥æ‰¾ ä¸ ä¿®æ”¹

```java
/**
 * Returns the element at the specified position in this list.
 *
 * @param index index of the element to return
 * @return the element at the specified position in this list
 * @throws IndexOutOfBoundsException {@inheritDoc}
 */
public E get(int index) {
    checkElementIndex(index);
    return node(index).item;
}

/**
 * Replaces the element at the specified position in this list with the
 * specified element.
 *
 * @param index index of the element to replace
 * @param element element to be stored at the specified position
 * @return the element previously at the specified position
 * @throws IndexOutOfBoundsException {@inheritDoc}
 */
public E set(int index, E element) {
    checkElementIndex(index);
    Node<E> x = node(index);
    E oldVal = x.item;
    x.item = element;
    return oldVal;
}
```

##### æ¸…ç©º

```java
/**
 * Removes all of the elements from this list.
 * The list will be empty after this call returns.
 */
public void clear() {
    // Clearing all of the links between nodes is "unnecessary", but:
    // - helps a generational GC if the discarded nodes inhabit
    //   more than one generation
    // - is sure to free memory even if there is a reachable Iterator
    for (Node<E> x = first; x != null; ) {
        Node<E> next = x.next;
        x.item = null;
        x.next = null;
        x.prev = null;
        x = next;
    }
    first = last = null;
    size = 0;
    modCount++;
}
```

##### toArray

è¯¥å‡½æ•°å°†é“¾è¡¨è½¬æ¢ä¸ºæ•°ç»„ï¼Œè‡ªåŠ¨ç±»å‹è½¬æ¢åŠŸèƒ½æ¶‰åŠ**åå°„æœºåˆ¶**ï¼ˆä¼ å…¥çš„ç›®æ ‡æ•°ç»„å®¹é‡ä¸å¤Ÿæ—¶é€šè¿‡åå°„æœºåˆ¶è‡ªåŠ¨åˆ›å»ºï¼‰ã€‚

```java
public <T> T[] toArray(T[] a) {
	if (a.length < size)
	    a = (T[])java.lang.reflect.Array.newInstance(
	                        a.getClass().getComponentType(), size);
	int i = 0;
	Object[] result = a;
	for (Node<E> x = first; x != null; x = x.next)
	    result[i++] = x.item;

	if (a.length > size)
	    a[size] = null;

	return a;
}
```



#### ğŸ“é˜Ÿåˆ—

å…¥åˆ— â€”â€” **offer**

å‡ºåˆ— â€”â€” **poll**

æŸ¥çœ‹è€Œä¸å‡ºåˆ— â€”â€” **peek**



å…³äºé˜Ÿåˆ—ï¼Œæœ‰å¦‚ä¸‹ä¾èµ–ç»§æ‰¿å…³ç³»ï¼š

queue -> deque -> { LinkedList,  ArrayDeque }



##### å®ç°ç±» â€”â€” LinkedList

å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºåŒå‘é“¾è¡¨ï¼Œoffer æ–¹æ³• ä¸ add æ–¹æ³•ç­‰åŒã€‚

```java
public boolean offer(E e) {
    return add(e);
}

public boolean add(E e) {
    linkLast(e);
    return true;

}

public E poll() {
    final Node<E> f = first;
    return (f == null) ? null : unlinkFirst(f);
}

public E peek() {
    final Node<E> f = first;
    return (f == null) ? null : f.item;
}
```

##### å®ç°ç±» â€”â€” ArrayDeque

```java
public boolean add(E e) {
    addLast(e);
    return true;
}

public void addLast(E e) {
        if (e == null)
            throw new NullPointerException();
        final Object[] es = elements;
        es[tail] = e;
        if (head == (tail = inc(tail, es.length)))
            grow(1);
}


public boolean offer(E e) {
    return offerLast(e);
}

public boolean offerLast(E e) {
    addLast(e);
    return true;
}

public E poll() {
    return pollFirst();
}
public E pollFirst() {
    final Object[] es;
    final int h;
    E e = elementAt(es = elements, h = head);
    if (e != null) {
        es[h] = null;
        head = inc(h, es.length);
    }
    return e;
}
```

ç”±äº è¯¥ç±»åº•å±‚ä¸ºæ•°ç»„ï¼Œæ•…æ¶‰åŠåˆ°æ•°ç»„çš„**åŠ¨æ€æ‰©å®¹**

```java
/**
 * The maximum size of array to allocate.
 * Some VMs reserve some header words in an array.
 * Attempts to allocate larger arrays may result in
 * OutOfMemoryError: Requested array size exceeds VM limit
 */
private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

/**
 * Increases the capacity of this deque by at least the given amount.
 *
 * @param needed the required minimum extra capacity; must be positive
 */
private void grow(int needed) {
    // overflow-conscious code
    final int oldCapacity = elements.length;
    int newCapacity;
    // Double capacity if small; else grow by 50%
    int jump = (oldCapacity < 64) ? (oldCapacity + 2) : (oldCapacity >> 1);
    if (jump < needed
        || (newCapacity = (oldCapacity + jump)) - MAX_ARRAY_SIZE > 0)
        newCapacity = newCapacity(needed, jump);
    final Object[] es = elements = Arrays.copyOf(elements, newCapacity);
    // Exceptionally, here tail == head needs to be disambiguated
    if (tail < head || (tail == head && es[head] != null)) {
        // wrap around; slide first leg forward to end of array
        int newSpace = newCapacity - oldCapacity;
        System.arraycopy(es, head,
                         es, head + newSpace,
                         oldCapacity - head);
        for (int i = head, to = (head += newSpace); i < to; i++)
            es[i] = null;
    }
}
```

å¯¹äºåŒå‘é˜Ÿåˆ—ï¼Œæ”¯æŒåŒå‘æ“ä½œï¼š

```java
boolean offerFirst(E e);
boolean offerLast(E e);
E pollFirst();
E pollLast();
E peekFirst();
E peekLast();
```

### ğŸ–Šä¼˜å…ˆé˜Ÿåˆ—

ä¼˜å…ˆé˜Ÿåˆ—æ¶‰åŠå †ç®—æ³•

```java
public class PriorityQueue<E> extends AbstractQueue<E>
    implements java.io.Serializable {

    @java.io.Serial
    private static final long serialVersionUID = -7720805057305804111L;

    private static final int DEFAULT_INITIAL_CAPACITY = 11;

    /**
     * Priority queue represented as a balanced binary heap: the two
     * children of queue[n] are queue[2*n+1] and queue[2*(n+1)].  The
     * priority queue is ordered by comparator, or by the elements'
     * natural ordering, if comparator is null: For each node n in the
     * heap and each descendant d of n, n <= d.  The element with the
     * lowest value is in queue[0], assuming the queue is nonempty.
     */
    transient Object[] queue; // non-private to simplify nested class access

    /**
     * The number of elements in the priority queue.
     */
    int size;

    /**
     * The comparator, or null if priority queue uses elements'
     * natural ordering.
     */
    @SuppressWarnings("serial") // Conditionally serializable
    private final Comparator<? super E> comparator;

    /**
     * The number of times this priority queue has been
     * <i>structurally modified</i>.  See AbstractList for gory details.
     */
    transient int modCount;     // non-private to simplify nested class access
    
    // ......
}
```

#### åŠ¨æ€æ‰©å®¹

```java
/**
 * Increases the capacity of the array.
 *
 * @param minCapacity the desired minimum capacity
 */
private void grow(int minCapacity) {
    int oldCapacity = queue.length;
    // Double size if small; else grow by 50%
    int newCapacity = ArraysSupport.newLength(oldCapacity,
            minCapacity - oldCapacity, /* minimum growth */
            oldCapacity < 64 ? oldCapacity + 2 : oldCapacity >> 1
                                       /* preferred growth */);
    queue = Arrays.copyOf(queue, newCapacity);
}
```

#### æŸ¥æ‰¾æ“ä½œ

```java
private int indexOf(Object o) {
        if (o != null) {
            final Object[] es = queue;
            for (int i = 0, n = size; i < n; i++)
                if (o.equals(es[i]))
                    return i;
        }
        return -1;
}
```

#### æ’å…¥(å…¥åˆ—)æ“ä½œ

```java
public boolean offer(E e) {
    if (e == null)
        throw new NullPointerException();
    modCount++;
    int i = size;
    if (i >= queue.length)
        grow(i + 1);
    siftUp(i, e);
    size = i + 1;
    return true;
}

private void siftUp(int k, E x) {
        if (comparator != null)
            siftUpUsingComparator(k, x, queue, comparator);
        else
            siftUpComparable(k, x, queue);
}

private static <T> void siftUpUsingComparator(
        int k, T x, Object[] es, Comparator<? super T> cmp) {
        while (k > 0) {
            int parent = (k - 1) >>> 1;
            Object e = es[parent];
            if (cmp.compare(x, (T) e) >= 0)
                break;
            es[k] = e;
            k = parent;
        }
        es[k] = x;
}
```

#### åˆ é™¤æ“ä½œ

```java
 E removeAt(int i) {
 	// assert i >= 0 && i < size;
 	final Object[] es = queue;
 	modCount++;
 	int s = --size;
 	if (s == i) // removed last element
 	    es[i] = null;
 	else {
 	    E moved = (E) es[s];
 	    es[s] = null;
 	    siftDown(i, moved);
 	    if (es[i] == moved) {
 	        siftUp(i, moved);
 	        if (es[i] != moved)
 	            return moved;
 	    }
 	}
        return null;
}
    
private void siftDown(int k, E x) {
    if (comparator != null)
        siftDownUsingComparator(k, x, queue, size, comparator);
    else
        siftDownComparable(k, x, queue, size);
}

private static <T> void siftDownComparable(int k, T x, Object[] es, int n) {
    // assert n > 0;
    Comparable<? super T> key = (Comparable<? super T>)x;
    int half = n >>> 1;           // loop while a non-leaf
    while (k < half) {
        int child = (k << 1) + 1; // assume left child is least
        Object c = es[child];
        int right = child + 1;
        if (right < n &&
            ((Comparable<? super T>) c).compareTo((T) es[right]) > 0)
            c = es[child = right];
        if (key.compareTo((T) c) <= 0)
            break;
        es[k] = c;
        k = child;
    }
    es[k] = key;
}
```

#### å‡ºåˆ—

å‡ºåˆ—æ“ä½œæ˜¯åˆ é™¤æ“ä½œçš„ä¸€ç§ï¼Œä¹Ÿæ¶‰åŠå †çš„è°ƒæ•´æ“ä½œ

```java
public E poll() {
	final Object[] es;
	final E result;

	if ((result = (E) ((es = queue)[0])) != null) {
	    modCount++;
	    final int n;
	    final E x = (E) es[(n = --size)];
	    es[n] = null;
	    if (n > 0) {
	        final Comparator<? super E> cmp;
	        if ((cmp = comparator) == null)
	            siftDownComparable(0, x, es, n);
	        else
	            siftDownUsingComparator(0, x, es, n, cmp);
	    }
	}
	return result;
}
```



### ğŸ–Šå“ˆå¸Œè¡¨

#### é‡è¦å±æ€§

ä»é‡è¦å±æ€§ä¸­å¯ä»¥çœ‹å‡ºï¼Œå½“æ‹‰é“¾æ³•ä¸­å…ƒç´ è¾ƒå¤šæ—¶ï¼Œå°†æ‹‰é“¾çš„ç»“æ„(é“¾è¡¨)è½¬æ¢ä¸ºæ ‘ã€‚

```java
/* ---------------- Fields -------------- */

/**
 * The table, initialized on first use, and resized as
 * necessary. When allocated, length is always a power of two.
 * (We also tolerate length zero in some operations to allow
 * bootstrapping mechanics that are currently not needed.)
 */
transient Node<K,V>[] table;

/**
 * Holds cached entrySet(). Note that AbstractMap fields are used
 * for keySet() and values().
 */
transient Set<Map.Entry<K,V>> entrySet;
    /**
     * The default initial capacity - MUST be a power of two.
     */
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16

/**
 * The maximum capacity, used if a higher value is implicitly specified
 * by either of the constructors with arguments.
 * MUST be a power of two <= 1<<30.
 */
static final int MAXIMUM_CAPACITY = 1 << 30;

/**
 * The load factor used when none specified in constructor.
 */
static final float DEFAULT_LOAD_FACTOR = 0.75f;

/**
 * The bin count threshold for using a tree rather than list for a
 * bin.  Bins are converted to trees when adding an element to a
 * bin with at least this many nodes. The value must be greater
 * than 2 and should be at least 8 to mesh with assumptions in
 * tree removal about conversion back to plain bins upon
 * shrinkage.
 */
static final int TREEIFY_THRESHOLD = 8;

/**
 * The bin count threshold for untreeifying a (split) bin during a
 * resize operation. Should be less than TREEIFY_THRESHOLD, and at
 * most 6 to mesh with shrinkage detection under removal.
 */
static final int UNTREEIFY_THRESHOLD = 6;

/**
 * The smallest table capacity for which bins may be treeified.
 * (Otherwise the table is resized if too many nodes in a bin.)
 * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts
 * between resizing and treeification thresholds.
 */
static final int MIN_TREEIFY_CAPACITY = 64;
```

#### ç»“ç‚¹å°è£…

```java
/**
 * Basic hash bin node, used for most entries.  (See below for
 * TreeNode subclass, and in LinkedHashMap for its Entry subclass.)
 */
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    V value;
    Node<K,V> next;

    Node(int hash, K key, V value, Node<K,V> next) {
        this.hash = hash;
        this.key = key;
        this.value = value;
        this.next = next;
    }

    public final K getKey()        { return key; }
    public final V getValue()      { return value; }
    public final String toString() { return key + "=" + value; }

    public final int hashCode() {
        return Objects.hashCode(key) ^ Objects.hashCode(value);
    }

    public final V setValue(V newValue) {
        V oldValue = value;
        value = newValue;
        return oldValue;
    }

    public final boolean equals(Object o) {
        if (o == this)
            return true;

        return o instanceof Map.Entry<?, ?> e
                && Objects.equals(key, e.getKey())
                && Objects.equals(value, e.getValue());
    }
}

// ä¸‹åˆ—å‡½æ•°ç”¨äºåˆ›å»ºèŠ‚ç‚¹ (next ç”¨äºæ‹‰é“¾æ³•)
// Create a regular (non-tree) node
Node<K,V> newNode(int hash, K key, V value, Node<K,V> next) {
    return new Node<>(hash, key, value, next);
}
```

å…¶ä¸­

> Map.Entry

å°è£…äº† **é”®å€¼ã€ å€¼ã€ å“ˆå¸Œæ–¹æ³•ã€æ‹·è´æ–¹æ³•ã€æ¯”è¾ƒæ–¹æ³•ã€åˆ¤ç­‰æ–¹æ³•**


#### å“ˆå¸Œå‡½æ•°

```java
/**
 * Computes key.hashCode() and spreads (XORs) higher bits of hash
 * to lower.  Because the table uses power-of-two masking, sets of
 * hashes that vary only in bits above the current mask will
 * always collide. (Among known examples are sets of Float keys
 * holding consecutive whole numbers in small tables.)  So we
 * apply a transform that spreads the impact of higher bits
 * downward. There is a tradeoff between speed, utility, and
 * quality of bit-spreading. Because many common sets of hashes
 * are already reasonably distributed (so don't benefit from
 * spreading), and because we use trees to handle large sets of
 * collisions in bins, we just XOR some shifted bits in the
 * cheapest possible way to reduce systematic lossage, as well as
 * to incorporate impact of the highest bits that would otherwise
 * never be used in index calculations because of table bounds.
 */
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

ä»£ç ä¸­çš„ *hashcode* å‡½æ•°ç”±å¯¹è±¡å®ç°ã€‚Object ç±»ä¸­çš„ *hashcode* æ–¹æ³•ä¸º native æ–¹æ³•ã€‚

ç¢°æ’é—®é¢˜

#### æ’å…¥

```java
/**
 * Associates the specified value with the specified key in this map.
 * If the map previously contained a mapping for the key, the old
 * value is replaced.
 *
 * @param key key with which the specified value is to be associated
 * @param value value to be associated with the specified key
 * @return the previous value associated with {@code key}, or
 *         {@code null} if there was no mapping for {@code key}.
 *         (A {@code null} return can also indicate that the map
 *         previously associated {@code null} with {@code key}.)
 */
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

/**
 * Implements Map.put and related methods.
 *
 * @param hash hash for key
 * @param key the key
 * @param value the value to put
 * @param onlyIfAbsent if true, don't change existing value
 * @param evict if false, the table is in creation mode.
 * @return previous value, or null if none
 */
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);  // è¯¥ä½ç½®ä¸ºç©ºï¼Œç›´æ¥æ’å…¥
    else {
        Node<K,V> e; K k;
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        else if (p instanceof TreeNode)
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                // å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå½“ hash å€¼å†²çªæ—¶ä¼šåˆ©ç”¨ equals æ–¹æ³•è¿›ä¸€æ­¥æ¯”è¾ƒé”®å€¼æ˜¯å¦ç›¸åŒ
                if (e.hash == hash && 
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```

#### æŸ¥æ‰¾

æŸ¥æ‰¾è¿‡ç¨‹ä¸æ’å…¥çš„éå†è¿‡ç¨‹ç±»ä¼¼ã€‚

```java
public V get(Object key) {
    Node<K,V> e;
    return (e = getNode(key)) == null ? null : e.value;
}

 /**
 * Implements Map.get and related methods.
 *
 * @param key the key
 * @return the node, or null if none
 */
final Node<K,V> getNode(Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n, hash; K k;
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (first = tab[(n - 1) & (hash = hash(key))]) != null) {
        if (first.hash == hash && // always check first node
            ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        if ((e = first.next) != null) {
            if (first instanceof TreeNode)
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            do {
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

å‰©ä½™çš„ contains ç±»å‡½æ•°éƒ½æ˜¯åŸºäºä¸Šè¿°å‡½æ•°å®ç°çš„ã€‚

#### æ‹‰é“¾åˆ°æ ‘çš„è½¬æ¢

å½“æ‹‰é“¾é•¿åº¦è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œé“¾è¡¨å°†è½¬æ¢ä¸ºæ ‘ (**çº¢é»‘æ ‘**) 

```java
/**
 * Replaces all linked nodes in bin at index for given hash unless
 * table is too small, in which case resizes instead.
 */
final void treeifyBin(Node<K,V>[] tab, int hash) {
    int n, index; Node<K,V> e;
    if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY)
        resize();
    else if ((e = tab[index = (n - 1) & hash]) != null) {
        TreeNode<K,V> hd = null, tl = null;
        do {
            TreeNode<K,V> p = replacementTreeNode(e, null);
            if (tl == null)
                hd = p;
            else {
                p.prev = tl;
                tl.next = p;
            }
            tl = p;
        } while ((e = e.next) != null);
        if ((tab[index] = hd) != null)
            hd.treeify(tab);
    }
}

/**
 * Forms tree of the nodes linked from this node.
 */
final void treeify(Node<K,V>[] tab) {
    TreeNode<K,V> root = null;
    for (TreeNode<K,V> x = this, next; x != null; x = next) {
        next = (TreeNode<K,V>)x.next;
        x.left = x.right = null;
        if (root == null) {
            x.parent = null;
            x.red = false;
            root = x;
        }
        else {
            K k = x.key;
            int h = x.hash;
            Class<?> kc = null;
            for (TreeNode<K,V> p = root;;) {
                int dir, ph;
                K pk = p.key;
                if ((ph = p.hash) > h)
                    dir = -1;
                else if (ph < h)
                    dir = 1;
                else if ((kc == null &&
                          (kc = comparableClassFor(k)) == null) ||
                         (dir = compareComparables(kc, k, pk)) == 0)
                    dir = tieBreakOrder(k, pk);

                TreeNode<K,V> xp = p;
                if ((p = (dir <= 0) ? p.left : p.right) == null) {
                    x.parent = xp;
                    if (dir <= 0)
                        xp.left = x;
                    else
                        xp.right = x;
                    root = balanceInsertion(root, x);
                    break;
                }
            }
        }
    }
    moveRootToFront(tab, root);
}
```

ä»¥ä¸‹æ˜¯ **çº¢é»‘æ ‘** éƒ¨åˆ†çš„ä¸»è¦è°ƒæ•´ä»£ç 

```java
static <K,V> TreeNode<K,V> balanceInsertion(TreeNode<K,V> root,
                                                    TreeNode<K,V> x) {
    x.red = true;
    for (TreeNode<K,V> xp, xpp, xppl, xppr;;) {
        if ((xp = x.parent) == null) {
            x.red = false;
            return x;
        }
        else if (!xp.red || (xpp = xp.parent) == null)
            return root;
        if (xp == (xppl = xpp.left)) {
            if ((xppr = xpp.right) != null && xppr.red) {
                xppr.red = false;
                xp.red = false;
                xpp.red = true;
                x = xpp;
            }
            else {
                if (x == xp.right) {
                    root = rotateLeft(root, x = xp);
                    xpp = (xp = x.parent) == null ? null : xp.parent;
                }
                if (xp != null) {
                    xp.red = false;
                    if (xpp != null) {
                        xpp.red = true;
                        root = rotateRight(root, xpp);
                    }
                }
            }
        }
        else {
            if (xppl != null && xppl.red) {
                xppl.red = false;
                xp.red = false;
                xpp.red = true;
                x = xpp;
            }
            else {
                if (x == xp.left) {
                    root = rotateRight(root, x = xp);
                    xpp = (xp = x.parent) == null ? null : xp.parent;
                }
                if (xp != null) {
                    xp.red = false;
                    if (xpp != null) {
                        xpp.red = true;
                        root = rotateLeft(root, xpp);
                    }
                }
            }
        }
    }
}
```

```java
/**
 * Ensures that the given root is the first node of its bin.
 */
static <K,V> void moveRootToFront(Node<K,V>[] tab, TreeNode<K,V> root) {
    int n;
    if (root != null && tab != null && (n = tab.length) > 0) {
        int index = (n - 1) & root.hash;
        TreeNode<K,V> first = (TreeNode<K,V>)tab[index];
        if (root != first) {
            Node<K,V> rn;
            tab[index] = root;
            TreeNode<K,V> rp = root.prev;
            if ((rn = root.next) != null)
                ((TreeNode<K,V>)rn).prev = rp;
            if (rp != null)
                rp.next = rn;
            if (first != null)
                first.prev = root;
            root.next = first;
            root.prev = null;
        }
        assert checkInvariants(root);
    }
}
```

```java
/**
 * Tie-breaking utility for ordering insertions when equal
 * hashCodes and non-comparable. We don't require a total
 * order, just a consistent insertion rule to maintain
 * equivalence across rebalancings. Tie-breaking further than
 * necessary simplifies testing a bit.
 */
static int tieBreakOrder(Object a, Object b) {
    int d;
    if (a == null || b == null ||
        (d = a.getClass().getName().
         compareTo(b.getClass().getName())) == 0)
        d = (System.identityHashCode(a) <= System.identityHashCode(b) ?
             -1 : 1);
    return d;
}
```



#### åˆ é™¤

```java
/**
 * Removes the mapping for the specified key from this map if present.
 *
 * @param  key key whose mapping is to be removed from the map
 * @return the previous value associated with {@code key}, or
 *         {@code null} if there was no mapping for {@code key}.
 *         (A {@code null} return can also indicate that the map
 *         previously associated {@code null} with {@code key}.)
 */
public V remove(Object key) {
    Node<K,V> e;
    return (e = removeNode(hash(key), key, null, false, true)) == null ?
        null : e.value;
}

/**
 * Implements Map.remove and related methods.
 *
 * @param hash hash for key
 * @param key the key
 * @param value the value to match if matchValue, else ignored
 * @param matchValue if true only remove if value is equal
 * @param movable if false do not move other nodes while removing
 * @return the node, or null if none
 */
final Node<K,V> removeNode(int hash, Object key, Object value,
                           boolean matchValue, boolean movable) {
    Node<K,V>[] tab; Node<K,V> p; int n, index;
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (p = tab[index = (n - 1) & hash]) != null) {
        Node<K,V> node = null, e; K k; V v;
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            node = p;
        else if ((e = p.next) != null) {
            if (p instanceof TreeNode)
                node = ((TreeNode<K,V>)p).getTreeNode(hash, key);
            else {
                do {
                    if (e.hash == hash &&
                        ((k = e.key) == key ||
                         (key != null && key.equals(k)))) {
                        node = e;
                        break;
                    }
                    p = e;
                } while ((e = e.next) != null);
            }
        }
        if (node != null && (!matchValue || (v = node.value) == value ||
                             (value != null && value.equals(v)))) {
            if (node instanceof TreeNode)
                ((TreeNode<K,V>)node).removeTreeNode(this, tab, movable);
            else if (node == p)
                tab[index] = node.next; // æ‹‰é“¾æ³•ï¼šç›´æ¥å– next æŒ‡å‘çš„èŠ‚ç‚¹æ¥å¡«è¡¥å½“å‰æ¡¶ä½ç½®
            else
                p.next = node.next;
            ++modCount;
            --size;
            afterNodeRemoval(node);
            return node;
        }
    }
    return null;
}
```

### ğŸ–Šå¤åˆç»“æ„ï¼šLinkedHashMap

LinkedHashMap åŒæ—¶å®ç°äº† HashMap ä¸ åŒå‘é“¾è¡¨çš„åŠŸèƒ½ï¼Œåœ¨ HashMap çš„åŠŸèƒ½åŸºç¡€ä¸Šï¼Œè¿˜å¯ä¾ç…§æ’å…¥é¡ºåºè®¿é—®èŠ‚ç‚¹ã€‚é¡ºåºè®¿é—®å¯åŸºäºç»´æŠ¤çš„åŒå‘é“¾è¡¨è¿›è¡Œå®ç°ã€‚

ä»ä¸‹åˆ—ä»£ç  ( ç»“åˆ HashMap çš„æ’å…¥ã€æŸ¥æ‰¾æºç  ) å¯ä»¥çœ‹å‡ºï¼Œ**å†™å…¥æ“ä½œ** æ¶‰åŠçš„å…ƒç´ éƒ½ä¼šç»´æŠ¤åœ¨ä¸€ä¸ªåŒå‘é“¾è¡¨ (é€šè¿‡é‡å†™ *newNode* å‡½æ•° ) ä¸­ã€‚ä¹Ÿå¯ä»¥è§‚å¯Ÿåˆ°ï¼Œæ”¹å†™

> removeEldestEntry

æ–¹æ³•å¯ç”¨äºåˆ¤æ–­æ˜¯å¦ç§»é™¤æœ€è€çš„å…ƒç´ ã€‚

```java
void afterNodeInsertion(boolean evict) { // possibly remove eldest
    LinkedHashMap.Entry<K,V> first;
    if (evict && (first = head) != null && removeEldestEntry(first)) {
        K key = first.key;
        removeNode(hash(key), key, null, false, true);
    }
}

public V get(Object key) {
    Node<K,V> e;
    if ((e = getNode(key)) == null)
        return null;
    if (accessOrder)
        afterNodeAccess(e);
    return e.value;
}

public void forEach(BiConsumer<? super K, ? super V> action) {
    if (action == null)
        throw new NullPointerException();
    int mc = modCount;
    for (LinkedHashMap.Entry<K,V> e = head; e != null; e = e.after)
        action.accept(e.key, e.value);
    if (modCount != mc)
        throw new ConcurrentModificationException();
}
```

åŒå‘é“¾è¡¨çš„ç»´æŠ¤æ˜¯é€šè¿‡é‡å†™ *newNode* å‡½æ•°å®ç°çš„ã€‚å…¶ä¸­ä¸‹åˆ—çš„ 

> Entry

ç»“æ„æ˜¯ *Node* çš„å­ç±»ï¼Œå«æœ‰å‰åæŒ‡é’ˆï¼Œå¯ç”¨äºæ„å»ºåŒå‘é“¾è¡¨èŠ‚ç‚¹ã€‚

```java
/**
 * HashMap.Node subclass for normal LinkedHashMap entries.
 */
static class Entry<K,V> extends HashMap.Node<K,V> {
    Entry<K,V> before, after;
    Entry(int hash, K key, V value, Node<K,V> next) {
        super(hash, key, value, next);
    }
}
    
Node<K,V> newNode(int hash, K key, V value, Node<K,V> e) {
    LinkedHashMap.Entry<K,V> p =
        new LinkedHashMap.Entry<>(hash, key, value, e);
    linkNodeLast(p);
    return p;
}
```



## ğŸ–Šå¤šçº¿ç¨‹

### å¤šçº¿ç¨‹çš„ä½¿ç”¨

å¤šçº¿ç¨‹åŸºäº **Thread** ç±»å®ç°ã€‚å¸¸ç”¨çš„æ–¹æ¡ˆæ˜¯åœ¨è‡ªå·±çš„ç±»ä¸­å®ç° **Runnable** æ¥å£ã€‚

```java
/**
 * Initializes a new platform {@code Thread}. This constructor has the same
 * effect as {@linkplain #Thread(ThreadGroup,Runnable,String) Thread}
 * {@code (null, task, gname)}, where {@code gname} is a newly generated
 * name. Automatically generated names are of the form
 * {@code "Thread-"+}<i>n</i>, where <i>n</i> is an integer.
 *
 * <p> For a non-null task, invoking this constructor directly is equivalent to:
 * <pre>{@code Thread.ofPlatform().unstarted(task); }</pre>
 *
 * @param  task
 *         the object whose {@code run} method is invoked when this thread
 *         is started. If {@code null}, this classes {@code run} method does
 *         nothing.
 *
 * @see <a href="#inheritance">Inheritance when creating threads</a>
 */
public Thread(Runnable task) {
    this(null, null, 0, task, 0, null);
}
```

åœ¨ Thread ç±»ä¸­ï¼Œå°†å°è£…æˆ‘ä»¬ç±»é‡å†™çš„

> run() 

æ–¹æ³•ã€‚

```java
/**
 * This method is run by the thread when it executes. Subclasses of {@code
 * Thread} may override this method.
 *
 * <p> This method is not intended to be invoked directly. If this thread is a
 * platform thread created with a {@link Runnable} task then invoking this method
 * will invoke the task's {@code run} method. If this thread is a virtual thread
 * then invoking this method directly does nothing.
 *
 * @implSpec The default implementation executes the {@link Runnable} task that
 * the {@code Thread} was created with. If the thread was created without a task
 * then this method does nothing.
 */
@Override
public void run() {
    Runnable task = holder.task;
    if (task != null) {
        task.run();
    }
}
```

#### Thread ç±»é‡è¦å±æ€§

```java
public class Thread implements Runnable {
    /* Make sure registerNatives is the first thing <clinit> does. */
    private static native void registerNatives();
    static {
        registerNatives();
    }

    /* Reserved for exclusive use by the JVM, maybe move to FieldHolder */
    private long eetop;

    // thread id
    private final long tid;

    // thread name
    private volatile String name;

    // interrupt status (read/written by VM)
    volatile boolean interrupted;

    // context ClassLoader
    private volatile ClassLoader contextClassLoader;

    // inherited AccessControlContext, this could be moved to FieldHolder
    @SuppressWarnings("removal")
    private AccessControlContext inheritedAccessControlContext;

    // Additional fields for platform threads.
    // All fields, except task, are accessed directly by the VM.
    private static class FieldHolder {
        final ThreadGroup group;
        final Runnable task;
        final long stackSize;
        volatile int priority;
        volatile boolean daemon;
        volatile int threadStatus;
        boolean stillborn;

        FieldHolder(ThreadGroup group,
                    Runnable task,
                    long stackSize,
                    int priority,
                    boolean daemon) {
            this.group = group;
            this.task = task;
            this.stackSize = stackSize;
            this.priority = priority;
            if (daemon)
                this.daemon = true;
        }
    }
    private final FieldHolder holder;

    /*
     * ThreadLocal values pertaining to this thread. This map is maintained
     * by the ThreadLocal class. */
    ThreadLocal.ThreadLocalMap threadLocals;

    /*
     * InheritableThreadLocal values pertaining to this thread. This map is
     * maintained by the InheritableThreadLocal class.
     */
    ThreadLocal.ThreadLocalMap inheritableThreadLocals;

    /*
     * Extent locals binding are maintained by the ExtentLocal class.
     */
    private Object extentLocalBindings;
```

#### çº¿ç¨‹çš„å¯åŠ¨

```java
/**
 * Schedules this thread to begin execution. The thread will execute
 * independently of the current thread.
 *
 * <p> A thread can be started at most once. In particular, a thread can not
 * be restarted after it has terminated.
 *
 * @throws IllegalThreadStateException if the thread was already started
 */
public void start() {
    synchronized (this) {
        // zero status corresponds to state "NEW".
        if (holder.threadStatus != 0)
            throw new IllegalThreadStateException();
        start0();
    }
}

private native void start0(); // native æ–¹æ³•
```

#### çº¿ç¨‹çš„ä¸­æ–­

```java
public void interrupt() {
    if (this != Thread.currentThread()) {
        checkAccess();

        // thread may be blocked in an I/O operation
        synchronized (interruptLock) {
            Interruptible b = nioBlocker;
            if (b != null) {
                interrupted = true;
                interrupt0();  // inform VM of interrupt
                b.interrupt(this);
                return;
            }
        }
    }
    interrupted = true;
    interrupt0();  // inform VM of interrupt
}
```

#### ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢

```java
/**
 * Waits at most {@code millis} milliseconds for this thread to terminate.
 * A timeout of {@code 0} means to wait forever.
 * This method returns immediately, without waiting, if the thread has not
 * been {@link #start() started}.
 *
 * @implNote
 * For platform threads, the implementation uses a loop of {@code this.wait}
 * calls conditioned on {@code this.isAlive}. As a thread terminates the
 * {@code this.notifyAll} method is invoked. It is recommended that
 * applications not use {@code wait}, {@code notify}, or
 * {@code notifyAll} on {@code Thread} instances.
 *
 * @param  millis
 *         the time to wait in milliseconds
 *
 * @throws  IllegalArgumentException
 *          if the value of {@code millis} is negative
 *
 * @throws  InterruptedException
 *          if any thread has interrupted the current thread. The
 *          <i>interrupted status</i> of the current thread is
 *          cleared when this exception is thrown.
 */
public final void join(long millis) throws InterruptedException {
    if (millis < 0)
        throw new IllegalArgumentException("timeout value is negative");

    if (this instanceof VirtualThread vthread) {
        if (isAlive()) {
            long nanos = MILLISECONDS.toNanos(millis);
            vthread.joinNanos(nanos);
        }
        return;
    }

    synchronized (this) {
        if (millis > 0) {
            if (isAlive()) {
                final long startTime = System.nanoTime();
                long delay = millis;
                do {
                    wait(delay);
                } while (isAlive() && (delay = millis -
                         NANOSECONDS.toMillis(System.nanoTime() - startTime)) > 0);
            }
        } else {
            while (isAlive()) {
                wait(0);
            }
        }
    }
}
```

#### è°ƒæ•´çº¿ç¨‹ä¼˜å…ˆçº§

```java
public final void setPriority(int newPriority) {
    checkAccess();
    if (newPriority > MAX_PRIORITY || newPriority < MIN_PRIORITY) {
        throw new IllegalArgumentException();
    }
    if (!isVirtual()) {
        priority(newPriority);
    }
}

void priority(int newPriority) {
    ThreadGroup g = holder.group;
    if (g != null) {
        int maxPriority = g.getMaxPriority();
        if (newPriority > maxPriority) {
            newPriority = maxPriority;
        }
        setPriority0(holder.priority = newPriority);
    }
}
```

#### Yield æ“ä½œ

```java
public static void yield() {
    if (currentThread() instanceof VirtualThread vthread) {
        vthread.tryYield();
    } else {
        yield0();
    }
}

private static native void yield0();

 /**
 * Attempts to yield the current virtual thread (Thread.yield).
 */
void tryYield() {
    assert Thread.currentThread() == this;
    setState(YIELDING);
    try {
        yieldContinuation();
    } finally {
        assert Thread.currentThread() == this;
        if (state() != RUNNING) {
            assert state() == YIELDING;
            setState(RUNNING);
        }
    }
}

/**
 * Unmounts this virtual thread, invokes Continuation.yield, and re-mounts the
 * thread when continued. When enabled, JVMTI must be notified from this method.
 * @return true if the yield was successful
 */
@ChangesCurrentThread
private boolean yieldContinuation() {
    boolean notifyJvmti = notifyJvmtiEvents;

    // unmount
    if (notifyJvmti) notifyJvmtiUnmountBegin(false);
    unmount();
    try {
        return Continuation.yield(VTHREAD_SCOPE);
    } finally {
        // re-mount
        mount();
        if (notifyJvmti) notifyJvmtiMountEnd(false);
    }
}
```

#### çº¿ç¨‹çš„ç¡çœ 

```java
/**
 * Causes the currently executing thread to sleep (temporarily cease
 * execution) for the specified number of milliseconds, subject to
 * the precision and accuracy of system timers and schedulers. The thread
 * does not lose ownership of any monitors.
 *
 * @param  millis
 *         the length of time to sleep in milliseconds
 *
 * @throws  IllegalArgumentException
 *          if the value of {@code millis} is negative
 *
 * @throws  InterruptedException
 *          if any thread has interrupted the current thread. The
 *          <i>interrupted status</i> of the current thread is
 *          cleared when this exception is thrown.
 */
public static void sleep(long millis) throws InterruptedException {
    if (millis < 0) {
        throw new IllegalArgumentException("timeout value is negative");
    }

    if (currentThread() instanceof VirtualThread vthread) {
        long nanos = MILLISECONDS.toNanos(millis);
        vthread.sleepNanos(nanos);
        return;
    }

    if (ThreadSleepEvent.isTurnedOn()) {
        ThreadSleepEvent event = new ThreadSleepEvent();
        try {
            event.time = MILLISECONDS.toNanos(millis);
            event.begin();
            sleep0(millis);
        } finally {
            event.commit();
        }
    } else {
        sleep0(millis);
    }
}

private static native void sleep0(long millis) throws InterruptedException;
```



#### å¹³å°çº¿ç¨‹ & è™šæ‹Ÿçº¿ç¨‹

ç”±ä¸Šè¿°ä»£ç å¯ä»¥å‘ç°ï¼ŒJava çº¿ç¨‹å¯åˆ†ä¸º **å¹³å°çº¿ç¨‹** ä¸ **è™šæ‹Ÿçº¿ç¨‹**ã€‚å®ƒä»¬çš„ç‰¹ç‚¹å¦‚ä¸‹ï¼šï¼ˆæ‘˜è‡ªæºä»£ç æ–‡æ¡£ï¼‰

> **Platform threads**
> Thread supports the creation of platform threads that are typically mapped 1:1 to kernel threads scheduled by the operating system. Platform threads will usually have a large stack and other resources that are maintained by the operating system. Platforms threads are suitable for executing all types of tasks but may be a limited resource.
> Platform threads get an automatically generated thread name by default.
> Platform threads are designated daemon or non-daemon threads. When the Java virtual machine starts up, there is usually one non-daemon thread (the thread that typically calls the application's main method). The Java virtual machine terminates when all started non-daemon threads have terminated. Unstarted non-daemon threads do not prevent the Java virtual machine from terminating. The Java virtual machine can also be terminated by invoking the Runtime.exit(int) method, in which case it will terminate even if there are non-daemon threads still running.
> In addition to the daemon status, platform threads have a thread priority and are members of a thread group.



> **Virtual threads**
> Thread also supports the creation of virtual threads. Virtual threads are typically user-mode threads scheduled by the Java runtime rather than the operating system. Virtual threads will typically require few resources and a single Java virtual machine may support millions of virtual threads. Virtual threads are suitable for executing tasks that spend most of the time blocked, often waiting for I/O operations to complete. Virtual threads are not intended for long running CPU intensive operations.
> Virtual threads typically employ a small set of platform threads used as carrier threads. Locking and I/O operations are examples of operations where a carrier thread may be re-scheduled from one virtual thread to another. Code executing in a virtual thread is not aware of the underlying carrier thread. The currentThread() method, used to obtain a reference to the current thread, will always return the Thread object for the virtual thread.
> Virtual threads do not have a thread name by default. The getName method returns the empty string if a thread name is not set.
> Virtual threads are daemon threads and so do not prevent the Java virtual machine from terminating. Virtual threads have a fixed thread priority that cannot be changed.

å¯ä½¿ç”¨ä¸‹åˆ—æ–¹æ³•åˆ›å»ºè™šæ‹Ÿçº¿ç¨‹

```java
Thread thread1 = Thread.ofVirtual().start(runnable);
```

å…¶ä¸­ï¼Œå¯¹äºéƒ¨åˆ† jdk ç‰ˆæœ¬ï¼Œè™šæ‹Ÿçº¿ç¨‹ä¸ºé¢„è§ˆç‰ˆåŠŸèƒ½ï¼Œä¸ºä½¿ç”¨é¢„è§ˆåŠŸèƒ½ï¼Œéœ€è¦åœ¨ Idea IDE åŒæ—¶ ä¿®æ”¹

1. Java ç¼–è¯‘å™¨
2. è¿è¡Œ VM å‚æ•°

å¹¶ä¸ºä¸Šè¿°å¢åŠ :

> --enable-preview

#### çº¿ç¨‹åŒæ­¥

çº¿ç¨‹åŒæ­¥çš„å¸¸ç”¨æ–¹æ³•ï¼š

 1. synchronized å…³é”®å­— ï¼ˆå¯ä¿®é¥°æ•´ä¸ªæ–¹æ³• æˆ– ä»£ç æ®µï¼‰
 2. volatile å…³é”®å­— (ä¿®é¥°å˜é‡)
 3. åˆ›å»ºçº¿ç¨‹æœ¬åœ°å‰¯æœ¬å˜é‡ (éš”ç¦»å˜é‡)
 4. ä½¿ç”¨ **ReentrantLock**
5. **é˜»å¡é˜Ÿåˆ—**

##### é˜»å¡é˜Ÿåˆ—

###### å®ç°ç±»

|        å®ç°ç±»         |                             ç®€ä»‹                             |
| :-------------------: | :----------------------------------------------------------: |
|  ArrayBlockingQueue   |               ä¸€ä¸ªç”±æ•°ç»„ç»“æ„ç»„æˆçš„æœ‰ç•Œé˜»å¡é˜Ÿåˆ—               |
|  LinkedBlockingQueue  |               ä¸€ä¸ªç”±é“¾è¡¨ç»“æ„ç»„æˆçš„æœ‰ç•Œé˜»å¡é˜Ÿåˆ—               |
| PriorityBlockingQueue |               ä¸€ä¸ªæ”¯æŒä¼˜å…ˆçº§æ’åºçš„æ— ç•Œé˜»å¡é˜Ÿåˆ—               |
|      DelayQueue       |             ä¸€ä¸ªä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—å®ç°çš„æ— ç•Œé˜»å¡é˜Ÿåˆ—             |
|   SynchronousQueue    |                   ä¸€ä¸ªä¸å­˜å‚¨å…ƒç´ çš„é˜»å¡é˜Ÿåˆ—                   |
|  LinkedTransferQueue  | ä¸€ä¸ªç”±é“¾è¡¨ç»“æ„ç»„æˆçš„æ— ç•Œé˜»å¡é˜Ÿåˆ—ï¼ˆå®ç°äº†ç»§æ‰¿äº BlockingQueue çš„ TransferQueueï¼‰ |
|  LinkedBlockingDeque  |               ä¸€ä¸ªç”±é“¾è¡¨ç»“æ„ç»„æˆçš„åŒå‘é˜»å¡é˜Ÿåˆ—               |



*ä¸‹è¿°ä¸ºæ ¸å¿ƒæ“ä½œçš„æ ¸å¿ƒä»£ç  ( ä»¥ **ArrayBlockingQueue** ä¸ºä¾‹ )*

###### å…¥åˆ—

ç”±ä¸‹è¿°æºç å¯è§‚å¯Ÿåˆ°ï¼Œåº•å±‚ä½¿ç”¨äº† **ReentrantLock** å¯é‡å…¥é”ã€‚

```java
/**
 * Inserts the specified element at the tail of this queue if it is
 * possible to do so immediately without exceeding the queue's capacity,
 * returning {@code true} upon success and {@code false} if this queue
 * is full.  This method is generally preferable to method {@link #add},
 * which can fail to insert an element only by throwing an exception.
 *
 * @throws NullPointerException if the specified element is null
 */
public boolean offer(E e) {
    Objects.requireNonNull(e);
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        if (count == items.length)
            return false;
        else {
            enqueue(e);
            return true;
        }
    } finally {
        lock.unlock();
    }
}

/**
 * Inserts the specified element at the tail of this queue, waiting
 * up to the specified wait time for space to become available if
 * the queue is full.
 *
 * @throws InterruptedException {@inheritDoc}
 * @throws NullPointerException {@inheritDoc}
 */
public boolean offer(E e, long timeout, TimeUnit unit)
    throws InterruptedException {

    Objects.requireNonNull(e);
    long nanos = unit.toNanos(timeout);
    final ReentrantLock lock = this.lock;
    lock.lockInterruptibly();
    try {
        while (count == items.length) {
            if (nanos <= 0L)
                return false;
            nanos = notFull.awaitNanos(nanos); // ç­‰å¾…è¶…æ—¶
        }
        enqueue(e);
        return true;
    } finally {
        lock.unlock();
    }
}
```

###### å‡ºåˆ—

```java
public E poll() {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        return (count == 0) ? null : dequeue();
    } finally {
        lock.unlock();
    }
}

public E poll(long timeout, TimeUnit unit) throws InterruptedException {
    long nanos = unit.toNanos(timeout);
    final ReentrantLock lock = this.lock;
    lock.lockInterruptibly();
    try {
        while (count == 0) {
            if (nanos <= 0L)
                return null;
            nanos = notEmpty.awaitNanos(nanos);
        }
        return dequeue();
    } finally {
        lock.unlock();
    }
}

 /**
 * Extracts element at current take position, advances, and signals.
 * Call only when holding lock.
 */
private E dequeue() {
    // assert lock.isHeldByCurrentThread();
    // assert lock.getHoldCount() == 1;
    // assert items[takeIndex] != null;
    final Object[] items = this.items;
    @SuppressWarnings("unchecked")
    E e = (E) items[takeIndex];
    items[takeIndex] = null;
    if (++takeIndex == items.length) takeIndex = 0;
    count--;
    if (itrs != null)
        itrs.elementDequeued();
    notFull.signal();
    return e;
}
```

###### æ£€æŸ¥

```java
public E peek() {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        return itemAt(takeIndex); // null when queue is empty
    } finally {
        lock.unlock();
    }
}
```

##### çº¿ç¨‹ç­‰å¾…ä¸å”¤é†’

ä¸»è¦æ–¹æ³•å¦‚ä¸‹ ( **Object** ç±»æ–¹æ³• ) ï¼š

```java
/**
 * Causes the current thread to wait until it is awakened, typically
 * by being <em>notified</em> or <em>interrupted</em>.
 * <p>
 * In all respects, this method behaves as if {@code wait(0L, 0)}
 * had been called. See the specification of the {@link #wait(long, int)} method
 * for details.
 *
 * @throws IllegalMonitorStateException if the current thread is not
 *         the owner of the object's monitor
 * @throws InterruptedException if any thread interrupted the current thread before or
 *         while the current thread was waiting. The <em>interrupted status</em> of the
 *         current thread is cleared when this exception is thrown.
 * @see    #notify()
 * @see    #notifyAll()
 * @see    #wait(long)
 * @see    #wait(long, int)
 */
public final void wait() throws InterruptedException {
    wait(0L);
}

/**
 * Causes the current thread to wait until it is awakened, typically
 * by being <em>notified</em> or <em>interrupted</em>, or until a
 * certain amount of real time has elapsed.
 * <p>
 * In all respects, this method behaves as if {@code wait(timeoutMillis, 0)}
 * had been called. See the specification of the {@link #wait(long, int)} method
 * for details.
 *
 * @param  timeoutMillis the maximum time to wait, in milliseconds
 * @throws IllegalArgumentException if {@code timeoutMillis} is negative
 * @throws IllegalMonitorStateException if the current thread is not
 *         the owner of the object's monitor
 * @throws InterruptedException if any thread interrupted the current thread before or
 *         while the current thread was waiting. The <em>interrupted status</em> of the
 *         current thread is cleared when this exception is thrown.
 * @see    #notify()
 * @see    #notifyAll()
 * @see    #wait()
 * @see    #wait(long, int)
 */
public final void wait(long timeoutMillis) throws InterruptedException {
    long comp = Blocker.begin();
    try {
        wait0(timeoutMillis);
    } catch (InterruptedException e) {
        Thread thread = Thread.currentThread();
        if (thread.isVirtual())
            thread.getAndClearInterrupt();
        throw e;
    } finally {
        Blocker.end(comp);
    }
}

// final modifier so method not in vtable
private final native void wait0(long timeoutMillis) throws InterruptedException;
```

```java
/**
 * Wakes up a single thread that is waiting on this object's
 * monitor. If any threads are waiting on this object, one of them
 * is chosen to be awakened. The choice is arbitrary and occurs at
 * the discretion of the implementation. A thread waits on an object's
 * monitor by calling one of the {@code wait} methods.
 * <p>
 * The awakened thread will not be able to proceed until the current
 * thread relinquishes the lock on this object. The awakened thread will
 * compete in the usual manner with any other threads that might be
 * actively competing to synchronize on this object; for example, the
 * awakened thread enjoys no reliable privilege or disadvantage in being
 * the next thread to lock this object.
 * <p>
 * This method should only be called by a thread that is the owner
 * of this object's monitor. A thread becomes the owner of the
 * object's monitor in one of three ways:
 * <ul>
 * <li>By executing a synchronized instance method of that object.
 * <li>By executing the body of a {@code synchronized} statement
 *     that synchronizes on the object.
 * <li>For objects of type {@code Class,} by executing a
 *     static synchronized method of that class.
 * </ul>
 * <p>
 * Only one thread at a time can own an object's monitor.
 *
 * @throws  IllegalMonitorStateException  if the current thread is not
 *               the owner of this object's monitor.
 * @see        java.lang.Object#notifyAll()
 * @see        java.lang.Object#wait()
 */
@IntrinsicCandidate
public final native void notify();

/**
 * Wakes up all threads that are waiting on this object's monitor. A
 * thread waits on an object's monitor by calling one of the
 * {@code wait} methods.
 * <p>
 * The awakened threads will not be able to proceed until the current
 * thread relinquishes the lock on this object. The awakened threads
 * will compete in the usual manner with any other threads that might
 * be actively competing to synchronize on this object; for example,
 * the awakened threads enjoy no reliable privilege or disadvantage in
 * being the next thread to lock this object.
 * <p>
 * This method should only be called by a thread that is the owner
 * of this object's monitor. See the {@code notify} method for a
 * description of the ways in which a thread can become the owner of
 * a monitor.
 *
 * @throws  IllegalMonitorStateException  if the current thread is not
 *               the owner of this object's monitor.
 * @see        java.lang.Object#notify()
 * @see        java.lang.Object#wait()
 */
@IntrinsicCandidate
public final native void notifyAll();
```

> *å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šå¿…é¡»é¦–å…ˆè·å– wait ä¸ notify ç”¨äºåŒæ­¥çš„å¯¹è±¡çš„æ‰€æœ‰æƒï¼Œå³é‡‡ç”¨ **synchronized** å…³é”®å­—è·å–å¯¹è±¡æ‰€æœ‰æƒã€‚*

### âš¡çº¿ç¨‹æ± 

#### ğŸ“ThreadPoolExecutor

##### å®˜æ–¹ç®€ä»‹

> An ExecutorService that executes each submitted task using one of possibly several pooled threads, normally configured using Executors factory methods.
> Thread pools address two different problems: they usually provide improved performance when executing large numbers of asynchronous tasks, due to reduced per-task invocation overhead, and they provide a means of bounding and managing the resources, including threads, consumed when executing a collection of tasks. Each ThreadPoolExecutor also maintains some basic statistics, such as the number of completed tasks.
> To be useful across a wide range of contexts, this class provides many adjustable parameters and extensibility hooks. However, programmers are urged to use the more convenient Executors factory methods Executors.newCachedThreadPool (unbounded thread pool, with automatic thread reclamation), Executors.newFixedThreadPool (fixed size thread pool) and Executors.newSingleThreadExecutor (single background thread), that preconfigure settings for the most common usage scenarios. Otherwise, use the following guide when manually configuring and tuning this class:
>
> **Core and maximum pool sizes**
>
> A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize) according to the bounds set by corePoolSize (see getCorePoolSize) and maximumPoolSize (see getMaximumPoolSize). When a new task is submitted in method execute(Runnable), if fewer than corePoolSize threads are running, a new thread is created to handle the request, even if other worker threads are idle. Else if fewer than maximumPoolSize threads are running, a new thread will be created to handle the request only if the queue is full. By setting corePoolSize and maximumPoolSize the same, you create a fixed-size thread pool. By setting maximumPoolSize to an essentially unbounded value such as Integer.MAX_VALUE, you allow the pool to accommodate an arbitrary number of concurrent tasks. Most typically, core and maximum pool sizes are set only upon construction, but they may also be changed dynamically using setCorePoolSize and setMaximumPoolSize.
>
> **On-demand construction**
> By default, even core threads are initially created and started only when new tasks arrive, but this can be overridden dynamically using method prestartCoreThread or prestartAllCoreThreads. You probably want to prestart threads if you construct the pool with a non-empty queue.
>
> **Creating new threads**
> New threads are created using a ThreadFactory. If not otherwise specified, a Executors.defaultThreadFactory is used, that creates threads to all be in the same ThreadGroup and with the same NORM_PRIORITY priority and non-daemon status. By supplying a different ThreadFactory, you can alter the thread's name, thread group, priority, daemon status, etc. If a ThreadFactory fails to create a thread when asked by returning null from newThread, the executor will continue, but might not be able to execute any tasks. Threads should possess the "modifyThread" RuntimePermission. If worker threads or other threads using the pool do not possess this permission, service may be degraded: configuration changes may not take effect in a timely manner, and a shutdown pool may remain in a state in which termination is possible but not completed.
>
> **Keep-alive times**
> If the pool currently has more than corePoolSize threads, excess threads will be terminated if they have been idle for more than the keepAliveTime (see getKeepAliveTime(TimeUnit)). This provides a means of reducing resource consumption when the pool is not being actively used. If the pool becomes more active later, new threads will be constructed. This parameter can also be changed dynamically using method setKeepAliveTime(long, TimeUnit). Using a value of Long.MAX_VALUE TimeUnit.NANOSECONDS effectively disables idle threads from ever terminating prior to shut down. By default, the keep-alive policy applies only when there are more than corePoolSize threads, but method allowCoreThreadTimeOut(boolean) can be used to apply this time-out policy to core threads as well, so long as the keepAliveTime value is non-zero.
>
> **Queuing**
> Any BlockingQueue may be used to transfer and hold submitted tasks. The use of this queue interacts with pool sizing:
>
> 1. If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing.
>
> 2. If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
>
> 3. If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.
>
> 
>
> There are three general strategies for queuing:
>
> 1. Direct handoffs. A good default choice for a work queue is a SynchronousQueue that hands off tasks to threads without otherwise holding them. Here, an attempt to queue a task will fail if no threads are immediately available to run it, so a new thread will be constructed. This policy avoids lockups when handling sets of requests that might have internal dependencies. Direct handoffs generally require unbounded maximumPoolSizes to avoid rejection of new submitted tasks. This in turn admits the possibility of unbounded thread growth when commands continue to arrive on average faster than they can be processed.
>
> 2. Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed.
>
> 3. Bounded queues. A bounded queue (for example, an ArrayBlockingQueue) helps prevent resource exhaustion when used with finite maximumPoolSizes, but can be more difficult to tune and control. Queue sizes and maximum pool sizes may be traded off for each other: Using large queues and small pools minimizes CPU usage, OS resources, and context-switching overhead, but can lead to artificially low throughput. If tasks frequently block (for example if they are I/O bound), a system may be able to schedule time for more threads than you otherwise allow. Use of small queues generally requires larger pool sizes, which keeps CPUs busier but may encounter unacceptable scheduling overhead, which also decreases throughput.
>

> **Rejected tasks**
> New tasks submitted in method execute(Runnable) will be rejected when the Executor has been shut down, and also when the Executor uses finite bounds for both maximum threads and work queue capacity, and is saturated. In either case, the execute method invokes the RejectedExecutionHandler.rejectedExecution(Runnable, ThreadPoolExecutor) method of its RejectedExecutionHandler. Four predefined handler policies are provided:
>
> 1. In the default ThreadPoolExecutor.AbortPolicy, the handler throws a runtime RejectedExecutionException upon rejection.
> 2. In ThreadPoolExecutor.CallerRunsPolicy, the thread that invokes execute itself runs the task. This provides a simple feedback control mechanism that will slow down the rate that new tasks are submitted.
> 3. In ThreadPoolExecutor.DiscardPolicy, a task that cannot be executed is simply dropped. This policy is designed only for those rare cases in which task completion is never relied upon.
> 4. In ThreadPoolExecutor.DiscardOldestPolicy, if the executor is not shut down, the task at the head of the work queue is dropped, and then execution is retried (which can fail again, causing this to be repeated.) This po licy is rarely acceptable. In nearly all cases, you should also cancel the task to cause an exception in any component waiting for its completion, and/or log the failure, as illustrated in ThreadPoolExecutor.DiscardOldestPolicy documentation.
>
> It is possible to define and use other kinds of RejectedExecutionHandler classes. Doing so requires some care especially when policies are designed to work only under particular capacity or queuing policies.
>
> **Hook methods**
> This class provides protected overridable beforeExecute(Thread, Runnable) and afterExecute(Runnable, Throwable) methods that are called before and after execution of each task. These can be used to manipulate the execution environment; for example, reinitializing ThreadLocals, gathering statistics, or adding log entries. Additionally, method terminated can be overridden to perform any special processing that needs to be done once the Executor has fully terminated.
> If hook, callback, or BlockingQueue methods throw exceptions, internal worker threads may in turn fail, abruptly terminate, and possibly be replaced.
>
> **Queue maintenance**
> Method getQueue() allows access to the work queue for purposes of monitoring and debugging. Use of this method for any other purpose is strongly discouraged. Two supplied methods, remove(Runnable) and purge are available to assist in storage reclamation when large numbers of queued tasks become cancelled.
>
> **Reclamation**
> A pool that is no longer referenced in a program AND has no remaining threads may be reclaimed (garbage collected) without being explicitly shutdown. You can configure a pool to allow all unused threads to eventually die by setting appropriate keep-alive times, using a lower bound of zero core threads and/or setting allowCoreThreadTimeOut(boolean).
>
> **Extension example**. 
>
> Most extensions of this class override one or more of the protected hook methods. For example, here is a subclass that adds a simple pause/resume feature:
>
> ```java
> class PausableThreadPoolExecutor extends ThreadPoolExecutor {
> 	private boolean isPaused;
> 	private ReentrantLock pauseLock = new ReentrantLock();
> 	private Condition unpaused = pauseLock.newCondition();
> 	
> 	public PausableThreadPoolExecutor(...) { super(...); }
> 	
> 	protected void beforeExecute(Thread t, Runnable r) {
> 	  super.beforeExecute(t, r);
> 	  pauseLock.lock();
> 	  try {
> 	    while (isPaused) unpaused.await();
> 	  } catch (InterruptedException ie) {
> 	    t.interrupt();
> 	  } finally {
> 	    pauseLock.unlock();
> 	  }
> 	}
> 	
> 	public void pause() {
> 	  pauseLock.lock();
> 	  try {
> 	    isPaused = true;
> 	  } finally {
> 	    pauseLock.unlock();
> 	  }
> 	}
> 	
> 	public void resume() {
> 	  pauseLock.lock();
> 	  try {
> 	    isPaused = false;
> 	    unpaused.signalAll();
> 	  } finally {
> 	    pauseLock.unlock();
> 	  }
> 	}
> }
> ```

##### æ ¸å¿ƒæ„é€ å‡½æ•°

```java
/**
 * Creates a new {@code ThreadPoolExecutor} with the given initial
 * parameters.
 *
 * @param corePoolSize the number of threads to keep in the pool, even
 *        if they are idle, unless {@code allowCoreThreadTimeOut} is set
 * @param maximumPoolSize the maximum number of threads to allow in the
 *        pool
 * @param keepAliveTime when the number of threads is greater than
 *        the core, this is the maximum time that excess idle threads
 *        will wait for new tasks before terminating.
 * @param unit the time unit for the {@code keepAliveTime} argument
 * @param workQueue the queue to use for holding tasks before they are
 *        executed.  This queue will hold only the {@code Runnable}
 *        tasks submitted by the {@code execute} method.
 * @param threadFactory the factory to use when the executor
 *        creates a new thread
 * @param handler the handler to use when execution is blocked
 *        because the thread bounds and queue capacities are reached
 * @throws IllegalArgumentException if one of the following holds:<br>
 *         {@code corePoolSize < 0}<br>
 *         {@code keepAliveTime < 0}<br>
 *         {@code maximumPoolSize <= 0}<br>
 *         {@code maximumPoolSize < corePoolSize}
 * @throws NullPointerException if {@code workQueue}
 *         or {@code threadFactory} or {@code handler} is null
 */
public ThreadPoolExecutor(int corePoolSize,
                          int maximumPoolSize,
                          long keepAliveTime,
                          TimeUnit unit,
                          BlockingQueue<Runnable> workQueue,
                          ThreadFactory threadFactory,
                          RejectedExecutionHandler handler) {
    if (corePoolSize < 0 ||
        maximumPoolSize <= 0 ||
        maximumPoolSize < corePoolSize ||
        keepAliveTime < 0)
        throw new IllegalArgumentException();
    if (workQueue == null || threadFactory == null || handler == null)
        throw new NullPointerException();
    this.corePoolSize = corePoolSize;
    this.maximumPoolSize = maximumPoolSize;
    this.workQueue = workQueue;
    this.keepAliveTime = unit.toNanos(keepAliveTime);
    this.threadFactory = threadFactory;
    this.handler = handler;

    String name = Objects.toIdentityString(this);
    this.container = SharedThreadContainer.create(name);
}
```

å…¶ä¸­å‰ 4 ä¸ªå‚æ•° åˆ†åˆ« æŒ‡å®š çº¿ç¨‹æ±  çš„ *æ ¸å¿ƒçº¿ç¨‹æ•°ï¼Œæœ€å¤§ä¸‹çº¿ç¨‹æ•°ï¼Œçº¿ç¨‹å­˜æ´»æ—¶é—´ä¸æ—¶é—´å•ä½*ã€‚

###### BlockingQueue

è¯¥å‚æ•°æŒ‡å®šä¸€ä¸ªé˜»å¡é˜Ÿåˆ—ç±»ç”¨äºç®¡ç†çº¿ç¨‹æ± ï¼Œä¸åŒçš„å®ç°ç±»ä»£è¡¨ç€ä¸åŒçš„ç­–ç•¥ï¼Œå®˜æ–¹æä¾›ä»¥ä¸‹å®ç°ç±»ï¼š

|        å®ç°ç±»         |                             ç®€ä»‹                             |
| :-------------------: | :----------------------------------------------------------: |
|  ArrayBlockingQueue   |              ä¸€ä¸ªç”±æ•°ç»„ç»“æ„ç»„æˆçš„æœ‰ç•Œé˜»å¡é˜Ÿåˆ—ã€‚              |
|  LinkedBlockingQueue  |              ä¸€ä¸ªç”±é“¾è¡¨ç»“æ„ç»„æˆçš„æœ‰ç•Œé˜»å¡é˜Ÿåˆ—ã€‚              |
|   SynchronousQueue    |    ä¸€ä¸ªä¸å­˜å‚¨å…ƒç´ çš„é˜»å¡é˜Ÿåˆ—ï¼Œå³ç›´æ¥æäº¤ç»™çº¿ç¨‹ä¸ä¿æŒå®ƒä»¬ã€‚    |
| PriorityBlockingQueue |              ä¸€ä¸ªæ”¯æŒä¼˜å…ˆçº§æ’åºçš„æ— ç•Œé˜»å¡é˜Ÿåˆ—ã€‚              |
|      DelayQueue       | ä¸€ä¸ªä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—å®ç°çš„æ— ç•Œé˜»å¡é˜Ÿåˆ—ï¼Œåªæœ‰åœ¨å»¶è¿ŸæœŸæ»¡æ—¶æ‰èƒ½ä»ä¸­æå–å…ƒç´ ã€‚ |
|  LinkedTransferQueue  | ä¸€ä¸ªç”±é“¾è¡¨ç»“æ„ç»„æˆçš„æ— ç•Œé˜»å¡é˜Ÿåˆ—ã€‚ä¸ SynchronousQueue ç±»ä¼¼ï¼Œè¿˜å«æœ‰éé˜»å¡æ–¹æ³•ã€‚ |
|  LinkedBlockingDeque  |              ä¸€ä¸ªç”±é“¾è¡¨ç»“æ„ç»„æˆçš„åŒå‘é˜»å¡é˜Ÿåˆ—ã€‚              |

###### ThreadFactory

æ­¤å‚æ•°ç”¨äºæŒ‡å®šä¸€ä¸ªçº¿ç¨‹å·¥å‚ï¼Œä¸»è¦ç”¨äºåˆ›å»ºçº¿ç¨‹ï¼Œé»˜è®¤åŠæ­£å¸¸ä¼˜å…ˆçº§ã€å®ˆæŠ¤çº¿ç¨‹ã€‚

ThreadFactory æ˜¯ä¸€ä¸ªæ¥å£ï¼Œåªè¦å®ç°äº†ä»¥ä¸‹æ–¹æ³•å³ä¸ºä¸€ä¸ªçº¿ç¨‹å·¥å‚ã€‚

```java
public interface ThreadFactory {

    /**
     * Constructs a new unstarted {@code Thread} to run the given runnable.
     *
     * @param r a runnable to be executed by new thread instance
     * @return constructed thread, or {@code null} if the request to
     *         create a thread is rejected
     *
     * @see <a href="../../lang/Thread.html#inheritance">Inheritance when
     * creating threads</a>
     */
    Thread newThread(Runnable r);
}
```

å¯é€šè¿‡

> Executors.*defaultThreadFactory*()

è¿”å›ä¸€ä¸ª**é»˜è®¤å·¥å‚**ï¼Œè¯¥å·¥å‚ é»˜è®¤æ‰€æœ‰çº¿ç¨‹ä¼˜å…ˆçº§ä¸ºé»˜è®¤ä¼˜å…ˆçº§ï¼Œä¸”ä¸ºéå®ˆæŠ¤çº¿ç¨‹

```java
/**
 * Returns a default thread factory used to create new threads.
 * This factory creates all new threads used by an Executor in the
 * same {@link ThreadGroup}. If there is a {@link
 * java.lang.SecurityManager}, it uses the group of {@link
 * System#getSecurityManager}, else the group of the thread
 * invoking this {@code defaultThreadFactory} method. Each new
 * thread is created as a non-daemon thread with priority set to
 * the smaller of {@code Thread.NORM_PRIORITY} and the maximum
 * priority permitted in the thread group.  New threads have names
 * accessible via {@link Thread#getName} of
 * <em>pool-N-thread-M</em>, where <em>N</em> is the sequence
 * number of this factory, and <em>M</em> is the sequence number
 * of the thread created by this factory.
 * @return a thread factory
 */
public static ThreadFactory defaultThreadFactory() {
    return new DefaultThreadFactory();
}


/**
 * The default thread factory.
 */
private static class DefaultThreadFactory implements ThreadFactory {
    private static final AtomicInteger poolNumber = new AtomicInteger(1);
    private final ThreadGroup group;
    private final AtomicInteger threadNumber = new AtomicInteger(1);
    private final String namePrefix;

    DefaultThreadFactory() {
        @SuppressWarnings("removal")
        SecurityManager s = System.getSecurityManager();
        group = (s != null) ? s.getThreadGroup() :
                              Thread.currentThread().getThreadGroup();
        namePrefix = "pool-" +
                      poolNumber.getAndIncrement() +
                     "-thread-";
    }

    public Thread newThread(Runnable r) {
        Thread t = new Thread(group, r,
                              namePrefix + threadNumber.getAndIncrement(),
                              0);
        if (t.isDaemon())
            t.setDaemon(false);
        if (t.getPriority() != Thread.NORM_PRIORITY)
            t.setPriority(Thread.NORM_PRIORITY);
        return t;
    }
}
```

###### Executors ç±» æä¾›çš„å…¶å®ƒçº¿ç¨‹å·¥å‚

ä»¥ä¸‹ç±» æ”¯æŒä¿®æ”¹çº¿ç¨‹çš„ä¸Šä¸‹æ–‡ä¸ç±»åŠ è½½å™¨

```java
/**
 * Thread factory capturing access control context and class loader.
 */
private static class PrivilegedThreadFactory extends DefaultThreadFactory {
    @SuppressWarnings("removal")
    final AccessControlContext acc;
    final ClassLoader ccl;

    @SuppressWarnings("removal")
    PrivilegedThreadFactory() {
        super();
        SecurityManager sm = System.getSecurityManager();
        if (sm != null) {
            // Calls to getContextClassLoader from this class
            // never trigger a security check, but we check
            // whether our callers have this permission anyways.
            sm.checkPermission(SecurityConstants.GET_CLASSLOADER_PERMISSION);

            // Fail fast
            sm.checkPermission(new RuntimePermission("setContextClassLoader"));
        }
        this.acc = AccessController.getContext();
        this.ccl = Thread.currentThread().getContextClassLoader();
    }

    public Thread newThread(final Runnable r) {
        return super.newThread(new Runnable() {
            @SuppressWarnings("removal")
            public void run() {
                AccessController.doPrivileged(new PrivilegedAction<>() {
                    public Void run() {
                        Thread.currentThread().setContextClassLoader(ccl);
                        r.run();
                        return null;
                    }
                }, acc);
            }
        });
    }
}
```

###### RejectedExecutionHandler

RejectedExecutionHandler ä¸ºä¸€ä¸ªæ¥å£ï¼ŒæŒ‡å®šçº¿ç¨‹æ± çš„æ‹’ç»è¡Œä¸º

```java
/**
 * A handler for tasks that cannot be executed by a {@link ThreadPoolExecutor}.
 *
 * @since 1.5
 * @author Doug Lea
 */
public interface RejectedExecutionHandler {

    /**
     * Method that may be invoked by a {@link ThreadPoolExecutor} when
     * {@link ThreadPoolExecutor#execute execute} cannot accept a
     * task.  This may occur when no more threads or queue slots are
     * available because their bounds would be exceeded, or upon
     * shutdown of the Executor.
     *
     * <p>In the absence of other alternatives, the method may throw
     * an unchecked {@link RejectedExecutionException}, which will be
     * propagated to the caller of {@code execute}.
     *
     * @param r the runnable task requested to be executed
     * @param executor the executor attempting to execute this task
     * @throws RejectedExecutionException if there is no remedy
     */
    void rejectedExecution(Runnable r, ThreadPoolExecutor executor);
}
```

åœ¨ ä»»åŠ¡æäº¤ é˜¶æ®µ (è¯¦è§ *ä»»åŠ¡æäº¤* èŠ‚) çš„ä»¥ä¸‹å‡½æ•°è¢«è°ƒç”¨

```java
/**
 * Invokes the rejected execution handler for the given command.
 * Package-protected for use by ScheduledThreadPoolExecutor.
 */
final void reject(Runnable command) {
    handler.rejectedExecution(command, this);
}
```

è¯¥æ¥å£ç›®å‰å«4ä¸ªå®ç°ç±»ï¼š

**AbortPolicy** ç›´æ¥æŠ›å‡ºå¼‚å¸¸

```java
/**
 * A handler for rejected tasks that throws a
 * {@link RejectedExecutionException}.
 *
 * This is the default handler for {@link ThreadPoolExecutor} and
 * {@link ScheduledThreadPoolExecutor}.
 */
public static class AbortPolicy implements RejectedExecutionHandler {
    /**
     * Creates an {@code AbortPolicy}.
     */
    public AbortPolicy() { }

    /**
     * Always throws RejectedExecutionException.
     *
     * @param r the runnable task requested to be executed
     * @param e the executor attempting to execute this task
     * @throws RejectedExecutionException always
     */
    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
        throw new RejectedExecutionException("Task " + r.toString() +
                                             " rejected from " +
                                             e.toString());
    }
}
```

**CallerRunsPolicy** ç›´æ¥æ‰§è¡Œè¯¥ä»»åŠ¡ï¼Œé™¤éçº¿ç¨‹æ± è¢«å…³é—­

```java
/**
 * A handler for rejected tasks that runs the rejected task
 * directly in the calling thread of the {@code execute} method,
 * unless the executor has been shut down, in which case the task
 * is discarded.
 */
public static class CallerRunsPolicy implements RejectedExecutionHandler {
    /**
     * Creates a {@code CallerRunsPolicy}.
     */
    public CallerRunsPolicy() { }

    /**
     * Executes task r in the caller's thread, unless the executor
     * has been shut down, in which case the task is discarded.
     *
     * @param r the runnable task requested to be executed
     * @param e the executor attempting to execute this task
     */
    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
        if (!e.isShutdown()) {
            r.run();
        }
    }
}
```

**DiscardOldestPolicy** å°†é˜Ÿåˆ—ä¸‹ä¸€ä¸ªçš„æœªæ‰§è¡Œä»»åŠ¡å‡ºåˆ—ï¼Œç„¶åå°†æœ€æ–°çš„ä»»åŠ¡åŠ å…¥å¹¶å°è¯•é‡æ–°æäº¤

```java
public static class DiscardOldestPolicy implements RejectedExecutionHandler {
        /**
         * Creates a {@code DiscardOldestPolicy} for the given executor.
         */
        public DiscardOldestPolicy() { }

        /**
         * Obtains and ignores the next task that the executor
         * would otherwise execute, if one is immediately available,
         * and then retries execution of task r, unless the executor
         * is shut down, in which case task r is instead discarded.
         *
         * @param r the runnable task requested to be executed
         * @param e the executor attempting to execute this task
         */
        public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
            if (!e.isShutdown()) {
                e.getQueue().poll();
                e.execute(r);
            }
        }
    }
```

DiscardPolicy ç›´æ¥æŠ›å¼ƒè¯¥ä»»åŠ¡ï¼Œä¸åšä»»ä½•æ“ä½œ

```java
/**
 * A handler for rejected tasks that silently discards the
 * rejected task.
 */
public static class DiscardPolicy implements RejectedExecutionHandler {
    /**
     * Creates a {@code DiscardPolicy}.
     */
    public DiscardPolicy() { }

    /**
     * Does nothing, which has the effect of discarding task r.
     *
     * @param r the runnable task requested to be executed
     * @param e the executor attempting to execute this task
     */
    public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
    }
}
```

##### å±æ€§å­—æ®µ

```java
public class ThreadPoolExecutor extends AbstractExecutorService {
    /**
     * The main pool control state, ctl, is an atomic integer packing
     * two conceptual fields
     *   workerCount, indicating the effective number of threads
     *   runState,    indicating whether running, shutting down etc
     *
     * In order to pack them into one int, we limit workerCount to
     * (2^29)-1 (about 500 million) threads rather than (2^31)-1 (2
     * billion) otherwise representable. If this is ever an issue in
     * the future, the variable can be changed to be an AtomicLong,
     * and the shift/mask constants below adjusted. But until the need
     * arises, this code is a bit faster and simpler using an int.
     *
     * The workerCount is the number of workers that have been
     * permitted to start and not permitted to stop.  The value may be
     * transiently different from the actual number of live threads,
     * for example when a ThreadFactory fails to create a thread when
     * asked, and when exiting threads are still performing
     * bookkeeping before terminating. The user-visible pool size is
     * reported as the current size of the workers set.
     *
     * The runState provides the main lifecycle control, taking on values:
     *
     *   RUNNING:  Accept new tasks and process queued tasks
     *   SHUTDOWN: Don't accept new tasks, but process queued tasks
     *   STOP:     Don't accept new tasks, don't process queued tasks,
     *             and interrupt in-progress tasks
     *   TIDYING:  All tasks have terminated, workerCount is zero,
     *             the thread transitioning to state TIDYING
     *             will run the terminated() hook method
     *   TERMINATED: terminated() has completed
     *
     * The numerical order among these values matters, to allow
     * ordered comparisons. The runState monotonically increases over
     * time, but need not hit each state. The transitions are:
     *
     * RUNNING -> SHUTDOWN
     *    On invocation of shutdown()
     * (RUNNING or SHUTDOWN) -> STOP
     *    On invocation of shutdownNow()
     * SHUTDOWN -> TIDYING
     *    When both queue and pool are empty
     * STOP -> TIDYING
     *    When pool is empty
     * TIDYING -> TERMINATED
     *    When the terminated() hook method has completed
     *
     * Threads waiting in awaitTermination() will return when the
     * state reaches TERMINATED.
     *
     * Detecting the transition from SHUTDOWN to TIDYING is less
     * straightforward than you'd like because the queue may become
     * empty after non-empty and vice versa during SHUTDOWN state, but
     * we can only terminate if, after seeing that it is empty, we see
     * that workerCount is 0 (which sometimes entails a recheck -- see
     * below).
     */
    private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
    private static final int COUNT_BITS = Integer.SIZE - 3;
    private static final int COUNT_MASK = (1 << COUNT_BITS) - 1;

    // runState is stored in the high-order bits
    private static final int RUNNING    = -1 << COUNT_BITS;
    private static final int SHUTDOWN   =  0 << COUNT_BITS;
    private static final int STOP       =  1 << COUNT_BITS;
    private static final int TIDYING    =  2 << COUNT_BITS;
    private static final int TERMINATED =  3 << COUNT_BITS;

    // Packing and unpacking ctl
    private static int runStateOf(int c)     { return c & ~COUNT_MASK; }
    private static int workerCountOf(int c)  { return c & COUNT_MASK; }
    private static int ctlOf(int rs, int wc) { return rs | wc; }
    
    // ...
}
```

ç”±å±æ€§å­—æ®µå¯ä»¥è§‚å¯ŸçŸ¥é“çº¿ç¨‹æ± çš„5ç§çŠ¶æ€ã€‚

##### ä»»åŠ¡æäº¤

```java
/**
 * Executes the given task sometime in the future.  The task
 * may execute in a new thread or in an existing pooled thread.
 *
 * If the task cannot be submitted for execution, either because this
 * executor has been shutdown or because its capacity has been reached,
 * the task is handled by the current {@link RejectedExecutionHandler}.
 *
 * @param command the task to execute
 * @throws RejectedExecutionException at discretion of
 *         {@code RejectedExecutionHandler}, if the task
 *         cannot be accepted for execution
 * @throws NullPointerException if {@code command} is null
 */
public void execute(Runnable command) {
    if (command == null)
        throw new NullPointerException();
    /*
     * Proceed in 3 steps:
     *
     * 1. If fewer than corePoolSize threads are running, try to
     * start a new thread with the given command as its first
     * task.  The call to addWorker atomically checks runState and
     * workerCount, and so prevents false alarms that would add
     * threads when it shouldn't, by returning false.
     *
     * 2. If a task can be successfully queued, then we still need
     * to double-check whether we should have added a thread
     * (because existing ones died since last checking) or that
     * the pool shut down since entry into this method. So we
     * recheck state and if necessary roll back the enqueuing if
     * stopped, or start a new thread if there are none.
     *
     * 3. If we cannot queue task, then we try to add a new
     * thread.  If it fails, we know we are shut down or saturated
     * and so reject the task.
     */
    int c = ctl.get();
    if (workerCountOf(c) < corePoolSize) {
        if (addWorker(command, true))
            return;
        c = ctl.get();
    }
    if (isRunning(c) && workQueue.offer(command)) {
        int recheck = ctl.get();
        if (! isRunning(recheck) && remove(command))
            reject(command);
        else if (workerCountOf(recheck) == 0)
            addWorker(null, false);
    }
    else if (!addWorker(command, false))
        reject(command);
}
```

ä»æºä»£ç ä¸­å¯ä»¥çœ‹å‡ºï¼Œä»»åŠ¡æäº¤æ¶‰åŠé˜»å¡é˜Ÿåˆ—çš„ **offer** æ–¹æ³•ã€‚

åŒæ—¶ä¹Ÿå¯è§‚å¯Ÿåˆ°ä»»åŠ¡æäº¤çš„è§„åˆ™ï¼š

> 1. Running ä»»åŠ¡æ•° < æ ¸å¿ƒçº¿ç¨‹æ•°ï¼šåˆ›å»ºæ–°çº¿ç¨‹
> 2. å¦åˆ™ å°è¯•å…¥åˆ—
> 3. å…¥åˆ—å¤±è´¥ï¼Œåˆ™æ‹’ç»æ–°ä»»åŠ¡ã€‚

##### ä»»åŠ¡ç§»é™¤

```java
/**
 * Removes this task from the executor's internal queue if it is
 * present, thus causing it not to be run if it has not already
 * started.
 *
 * <p>This method may be useful as one part of a cancellation
 * scheme.  It may fail to remove tasks that have been converted
 * into other forms before being placed on the internal queue.
 * For example, a task entered using {@code submit} might be
 * converted into a form that maintains {@code Future} status.
 * However, in such cases, method {@link #purge} may be used to
 * remove those Futures that have been cancelled.
 *
 * @param task the task to remove
 * @return {@code true} if the task was removed
 */
public boolean remove(Runnable task) {
    boolean removed = workQueue.remove(task);
    tryTerminate(); // In case SHUTDOWN and now empty
    return removed;
}
```

##### çº¿ç¨‹æ± çš„å…³é—­

```java
/**
 * Initiates an orderly shutdown in which previously submitted
 * tasks are executed, but no new tasks will be accepted.
 * Invocation has no additional effect if already shut down.
 *
 * <p>This method does not wait for previously submitted tasks to
 * complete execution.  Use {@link #awaitTermination awaitTermination}
 * to do that.
 *
 * @throws SecurityException {@inheritDoc}
 */
public void shutdown() {
    final ReentrantLock mainLock = this.mainLock;
    mainLock.lock();
    try {
        checkShutdownAccess();
        advanceRunState(SHUTDOWN);
        interruptIdleWorkers();
        onShutdown(); // hook for ScheduledThreadPoolExecutor
    } finally {
        mainLock.unlock();
    }
    tryTerminate();
}
```

#### ğŸ“ThreadPoolExecutor çš„å°è£…ç±»

```java
/**
 * Creates a thread pool that reuses a fixed number of threads
 * operating off a shared unbounded queue.  At any point, at most
 * {@code nThreads} threads will be active processing tasks.
 * If additional tasks are submitted when all threads are active,
 * they will wait in the queue until a thread is available.
 * If any thread terminates due to a failure during execution
 * prior to shutdown, a new one will take its place if needed to
 * execute subsequent tasks.  The threads in the pool will exist
 * until it is explicitly {@link ExecutorService#shutdown shutdown}.
 *
 * @param nThreads the number of threads in the pool
 * @return the newly created thread pool
 * @throws IllegalArgumentException if {@code nThreads <= 0}
 */
public static ExecutorService newFixedThreadPool(int nThreads) {
    return new ThreadPoolExecutor(nThreads, nThreads,
                                  0L, TimeUnit.MILLISECONDS,
                                  new LinkedBlockingQueue<Runnable>());
}

 /**
 * Creates a thread pool that reuses a fixed number of threads
 * operating off a shared unbounded queue, using the provided
 * ThreadFactory to create new threads when needed.  At any point,
 * at most {@code nThreads} threads will be active processing
 * tasks.  If additional tasks are submitted when all threads are
 * active, they will wait in the queue until a thread is
 * available.  If any thread terminates due to a failure during
 * execution prior to shutdown, a new one will take its place if
 * needed to execute subsequent tasks.  The threads in the pool will
 * exist until it is explicitly {@link ExecutorService#shutdown
 * shutdown}.
 *
 * @param nThreads the number of threads in the pool
 * @param threadFactory the factory to use when creating new threads
 * @return the newly created thread pool
 * @throws NullPointerException if threadFactory is null
 * @throws IllegalArgumentException if {@code nThreads <= 0}
 */
public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) {
    return new ThreadPoolExecutor(nThreads, nThreads,
                                  0L, TimeUnit.MILLISECONDS,
                                  new LinkedBlockingQueue<Runnable>(),
                                  threadFactory);
}
```

```java
 /**
 * Creates a thread pool that creates new threads as needed, but
 * will reuse previously constructed threads when they are
 * available.  These pools will typically improve the performance
 * of programs that execute many short-lived asynchronous tasks.
 * Calls to {@code execute} will reuse previously constructed
 * threads if available. If no existing thread is available, a new
 * thread will be created and added to the pool. Threads that have
 * not been used for sixty seconds are terminated and removed from
 * the cache. Thus, a pool that remains idle for long enough will
 * not consume any resources. Note that pools with similar
 * properties but different details (for example, timeout parameters)
 * may be created using {@link ThreadPoolExecutor} constructors.
 *
 * @return the newly created thread pool
 */
public static ExecutorService newCachedThreadPool() {
    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                  60L, TimeUnit.SECONDS,
                                  new SynchronousQueue<Runnable>());
}

 /**
 * Creates a thread pool that creates new threads as needed, but
 * will reuse previously constructed threads when they are
 * available, and uses the provided
 * ThreadFactory to create new threads when needed.
 *
 * @param threadFactory the factory to use when creating new threads
 * @return the newly created thread pool
 * @throws NullPointerException if threadFactory is null
 */
public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) {
    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                  60L, TimeUnit.SECONDS,
                                  new SynchronousQueue<Runnable>(),
                                  threadFactory);
}
```

```java
/**
 * Creates an Executor that uses a single worker thread operating
 * off an unbounded queue. (Note however that if this single
 * thread terminates due to a failure during execution prior to
 * shutdown, a new one will take its place if needed to execute
 * subsequent tasks.)  Tasks are guaranteed to execute
 * sequentially, and no more than one task will be active at any
 * given time. Unlike the otherwise equivalent
 * {@code newFixedThreadPool(1)} the returned executor is
 * guaranteed not to be reconfigurable to use additional threads.
 *
 * @return the newly created single-threaded Executor
 */
public static ExecutorService newSingleThreadExecutor() {
    return new FinalizableDelegatedExecutorService
        (new ThreadPoolExecutor(1, 1,
                                0L, TimeUnit.MILLISECONDS,
                                new LinkedBlockingQueue<Runnable>()));
}

/**
 * Creates an Executor that uses a single worker thread operating
 * off an unbounded queue, and uses the provided ThreadFactory to
 * create a new thread when needed. Unlike the otherwise
 * equivalent {@code newFixedThreadPool(1, threadFactory)} the
 * returned executor is guaranteed not to be reconfigurable to use
 * additional threads.
 *
 * @param threadFactory the factory to use when creating new threads
 * @return the newly created single-threaded Executor
 * @throws NullPointerException if threadFactory is null
 */
public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) {
    return new FinalizableDelegatedExecutorService
        (new ThreadPoolExecutor(1, 1,
                                0L, TimeUnit.MILLISECONDS,
                                new LinkedBlockingQueue<Runnable>(),
                                threadFactory));
}
```

#### ğŸ“ThreadPoolExecutor çš„ç»§æ‰¿ç±»

```java
/**
A ThreadPoolExecutor that can additionally schedule commands to run after a given delay, or to execute periodically. This class is preferable to java.util.Timer when multiple worker threads are needed, or when the additional flexibility or capabilities of ThreadPoolExecutor (which this class extends) are required.

Delayed tasks execute no sooner than they are enabled, but without any real-time guarantees about when, after they are enabled, they will commence. Tasks scheduled for exactly the same execution time are enabled in first-in-first-out (FIFO) order of submission.

When a submitted task is cancelled before it is run, execution is suppressed. By default, such a cancelled task is not automatically removed from the work queue until its delay elapses. While this enables further inspection and monitoring, it may also cause unbounded retention of cancelled tasks. To avoid this, use setRemoveOnCancelPolicy to cause tasks to be immediately removed from the work queue at time of cancellation.
Successive executions of a periodic task scheduled via scheduleAtFixedRate or scheduleWithFixedDelay do not overlap. While different executions may be performed by different threads, the effects of prior executions happen-before those of subsequent ones.

While this class inherits from ThreadPoolExecutor, a few of the inherited tuning methods are not useful for it. In particular, because it acts as a fixed-sized pool using corePoolSize threads and an unbounded queue, adjustments to maximumPoolSize have no useful effect. Additionally, it is almost never a good idea to set corePoolSize to zero or use allowCoreThreadTimeOut because this may leave the pool without threads to handle tasks once they become eligible to run.

As with ThreadPoolExecutor, if not otherwise specified, this class uses Executors.defaultThreadFactory as the default thread factory, and ThreadPoolExecutor.AbortPolicy as the default rejected execution handler.
*/

public class ScheduledThreadPoolExecutor
        extends ThreadPoolExecutor
        implements ScheduledExecutorService {
    // ...
}


/**
 * Creates a thread pool that can schedule commands to run after a
 * given delay, or to execute periodically.
 * @param corePoolSize the number of threads to keep in the pool,
 * even if they are idle
 * @param threadFactory the factory to use when the executor
 * creates a new thread
 * @return the newly created scheduled thread pool
 * @throws IllegalArgumentException if {@code corePoolSize < 0}
 * @throws NullPointerException if threadFactory is null
 */
public static ScheduledExecutorService newScheduledThreadPool(
        int corePoolSize, ThreadFactory threadFactory) {
    return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory);
}
```

å…¶æ„é€ å‡½æ•°åŸºäºåŸºç±» ThreadPoolExecutor çš„æ„é€ å‡½æ•°ï¼Œä¸”å‡**æœªæä¾›æŒ‡å®šè‡ªå®šä¹‰ä»»åŠ¡é˜Ÿåˆ—çš„åŠŸèƒ½**ã€‚

```java
/**
 * Creates a new {@code ScheduledThreadPoolExecutor} with the
 * given core pool size.
 *
 * @param corePoolSize the number of threads to keep in the pool, even
 *        if they are idle, unless {@code allowCoreThreadTimeOut} is set
 * @throws IllegalArgumentException if {@code corePoolSize < 0}
 */
public ScheduledThreadPoolExecutor(int corePoolSize) {
    super(corePoolSize, Integer.MAX_VALUE,
          DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,
          new DelayedWorkQueue());
}

/**
 * Creates a new {@code ScheduledThreadPoolExecutor} with the
 * given initial parameters.
 *
 * @param corePoolSize the number of threads to keep in the pool, even
 *        if they are idle, unless {@code allowCoreThreadTimeOut} is set
 * @param threadFactory the factory to use when the executor
 *        creates a new thread
 * @throws IllegalArgumentException if {@code corePoolSize < 0}
 * @throws NullPointerException if {@code threadFactory} is null
 */
public ScheduledThreadPoolExecutor(int corePoolSize,
                                   ThreadFactory threadFactory) {
    super(corePoolSize, Integer.MAX_VALUE,
          DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,
          new DelayedWorkQueue(), threadFactory);
}

/**
 * Creates a new {@code ScheduledThreadPoolExecutor} with the
 * given initial parameters.
 *
 * @param corePoolSize the number of threads to keep in the pool, even
 *        if they are idle, unless {@code allowCoreThreadTimeOut} is set
 * @param handler the handler to use when execution is blocked
 *        because the thread bounds and queue capacities are reached
 * @throws IllegalArgumentException if {@code corePoolSize < 0}
 * @throws NullPointerException if {@code handler} is null
 */
public ScheduledThreadPoolExecutor(int corePoolSize,
                                   RejectedExecutionHandler handler) {
    super(corePoolSize, Integer.MAX_VALUE,
          DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,
          new DelayedWorkQueue(), handler);
}

/**
 * Creates a new {@code ScheduledThreadPoolExecutor} with the
 * given initial parameters.
 *
 * @param corePoolSize the number of threads to keep in the pool, even
 *        if they are idle, unless {@code allowCoreThreadTimeOut} is set
 * @param threadFactory the factory to use when the executor
 *        creates a new thread
 * @param handler the handler to use when execution is blocked
 *        because the thread bounds and queue capacities are reached
 * @throws IllegalArgumentException if {@code corePoolSize < 0}
 * @throws NullPointerException if {@code threadFactory} or
 *         {@code handler} is null
 */
public ScheduledThreadPoolExecutor(int corePoolSize,
                                   ThreadFactory threadFactory,
                                   RejectedExecutionHandler handler) {
    super(corePoolSize, Integer.MAX_VALUE,
          DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS,
          new DelayedWorkQueue(), threadFactory, handler);
}
```

### ğŸ–Šå¤šçº¿ç¨‹å®‰å…¨æ•°æ®ç»“æ„

#### å¤šçº¿ç¨‹å®‰å…¨å®¹å™¨ç±»

Java æä¾›äº†ä¸€äº›å®¹å™¨ç±»ï¼Œèƒ½ç¡®ä¿æ•°æ®åœ¨å¤šçº¿ç¨‹åœºæ™¯ä¸‹çš„å®‰å…¨

1. Hashtable

2. ConcurrentHashMap

3. Vector

4. CopyOnWriteArrayList

5. StringBuffer

#### âœˆConcurrentHashMap çš„å¹¶è¡ŒåŸç†

ä»ä¸‹åˆ—ä»£ç ä¸­å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œ**è¯»å–æ˜¯ä¸åŠ é”çš„**ï¼›è€Œä»å…³é”®ä»£ç 

> synchronized (f)

å¯è§‚å¯Ÿåˆ°**å†™å…¥æ˜¯åªé’ˆå¯¹è¦è¯»å†™çš„æ¡¶åŠ é”**ã€‚

```java
/** Implementation for put and putIfAbsent */
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh; K fk; V fv;
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null, new Node<K,V>(hash, key, value)))
                break;                   // no lock when adding to empty bin
        }
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else if (onlyIfAbsent // check first node without acquiring lock
                 && fh == hash
                 && ((fk = f.key) == key || (fk != null && key.equals(fk)))
                 && (fv = f.val) != null)
            return fv;
        else {
            V oldVal = null;
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    if (fh >= 0) {
                        binCount = 1;
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key, value);
                                break;
                            }
                        }
                    }
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                       value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                    else if (f instanceof ReservationNode)
                        throw new IllegalStateException("Recursive update");
                }
            }
            if (binCount != 0) {
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    addCount(1L, binCount);
    return null;
}

public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    int h = spread(key.hashCode());
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (e = tabAt(tab, (n - 1) & h)) != null) {
        if ((eh = e.hash) == h) {
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        while ((e = e.next) != null) {
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

è€Œä¸‹åˆ—æ˜¯ **HashTable** çš„å†™å…¥æ“ä½œä»£ç ï¼Œå¯ä»¥çœ‹å‡ºå†™å…¥æ˜¯é’ˆå¯¹æ•´ä¸ªå†™å…¥æ“ä½œåŠ é”ï¼Œè¿™æ˜¯ä½æ•ˆçš„ã€‚

```java
public synchronized V put(K key, V value) {
    // Make sure the value is not null
    if (value == null) {
        throw new NullPointerException();
    }

    // Makes sure the key is not already in the hashtable.
    Entry<?,?> tab[] = table;
    int hash = key.hashCode();
    int index = (hash & 0x7FFFFFFF) % tab.length;
    @SuppressWarnings("unchecked")
    Entry<K,V> entry = (Entry<K,V>)tab[index];
    for(; entry != null ; entry = entry.next) {
        if ((entry.hash == hash) && entry.key.equals(key)) {
            V old = entry.value;
            entry.value = value;
            return old;
        }
    }

    addEntry(hash, key, value, index);
    return null;
}
```

##### â“è¯»çœŸçš„ä¸éœ€è¦åŠ é”å—

è¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼Œæ—¢ç„¶è¯»æ˜¯ä¸åŠ é”çš„ï¼Œä¼šä¸ä¼šå‡ºé—®é¢˜å‘¢ï¼Ÿå…¶å®ä¸ä¼šï¼Œè§‚å¯Ÿä¸‹åˆ—ä»£ç å¯çŸ¥ï¼ŒConcurrentHashMap é‡å†™äº† Node å°è£…ç±»ï¼Œä½¿ç”¨ **volatile** ä¿®é¥° **valã€next** ï¼Œè¿™å°±ä¿è¯äº†è¯»å†™ä¸€è‡´æ€§ã€‚

```java
/* ---------------- Nodes -------------- */

/**
 * Key-value entry.  This class is never exported out as a
 * user-mutable Map.Entry (i.e., one supporting setValue; see
 * MapEntry below), but can be used for read-only traversals used
 * in bulk tasks.  Subclasses of Node with a negative hash field
 * are special, and contain null keys and values (but are never
 * exported).  Otherwise, keys and vals are never null.
 */
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    volatile V val;
    volatile Node<K,V> next;

    Node(int hash, K key, V val) {
        this.hash = hash;
        this.key = key;
        this.val = val;
    }

    Node(int hash, K key, V val, Node<K,V> next) {
        this(hash, key, val);
        this.next = next;
    }

    public final K getKey()     { return key; }
    public final V getValue()   { return val; }
    public final int hashCode() { return key.hashCode() ^ val.hashCode(); }
    public final String toString() {
        return Helpers.mapEntryToString(key, val);
    }
    public final V setValue(V value) {
        throw new UnsupportedOperationException();
    }

    public final boolean equals(Object o) {
        Object k, v, u; Map.Entry<?,?> e;
        return ((o instanceof Map.Entry) &&
                (k = (e = (Map.Entry<?,?>)o).getKey()) != null &&
                (v = e.getValue()) != null &&
                (k == key || k.equals(key)) &&
                (v == (u = val) || v.equals(u)));
    }

    /**
     * Virtualized support for map.get(); overridden in subclasses.
     */
    Node<K,V> find(int h, Object k) {
        Node<K,V> e = this;
        if (k != null) {
            do {
                K ek;
                if (e.hash == h &&
                    ((ek = e.key) == k || (ek != null && k.equals(ek))))
                    return e;
            } while ((e = e.next) != null);
        }
        return null;
    }
}
```

ä½†è¿™ä»…ä»…ä¸å¤Ÿï¼Œè¿™åªèƒ½ä¿è¯ *val ã€ next* åˆ†åˆ«æ˜¯ä¸€è‡´çš„ï¼Œè€Œä¸èƒ½ä¿è¯ *(val, next)*  æ•´ä½“æ˜¯ä¸€è‡´çš„ã€‚é‚£ä¹ˆåè€…æ˜¯å¦‚ä½•ä¿è¯çš„å‘¢ï¼Ÿ

ä»æºç ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºæ‹‰é“¾å†™å…¥ï¼Œæ˜¯é€šè¿‡**å°¾æ’** å®ç°çš„ï¼Œè€Œè¯»å–æ˜¯ä»æ‹‰é“¾**å¤´éƒ¨**å¼€å§‹éå†çš„ï¼Œè¿™è¡¨æ˜å†™å…¥å¹¶ä¸ä¼šå½±å“å‰é¢çš„æ‹‰é“¾ï¼Œä»è€Œä¸å½±å“è¯»å–ã€‚

å¯¹äºçº¢é»‘æ ‘è½¬æ¢ï¼Œå…ˆå»ºç«‹çº¢é»‘æ ‘å‰¯æœ¬ï¼Œç„¶åå°†æ ‘å¼•ç”¨æ›¿æ¢æ‹‰é“¾å¼•ç”¨ã€‚

## âœ¨å†…å­˜ç®¡ç†

### ğŸ“æ­å»º JDK æºç é˜…è¯»ç¯å¢ƒ

è¦æ›´å¥½åœ°ç†è§£ Java åº•å±‚ï¼Œéš¾ä»¥é¿å…åœ°æ¥è§¦åˆ° native æ–¹æ³•ã€hotspot åº•å±‚å®ç°ï¼Œè¿™æ—¶å€™ä¸å¾—ä¸æŸ¥çœ‹ JDK åº•å±‚æºç ã€‚*OpenJDK* æ˜¯å¼€æºçš„ï¼Œæ•…å¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥æºç è¿›è¡Œè§‚å¯Ÿã€‚

ä¸ºäº†æ›´å¥½åœ°é˜…è¯»æºç ï¼Œéš¾ä»¥é¿å…æ¥è§¦åˆ°æºç çš„ç¼–è¯‘ï¼Œå› ä¸ºå¦‚æœæ— æ³•ç¼–è¯‘æºç ï¼Œå¤§å¤šæ•°é˜…è¯»ç¯å¢ƒæ˜¯åŸºäº**ç¬¦å·æŸ¥æ‰¾**çš„æ–¹å¼è·³è½¬çš„ï¼Œè¿™å¹¶ä¸ååˆ†å‡†ç¡®ã€‚è€Œé€šè¿‡ç¼–è¯‘ï¼Œç”Ÿæˆ **ç¼–è¯‘æ•°æ®åº“ ( compile_commands.json )** æ–‡ä»¶å¯ä»¥æå¤§ç¨‹åº¦æ”¹å–„è¿™ä¸ªé—®é¢˜ã€‚

**OpenJDK** çš„ç†æƒ³ç¼–è¯‘ç¯å¢ƒæ˜¯ **Linux**ï¼Œç”±äºæºç é‡‡ç”¨ **Makefile** æœºåˆ¶ï¼Œæ•…å¯åŸºäº **Bear** ç”Ÿæˆ **compile_commands.json** æ–‡ä»¶ã€‚å¯é‡‡ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š

> bear -- ./configure  --disable-javac-server
> bear -- make

å®Œæˆç¼–è¯‘å¹¶ç”Ÿæˆ compile_commands.json

å¦å¤–ï¼Œç”±äºç¼–è¯‘ JDK éœ€è¦ç¼–è¯‘ JDK ä¸­çš„ Javaæºç ï¼Œæ•…éœ€è¦ä¸€ä¸ª**é¢„å…ˆè£…å¥½çš„JDK ( å¼•å¯¼JDK )** ,ä¸€èˆ¬å¯é€‰æ‹©ä¸ç¼–è¯‘ç›®æ ‡JDK ç‰ˆæœ¬ç›¸è¿‘ç”šè‡³ç›¸åŒçš„ JDK ç‰ˆæœ¬ã€‚

### ğŸ“Garbage collect æœºåˆ¶

Java å†…å­˜å›æ”¶æœºåˆ¶åŒ…å«é™æ€å†…å­˜å›æ”¶æœºåˆ¶ä¸åŠ¨æ€å†…å­˜å›æ”¶æœºåˆ¶ï¼Œå‰è€…åœ¨ç¼–è¯‘æœŸå³å¯ç¡®å®šï¼Œæ•…å›æ”¶ç­–ç•¥è¾ƒç®€å•ï¼Œåˆ†é…çš„å†…å­˜åœ¨**æ ˆ**ä¸Šï¼›åè€…è¾ƒä¸ºå¤æ‚ï¼Œå†…å­˜ä¸€èˆ¬åœ¨**å †**ä¸Šåˆ†é…ï¼Œéœ€è¦åœ¨è¿è¡Œæ—¶æœŸç¡®å®šå†…å­˜å›æ”¶æ—¶æœŸã€‚

*æ ˆä¸€èˆ¬æ˜¯çº¿ç¨‹ç‹¬å çš„ï¼Œå †æ˜¯è¿›ç¨‹ç‹¬å çš„ã€‚*

#### åƒåœ¾çš„æ£€æµ‹

Java æ˜¯åŸºäºå¼•ç”¨ä¾èµ–å…³ç³»æ¥ç¡®å®šå†…å­˜å¯¹è±¡æ˜¯å¦è¢«è§†ä¸ºåƒåœ¾çš„ï¼Œåœ¨ Java è¿è¡ŒæœŸï¼Œå¦‚æœæ²¿ç€ä»¥ **æ ¹å¯¹è±¡é›†åˆ** ä¸ºèµ·ç‚¹çš„å¼•ç”¨é“¾æœç´¢ï¼Œä¸”æœ€ç»ˆæ— æ³•è¢«æœç´¢åˆ°çš„å†…å­˜å¯¹è±¡å°†è¢«è§†ä¸ºåƒåœ¾ã€‚

**æ ¹å¯¹è±¡é›†åˆ **ä¸»è¦åŒ…å«ä¸‹åˆ—å…ƒç´ ï¼š

- åœ¨æ–¹æ³•ä¸­å±€éƒ¨å˜é‡åŒºçš„å¯¹è±¡çš„å¼•ç”¨
- åœ¨ Java æ“ä½œæ ˆä¸­çš„å¯¹è±¡å¼•ç”¨
- åœ¨å¸¸é‡æ± çš„å¯¹è±¡å¼•ç”¨
- åœ¨æœ¬åœ°æ–¹æ³•ä¸­æŒæœ‰çš„å¯¹è±¡å¼•ç”¨
- ç±»çš„ Class å¯¹è±¡

#### åŸºäºåˆ†ä»£çš„åƒåœ¾å›æ”¶ç®—æ³•

*Hotspot* ä¸»è¦ä½¿ç”¨çš„æ˜¯åŸºäºåˆ†ä»£çš„åƒåœ¾å›æ”¶ç®—æ³•ã€‚

æœ¬ç®—æ³•çš„è®¾è®¡æ€è·¯æ˜¯ï¼š

> æŒ‰å¯¹è±¡çš„å¯¿å‘½é•¿çŸ­æ¥åˆ†ç»„ï¼Œåˆ†ä¸ºå¹´è½»ä»£å’Œå¹´è€ä»£ã€‚æ–°åˆ›å»ºçš„å¯¹è±¡è¢«åˆ†ä¸ºå¹´è½»ä»£ï¼Œå¦‚æœå¯¹è±¡ç»è¿‡å‡ æ¬¡å›æ”¶åä»ç„¶å­˜æ´»ï¼Œé‚£ä¹ˆå†æŠŠè¿™ä¸ªå¯¹è±¡åˆ’åˆ†åˆ°å¹´è€ä»£ï¼Œä¸”é™ä½å¹´è€ä»£çš„æ”¶é›†é¢‘ç‡ï¼Œä»¥æ­¤å‡å°‘æ¯æ¬¡åƒåœ¾æ”¶é›†æ‰€éœ€è¦æ‰«æçš„å¯¹è±¡æ•°é‡ï¼Œä»è€Œæé«˜åƒåœ¾å›æ”¶æ•ˆç‡ã€‚

æ•…å¯ä»¥æŠŠå †åˆ†æˆè‹¥å¹²ä¸ª**å­å †**ï¼Œæ¯ä¸ªå­å †å¯¹åº”ä¸€ä¸ªå¹´é¾„ä»£ã€‚

> 1. Young åŒºåˆ†ä¸ºï¼š Eden åŒº + Survivor åŒºã€‚ è€Œ Survivor åŒº åˆ†ä¸º 1ï¼š1 çš„ From åŒº å’Œ To åŒºã€‚
> 2. Old åŒº
> 3. Perm åŒºã€‚æ­¤åŒºä¸»è¦å­˜æ”¾ç±»çš„ Class å¯¹è±¡ã€‚

**å›æ”¶ç­–ç•¥**å¦‚ä¸‹ï¼š

> 1. å½“ Eden åŒºæ»¡åè§¦å‘ **Minor GC** ï¼Œå°† Eden åŒºçš„å­˜æ´»å¯¹è±¡å¤åˆ¶åˆ° Survivor åŒºçš„å…¶ä¸­ä¸€ä¸ªå­åˆ†åŒºï¼Œä¸”å°†å¦ä¸€ä¸ªå­åˆ†åŒºçš„å¯¹è±¡ä¹Ÿç§»åˆ°è¯¥åˆ†åŒºä¸­ï¼Œä¿è¯ From åŒº å’Œ To åŒº å§‹ç»ˆæœ‰ä¸€ä¸ªä¸ºç©ºã€‚
> 2. å½“ Survivor åŒºå­˜ä¸ä¸‹ Minor gc åä»å­˜æ´»çš„å¯¹è±¡ï¼Œå°†å…¶ç§»åŠ¨åˆ° Old åŒºï¼›æˆ–è€… Survivor ä¸­è¶³å¤Ÿè€çš„å¯¹è±¡ï¼Œä¹Ÿç§»åŠ¨åˆ° Old åŒº
> 3. Old åŒºæ»¡åï¼Œè§¦å‘ Full gcï¼Œå›æ”¶æ•´ä¸ªå †çš„å†…å­˜ã€‚
> 4. Perm åŒº çš„åƒåœ¾å›æ”¶ä¹Ÿæ˜¯ç”± Full gc è§¦å‘çš„ã€‚

é’ˆå¯¹å †ä¸åŒçš„åŒºåŸŸï¼Œhotspot é‡‡ç”¨ä¸åŒçš„åƒåœ¾å›æ”¶ç­–ç•¥ï¼Œæˆªè‡³ **OpenJDK-21**ï¼Œæ”¯æŒ

> 1. epsilon
> 2. g1
> 3. parallel
> 4. serial
> 5. shenandoah
> 6. z

åƒåœ¾å›æ”¶ç®—æ³•ã€‚æ¥ä¸‹æ¥å°†è¯¦ç»†ä»‹ç»ã€‚

#### Java new è¯­æ³•çš„åº•å±‚å®ç°

##### æ€»ä½“æ­¥éª¤

å½“ä½¿ç”¨ new å…³é”®å­—åˆ›å»ºä¸€ä¸ªå¯¹è±¡æ—¶ï¼Œå°†è°ƒç”¨åº•å±‚å¦‚ä¸‹å®ç°ï¼š

```c++
// Allocation

JRT_ENTRY(void, InterpreterRuntime::_new(JavaThread* current, ConstantPool* pool, int index))
  Klass* k = pool->klass_at(index, CHECK); // ä»å¸¸é‡æ± æ‰¾åˆ°ç±»å‹
  InstanceKlass* klass = InstanceKlass::cast(k);

  // Make sure we are not instantiating an abstract klass
  klass->check_valid_for_instantiation(true, CHECK);

  // Make sure klass is initialized
  klass->initialize(CHECK);

  // At this point the class may not be fully initialized
  // because of recursive initialization. If it is fully
  // initialized & has_finalized is not set, we rewrite
  // it into its fast version (Note: no locking is needed
  // here since this is an atomic byte write and can be
  // done more than once).
  //
  // Note: In case of classes with has_finalized we don't
  //       rewrite since that saves us an extra check in
  //       the fast version which then would call the
  //       slow version anyway (and do a call back into
  //       Java).
  //       If we have a breakpoint, then we don't rewrite
  //       because the _breakpoint bytecode would be lost.
  oop obj = klass->allocate_instance(CHECK);
  current->set_vm_result(obj);
JRT_END
```

##### ç±»å‹åˆå§‹åŒ–

å…¶ä¸­ **Klass** ç±»å‹è¡¨æ˜è¦åˆ›å»ºçš„ Javaå¯¹è±¡ çš„ç±»å‹ï¼Œä»ä¸‹åˆ—ä»£ç ä¹Ÿå¯åˆ†æå¾—åˆ° Java æ”¯æŒçš„å¯¹è±¡ç±»å‹ã€‚JVM é¦–å…ˆä»å¸¸é‡æ± æ‰¾åˆ° è¯¥å¯¹è±¡çš„ç±»å‹ï¼Œä¸”è¿›è¡Œä¸€å®šçš„ç±»å‹è½¬æ¢ï¼Œå¹¶ç¡®ä¿ klass åˆå§‹åŒ–å®Œæˆ

```c++
class Klass : public Metadata {
  friend class VMStructs;
  friend class JVMCIVMStructs;
 public:
  // Klass Kinds for all subclasses of Klass
  enum KlassKind {
    InstanceKlassKind,
    InstanceRefKlassKind,
    InstanceMirrorKlassKind,
    InstanceClassLoaderKlassKind,
    InstanceStackChunkKlassKind,
    TypeArrayKlassKind,
    ObjArrayKlassKind,
    UnknownKlassKind
  };
  // ...
}
```

ç±»å‹åˆå§‹åŒ–ä»£ç å¦‚ä¸‹ï¼š

```c++
void InstanceKlass::initialize(TRAPS) {
  if (this->should_be_initialized()) {
    initialize_impl(CHECK);
    // Note: at this point the class may be initialized
    //       OR it may be in the state of being initialized
    //       in case of recursive initialization!
  } else {
    assert(is_initialized(), "sanity check");
  }
}
```

```c++
void InstanceKlass::initialize_impl(TRAPS) {
  HandleMark hm(THREAD);

  // Make sure klass is linked (verified) before initialization
  // A class could already be verified, since it has been reflected upon.
  link_class(CHECK);

  DTRACE_CLASSINIT_PROBE(required, -1);

  bool wait = false;
  bool throw_error = false;

  JavaThread* jt = THREAD;

  bool debug_logging_enabled = log_is_enabled(Debug, class, init);

  // refer to the JVM book page 47 for description of steps
  // Step 1
  {
    MonitorLocker ml(jt, _init_monitor);

    // Step 2
    while (is_being_initialized() && !is_init_thread(jt)) {
      if (debug_logging_enabled) {
        ResourceMark rm(jt);
        log_debug(class, init)("Thread \"%s\" waiting for initialization of %s by thread \"%s\"",
                               jt->name(), external_name(), init_thread_name());
      }

      wait = true;
      jt->set_class_to_be_initialized(this);
      ml.wait();
      jt->set_class_to_be_initialized(nullptr);
    }

    // Step 3
    if (is_being_initialized() && is_init_thread(jt)) {
      if (debug_logging_enabled) {
        ResourceMark rm(jt);
        log_debug(class, init)("Thread \"%s\" recursively initializing %s",
                               jt->name(), external_name());
      }
      DTRACE_CLASSINIT_PROBE_WAIT(recursive, -1, wait);
      return;
    }

    // Step 4
    if (is_initialized()) {
      if (debug_logging_enabled) {
        ResourceMark rm(jt);
        log_debug(class, init)("Thread \"%s\" found %s already initialized",
                               jt->name(), external_name());
      }
      DTRACE_CLASSINIT_PROBE_WAIT(concurrent, -1, wait);
      return;
    }

    // Step 5
    if (is_in_error_state()) {
      if (debug_logging_enabled) {
        ResourceMark rm(jt);
        log_debug(class, init)("Thread \"%s\" found %s is in error state",
                               jt->name(), external_name());
      }
      throw_error = true;
    } else {

      // Step 6
      set_init_state(being_initialized);
      set_init_thread(jt);
      if (debug_logging_enabled) {
        ResourceMark rm(jt);
        log_debug(class, init)("Thread \"%s\" is initializing %s",
                               jt->name(), external_name());
      }
    }
  }

  // Throw error outside lock
  if (throw_error) {
    DTRACE_CLASSINIT_PROBE_WAIT(erroneous, -1, wait);
    ResourceMark rm(THREAD);
    Handle cause(THREAD, get_initialization_error(THREAD));

    stringStream ss;
    ss.print("Could not initialize class %s", external_name());
    if (cause.is_null()) {
      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), ss.as_string());
    } else {
      THROW_MSG_CAUSE(vmSymbols::java_lang_NoClassDefFoundError(),
                      ss.as_string(), cause);
    }
  }

  // Step 7
  // Next, if C is a class rather than an interface, initialize it's super class and super
  // interfaces.
  if (!is_interface()) {
    Klass* super_klass = super();
    if (super_klass != nullptr && super_klass->should_be_initialized()) {
      super_klass->initialize(THREAD);
    }
    // If C implements any interface that declares a non-static, concrete method,
    // the initialization of C triggers initialization of its super interfaces.
    // Only need to recurse if has_nonstatic_concrete_methods which includes declaring and
    // having a superinterface that declares, non-static, concrete methods
    if (!HAS_PENDING_EXCEPTION && has_nonstatic_concrete_methods()) {
      initialize_super_interfaces(THREAD);
    }

    // If any exceptions, complete abruptly, throwing the same exception as above.
    if (HAS_PENDING_EXCEPTION) {
      Handle e(THREAD, PENDING_EXCEPTION);
      CLEAR_PENDING_EXCEPTION;
      {
        EXCEPTION_MARK;
        add_initialization_error(THREAD, e);
        // Locks object, set state, and notify all waiting threads
        set_initialization_state_and_notify(initialization_error, THREAD);
        CLEAR_PENDING_EXCEPTION;
      }
      DTRACE_CLASSINIT_PROBE_WAIT(super__failed, -1, wait);
      THROW_OOP(e());
    }
  }


  // Step 8
  {
    DTRACE_CLASSINIT_PROBE_WAIT(clinit, -1, wait);
    if (class_initializer() != nullptr) {
      // Timer includes any side effects of class initialization (resolution,
      // etc), but not recursive entry into call_class_initializer().
      PerfClassTraceTime timer(ClassLoader::perf_class_init_time(),
                               ClassLoader::perf_class_init_selftime(),
                               ClassLoader::perf_classes_inited(),
                               jt->get_thread_stat()->perf_recursion_counts_addr(),
                               jt->get_thread_stat()->perf_timers_addr(),
                               PerfClassTraceTime::CLASS_CLINIT);
      call_class_initializer(THREAD);
    } else {
      // The elapsed time is so small it's not worth counting.
      if (UsePerfData) {
        ClassLoader::perf_classes_inited()->inc();
      }
      call_class_initializer(THREAD);
    }
  }

  // Step 9
  if (!HAS_PENDING_EXCEPTION) {
    set_initialization_state_and_notify(fully_initialized, THREAD);
    debug_only(vtable().verify(tty, true);)
  }
  else {
    // Step 10 and 11
    Handle e(THREAD, PENDING_EXCEPTION);
    CLEAR_PENDING_EXCEPTION;
    // JVMTI has already reported the pending exception
    // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
    JvmtiExport::clear_detected_exception(jt);
    {
      EXCEPTION_MARK;
      add_initialization_error(THREAD, e);
      set_initialization_state_and_notify(initialization_error, THREAD);
      CLEAR_PENDING_EXCEPTION;   // ignore any exception thrown, class initialization error is thrown below
      // JVMTI has already reported the pending exception
      // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
      JvmtiExport::clear_detected_exception(jt);
    }
    DTRACE_CLASSINIT_PROBE_WAIT(error, -1, wait);
    if (e->is_a(vmClasses::Error_klass())) {
      THROW_OOP(e());
    } else {
      JavaCallArguments args(e);
      THROW_ARG(vmSymbols::java_lang_ExceptionInInitializerError(),
                vmSymbols::throwable_void_signature(),
                &args);
    }
  }
  DTRACE_CLASSINIT_PROBE_WAIT(end, -1, wait);
}
```

ä»æºä»£ç å¯è§‚å¯Ÿåˆ°ï¼Œç®—æ³•åˆ†ä¸º 11 æ­¥å®ç°ï¼š

> 1. åŠ é”ä»¥é¿å…å¤šçº¿ç¨‹å¹¶å‘åˆå§‹åŒ–
> 2. å¦‚æœå…¶ä»–çº¿ç¨‹æ­£åœ¨åˆå§‹åŒ–è¯¥ç±»å‹ï¼Œç­‰å¾…å…¶å®Œæˆå¹¶é€šçŸ¥ã€‚
> 3. è‹¥åˆå§‹åŒ–å·²å¼€å§‹ï¼Œåˆ™ç›´æ¥è¿”å›ã€‚æ­¤æ­¥ç”¨äºè§£å†³å¾ªç¯å¯¹è±¡å¼•ç”¨çš„é—®é¢˜ã€‚
> 4. è‹¥åˆå§‹åŒ–å·²å®Œæˆï¼Œç›´æ¥è¿”å›ã€‚
> 5. è‹¥åˆå§‹åŒ–å‘é€å¼‚å¸¸ï¼ŒæŠ¥é”™è¿”å›ã€‚
> 6. è®¾ç½®åˆå§‹åŒ–çŠ¶æ€ï¼Œè®¾ç½®æ‰§è¡Œåˆå§‹åŒ–çš„çº¿ç¨‹ä¸ºå½“å‰çº¿ç¨‹ã€‚
> 7. è‹¥å¯¹è±¡ç±»å‹éæ¥å£ç±»å‹ï¼Œåˆ™æ‰§è¡Œå…¶çˆ¶ç±»å‹ã€çˆ¶æ¥å£ç±»å‹çš„åˆå§‹åŒ–ã€‚
> 8. é€šè¿‡ call_class_initializer() æ‰§è¡Œå¯¹è±¡çš„é™æ€ä»£ç 
> 9. è‹¥åˆå§‹åŒ–è¿‡ç¨‹æ— å¼‚å¸¸ï¼Œé€šçŸ¥å…¶ä»–çº¿ç¨‹åˆå§‹åŒ–å·²å®Œæˆã€‚
> 10. è‹¥åˆå§‹åŒ–è¿‡ç¨‹å­˜åœ¨å¼‚å¸¸ï¼Œé€šçŸ¥å…¶ä»–çº¿ç¨‹åˆå§‹åŒ–å‘é€å¼‚å¸¸ã€‚

##### åœ¨å †ä¸Šåˆ›å»º  instanceOopDesc å¯¹è±¡

å½“å¯¹è±¡ç±»å‹åˆå§‹åŒ–å®Œæˆï¼Œåœ¨å †ä¸Šåˆ›å»º **instanceOopDesc å¯¹è±¡**ã€‚å…¶ä¸­ TRAPS å®šä¹‰å¦‚ä¸‹ï¼Œåœ¨æºç ä¸­ååˆ†å¸¸è§ã€‚

> \#define TRAPS  JavaThread* THREAD

```c++
instanceOop InstanceKlass::allocate_instance(TRAPS) {
  bool has_finalizer_flag = has_finalizer(); // Query before possible GC
  size_t size = size_helper();  // Query before forming handle.

  instanceOop i;

  i = (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);
  if (has_finalizer_flag && !RegisterFinalizersAtInit) {
    i = register_finalizer(i, CHECK_NULL);
  }
  return i;
}
```

```c++
// An instanceOop is an instance of a Java Class
// Evaluating "new HashTable()" will create an instanceOop.

class instanceOopDesc : public oopDesc {
 public:
  // aligned header size.
  static int header_size() { return sizeof(instanceOopDesc)/HeapWordSize; }

  // If compressed, the offset of the fields of the instance may not be aligned.
  static int base_offset_in_bytes() {
    return (UseCompressedClassPointers) ?
            klass_gap_offset_in_bytes() :
            sizeof(instanceOopDesc);

  }
};
```

instanceOopDesc ç»§æ‰¿è‡ª oopDescï¼Œåè€…å±æ€§å­—æ®µå¦‚ä¸‹ï¼š

```c++
class oopDesc {
  friend class VMStructs;
  friend class JVMCIVMStructs;
 private:
  volatile markWord _mark;
  union _metadata {
    Klass*      _klass;
    narrowKlass _compressed_klass;
  } _metadata;
```

å…¶ä¸­ markWord ç±»å‹è®°å½•äº†å¯¹è±¡çš„åŸºæœ¬ä¿¡æ¯å¦‚ hash å€¼ã€gc åˆ†ä»£å¹´é¾„ç­‰

```c++
class markWord {
 private:
  uintptr_t _value;

 public:
  // Constants
  static const int age_bits                       = 4;
  static const int lock_bits                      = 2;
  static const int first_unused_gap_bits          = 1;
  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;
  static const int hash_bits                      = max_hash_bits > 31 ? 31 : max_hash_bits;
  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);

  static const int lock_shift                     = 0;
  static const int age_shift                      = lock_bits + first_unused_gap_bits;
  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;

  static const uintptr_t lock_mask                = right_n_bits(lock_bits);
  static const uintptr_t lock_mask_in_place       = lock_mask << lock_shift;
  static const uintptr_t age_mask                 = right_n_bits(age_bits);
  static const uintptr_t age_mask_in_place        = age_mask << age_shift;
  static const uintptr_t hash_mask                = right_n_bits(hash_bits);
  static const uintptr_t hash_mask_in_place       = hash_mask << hash_shift;

  static const uintptr_t locked_value             = 0;
  static const uintptr_t unlocked_value           = 1;
  static const uintptr_t monitor_value            = 2;
  static const uintptr_t marked_value             = 3;

  static const uintptr_t no_hash                  = 0 ;  // no hash value assigned
  static const uintptr_t no_hash_in_place         = (uintptr_t)no_hash << hash_shift;
  static const uintptr_t no_lock_in_place         = unlocked_value;

  static const uint max_age                       = age_mask;
  // ...
  uint age() const { return (uint) mask_bits(value() >> age_shift, age_mask); }
  markWord set_age(uint v) const {
    assert((v & ~age_mask) == 0, "shouldn't overflow age field");
    return markWord((value() & ~age_mask_in_place) | ((v & age_mask) << age_shift));
  }
  markWord incr_age()      const { return age() == max_age ? markWord(_value) : set_age(age() + 1); }

  // hash operations
  intptr_t hash() const {
    return mask_bits(value() >> hash_shift, hash_mask);
  }
    
  // ...
}
```

```c++
inline intptr_t mask_bits (intptr_t  x, intptr_t m) { return x & m; }
#define nth_bit(n)        (((n) >= BitsPerWord) ? 0 : (OneBit << (n)))
#define right_n_bits(n)   (nth_bit(n) - 1)
```

ä»£ç ä¸­æ¶‰åŠ Java å¸¸é‡ç±»å‹çš„å¤§å°ï¼Œå®šä¹‰å¦‚ä¸‹ (åŸºäºä½è¿ç®—)ï¼š

```c++
const int LogBytesPerShort   = 1;
const int LogBytesPerInt     = 2;
#ifdef _LP64
const int LogBytesPerWord    = 3;
#else
const int LogBytesPerWord    = 2;
#endif
const int LogBytesPerLong    = 3;

const int BytesPerShort      = 1 << LogBytesPerShort;
const int BytesPerInt        = 1 << LogBytesPerInt;
const int BytesPerWord       = 1 << LogBytesPerWord;
const int BytesPerLong       = 1 << LogBytesPerLong;

const int LogBitsPerByte     = 3;
const int LogBitsPerShort    = LogBitsPerByte + LogBytesPerShort;
const int LogBitsPerInt      = LogBitsPerByte + LogBytesPerInt;
const int LogBitsPerWord     = LogBitsPerByte + LogBytesPerWord;
const int LogBitsPerLong     = LogBitsPerByte + LogBytesPerLong;

const int BitsPerByte        = 1 << LogBitsPerByte;
const int BitsPerShort       = 1 << LogBitsPerShort;
const int BitsPerInt         = 1 << LogBitsPerInt;
const int BitsPerWord        = 1 << LogBitsPerWord;
const int BitsPerLong        = 1 << LogBitsPerLong;
```

ä»æºç ä¸­è¿˜å¯çœ‹å‡ºï¼Œhash_shift ä¸ gc ä»£æ•°æœ‰å…³ï¼Œ*æ•…ç›¸åŒ value çš„ä¸åŒä»£çš„å¯¹è±¡ hashcode å¯èƒ½ä¸ä¸€è‡´*ã€‚

å¦å¤–å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šåœ¨å †ä¸Šåˆ†é…ç©ºé—´å‰ï¼Œè¿˜ä¼šæŸ¥è¯¢ç±»æ˜¯å¦å«æœ‰ finalize æ–¹æ³•ï¼Œå¦‚æœæœ‰ä¼šå°†å…¶æ³¨å†Œï¼Œæ‰§è¡Œ gc æ—¶ä¼šè°ƒç”¨ finalize æ–¹æ³•ã€‚

##### ğŸ“—åˆ†é…ç©ºé—´

æœ€åå°±æ˜¯åœ¨å †ä¸Šåˆ†é…åˆé€‚å¤§å°çš„ç©ºé—´ï¼Œå³

> i = (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);

å…¶ä¸­ **Universe** ç”¨äºä¿å­˜ JVM é‡è¦çš„ç³»ç»Ÿç±»ä¸å®ä¾‹ã€‚

```c++
class Universe: AllStatic {
  // Ugh.  Universe is much too friendly.
  friend class MarkSweep;
  friend class oopDesc;
  friend class ClassLoader;
  friend class SystemDictionary;
  friend class ReservedHeapSpace;
  friend class VMStructs;
  friend class VM_PopulateDumpSharedSpace;
  friend class Metaspace;
  friend class MetaspaceShared;
  friend class vmClasses;

  friend jint  universe_init();
  friend void  universe2_init();
  friend bool  universe_post_init();
  friend void  universe_post_module_init();

 private:
  // Known classes in the VM
  static Klass* _typeArrayKlassObjs[T_LONG+1];
  static Klass* _objectArrayKlassObj;
  // Special int-Array that represents filler objects that are used by GC to overwrite
  // dead objects. References to them are generally an error.
  static Klass* _fillerArrayKlassObj;

  // Known objects in the VM
  static OopHandle    _main_thread_group;             // Reference to the main thread group object
  static OopHandle    _system_thread_group;           // Reference to the system thread group object

  static OopHandle    _the_empty_class_array;         // Canonicalized obj array of type java.lang.Class
  static OopHandle    _the_null_string;               // A cache of "null" as a Java string
  static OopHandle    _the_min_jint_string;           // A cache of "-2147483648" as a Java string

  static OopHandle    _the_null_sentinel;             // A unique object pointer unused except as a sentinel for null.

  // preallocated error objects (no backtrace)
  static OopHandle    _out_of_memory_errors;
  static OopHandle    _class_init_stack_overflow_error;

  // preallocated cause message for delayed StackOverflowError
  static OopHandle    _delayed_stack_overflow_error_message;

  static LatestMethodCache* _finalizer_register_cache; // static method for registering finalizable objects
  static LatestMethodCache* _loader_addClass_cache;    // method for registering loaded classes in class loader vector
  static LatestMethodCache* _throw_illegal_access_error_cache; // Unsafe.throwIllegalAccessError() method
  static LatestMethodCache* _throw_no_such_method_error_cache; // Unsafe.throwNoSuchMethodError() method
  static LatestMethodCache* _do_stack_walk_cache;      // method for stack walker callback

  static Array<int>*            _the_empty_int_array;            // Canonicalized int array
  static Array<u2>*             _the_empty_short_array;          // Canonicalized short array
  static Array<Klass*>*         _the_empty_klass_array;          // Canonicalized klass array
  static Array<InstanceKlass*>* _the_empty_instance_klass_array; // Canonicalized instance klass array
  static Array<Method*>*        _the_empty_method_array;         // Canonicalized method array

  static Array<Klass*>*  _the_array_interfaces_array;

  // array of preallocated error objects with backtrace
  static OopHandle     _preallocated_out_of_memory_error_array;

  // number of preallocated error objects available for use
  static volatile jint _preallocated_out_of_memory_error_avail_count;

  // preallocated message detail strings for error objects
  static OopHandle _msg_metaspace;
  static OopHandle _msg_class_metaspace;

  static OopHandle    _null_ptr_exception_instance;   // preallocated exception object
  static OopHandle    _arithmetic_exception_instance; // preallocated exception object
  static OopHandle    _virtual_machine_error_instance; // preallocated exception object

  // References waiting to be transferred to the ReferenceHandler
  static OopHandle    _reference_pending_list;
  // ...
}
```

æ ¹æ® Universe ä¿å­˜çš„ Heap è°ƒç”¨ CollectedHeap ç±»çš„æ–¹æ³•å¦‚ä¸‹ï¼š

```c++
inline oop CollectedHeap::obj_allocate(Klass* klass, size_t size, TRAPS) {
  ObjAllocator allocator(klass, size, THREAD);
  return allocator.allocate();
}
```

ä¸Šè¿°ä»£ç ä¸­çš„ ObjAllocator ç®€å•åœ°å°è£…äº†å¯¹è±¡éœ€è¦æ‰§è¡Œçš„åˆå§‹åŒ–å‡½æ•°

```c++
class ObjAllocator: public MemAllocator {
public:
  ObjAllocator(Klass* klass, size_t word_size, Thread* thread = Thread::current())
    : MemAllocator(klass, word_size, thread) {}

  virtual oop initialize(HeapWord* mem) const;
};
```

###### MemAllocator::allocate

æ¥ä¸‹æ¥æ˜¯è°ƒç”¨ *MemAllocator::allocate* æ–¹æ³•

```c++
oop MemAllocator::allocate() const {
  oop obj = nullptr;
  {
    Allocation allocation(*this, &obj);
    HeapWord* mem = mem_allocate(allocation);
    if (mem != nullptr) {
      obj = initialize(mem);
    } else {
      // The unhandled oop detector will poison local variable obj,
      // so reset it to null if mem is null.
      obj = nullptr;
    }
  }
  return obj;
}

HeapWord* MemAllocator::mem_allocate(Allocation& allocation) const {
  if (UseTLAB) {
    // Try allocating from an existing TLAB.
    HeapWord* mem = mem_allocate_inside_tlab_fast();
    if (mem != nullptr) {
      return mem;
    }
  }

  return mem_allocate_slow(allocation);
}
```

###### tlab å¿«åˆ†é…

```c++
// å¿«é€Ÿ tlabï¼Œç›´æ¥åœ¨ tlab åˆ†é…
HeapWord* MemAllocator::mem_allocate_inside_tlab_fast() const {
  return _thread->tlab().allocate(_word_size);
}

inline HeapWord* ThreadLocalAllocBuffer::allocate(size_t size) {
  invariants();
  HeapWord* obj = top();
  if (pointer_delta(end(), obj) >= size) {
    // successful thread-local allocation
#ifdef ASSERT
    // Skip mangling the space corresponding to the object header to
    // ensure that the returned space is not considered parsable by
    // any concurrent GC thread.
    size_t hdr_size = oopDesc::header_size();
    Copy::fill_to_words(obj + hdr_size, size - hdr_size, badHeapWordVal);
#endif // ASSERT
    // This addition is safe because we know that top is
    // at least size below end, so the add can't wrap.
    set_top(obj + size);

    invariants();
    return obj;
  }
  return nullptr;
}
```

###### tlab æ…¢åˆ†é…

```c++
// æ…¢é€Ÿ tlabï¼Œå°è¯•ç”³è¯·æ–°çš„ tlab å†åˆ†é… æˆ– ç›´æ¥åœ¨ Eden åŒºåˆ†é…
HeapWord* MemAllocator::mem_allocate_slow(Allocation& allocation) const {
  // Allocation of an oop can always invoke a safepoint.
  debug_only(allocation._thread->check_for_valid_safepoint_state());

  if (UseTLAB) {
    // Try refilling the TLAB and allocating the object in it.
    HeapWord* mem = mem_allocate_inside_tlab_slow(allocation);
    if (mem != nullptr) {
      return mem;
    }
  }

  return mem_allocate_outside_tlab(allocation);
}

HeapWord* MemAllocator::mem_allocate_inside_tlab_slow(Allocation& allocation) const {
  HeapWord* mem = nullptr;
  ThreadLocalAllocBuffer& tlab = _thread->tlab();

  if (JvmtiExport::should_post_sampled_object_alloc()) {
    tlab.set_back_allocation_end();
    mem = tlab.allocate(_word_size);

    // We set back the allocation sample point to try to allocate this, reset it
    // when done.
    allocation._tlab_end_reset_for_sample = true;

    if (mem != nullptr) {
      return mem;
    }
  }

  // Retain tlab and allocate object in shared space if
  // the amount free in the tlab is too large to discard.
  if (tlab.free() > tlab.refill_waste_limit()) {
    tlab.record_slow_allocation(_word_size);
    return nullptr;
  }

  // Discard tlab and allocate a new one.
  // To minimize fragmentation, the last TLAB may be smaller than the rest.
  size_t new_tlab_size = tlab.compute_size(_word_size);

  tlab.retire_before_allocation();

  if (new_tlab_size == 0) {
    return nullptr;
  }

  // Allocate a new TLAB requesting new_tlab_size. Any size
  // between minimal and new_tlab_size is accepted.
  size_t min_tlab_size = ThreadLocalAllocBuffer::compute_min_size(_word_size);
  mem = Universe::heap()->allocate_new_tlab(min_tlab_size, new_tlab_size, &allocation._allocated_tlab_size);
  if (mem == nullptr) {
    assert(allocation._allocated_tlab_size == 0,
           "Allocation failed, but actual size was updated. min: " SIZE_FORMAT
           ", desired: " SIZE_FORMAT ", actual: " SIZE_FORMAT,
           min_tlab_size, new_tlab_size, allocation._allocated_tlab_size);
    return nullptr;
  }
  assert(allocation._allocated_tlab_size != 0, "Allocation succeeded but actual size not updated. mem at: "
         PTR_FORMAT " min: " SIZE_FORMAT ", desired: " SIZE_FORMAT,
         p2i(mem), min_tlab_size, new_tlab_size);

  if (ZeroTLAB) {
    // ..and clear it.
    Copy::zero_to_words(mem, allocation._allocated_tlab_size);
  } else {
    // ...and zap just allocated object.
#ifdef ASSERT
    // Skip mangling the space corresponding to the object header to
    // ensure that the returned space is not considered parsable by
    // any concurrent GC thread.
    size_t hdr_size = oopDesc::header_size();
    Copy::fill_to_words(mem + hdr_size, allocation._allocated_tlab_size - hdr_size, badHeapWordVal);
#endif // ASSERT
  }

  tlab.fill(mem, mem + _word_size, allocation._allocated_tlab_size);
  return mem;
}
```

###### tlab æ…¢åˆ†é…çš„ä¸¤è€…ç­–ç•¥

JVM å°½å¯èƒ½åœ°ä½¿ç”¨ **tlab** æœºåˆ¶ï¼Œè§‚å¯Ÿä»£ç å¯çŸ¥ï¼Œé’ˆå¯¹ tlab æ…¢åˆ†é…ï¼Œæœ‰ä¸¤ç§ç­–ç•¥ï¼Œè€Œå†³ç­–ä¾æ®æ˜¯ tlab çš„æœ€å¤§æµªè´¹ç©ºé—´

> tlab.refill_waste_limit()

###### ç›´æ¥åœ¨ Eden åŒºåˆ†é…

å¦‚æœé€‰æ‹©çš„æ˜¯ "æœ¬æ¬¡åˆ†é…ç›´æ¥åœ¨ Eden åŒºåˆ†é…ï¼Œä¿ç•™åŸæ¥ tlab" å°†è°ƒç”¨ä¸‹åˆ—æ ¸å¿ƒå‡½æ•°ï¼š

```c++
HeapWord* MemAllocator::mem_allocate_outside_tlab(Allocation& allocation) const {
  allocation._allocated_outside_tlab = true;
  HeapWord* mem = Universe::heap()->mem_allocate(_word_size, &allocation._overhead_limit_exceeded);
  if (mem == nullptr) {
    return mem;
  }

  size_t size_in_bytes = _word_size * HeapWordSize;
  _thread->incr_allocated_bytes(size_in_bytes);

  return mem;
}
```

å…¶ä¸­

> allocate_new_tlab  -> (æ…¢é€Ÿ tlab)
>
> mem_allocate -> (æœ€æ…¢ ç›´æ¥ä½¿ç”¨å †)

æ˜¯è™šå‡½æ•°ï¼Œå°†ä¸ CollectedHeap ç´§å¯†å…³è”ï¼Œä¸”ç”±å­ç±»æ”¹å†™ï¼Œæ˜¯ä¸åŒ å†…å­˜ç®¡ç†å™¨ çš„**å…¬å…±ç”³è¯·å†…å­˜æ¥å£**ã€‚

#### ğŸ‘€ThreadLocalAllocBuffer ( TLAB )

ä¸Šè¿°åˆ†ææ¶‰åŠäº† **tlab**ï¼Œæ¥ä¸‹æ¥å°†åˆ†æ tlab ä»¥åŠ æ·±ç†è§£ã€‚

##### TLAB ä½œç”¨

å¯¹äºå•çº¿ç¨‹åº”ç”¨ï¼Œæ¯æ¬¡åˆ†é…å†…å­˜ï¼Œä¼šè®°å½•ä¸Šæ¬¡åˆ†é…å¯¹è±¡å†…å­˜åœ°å€æœ«å°¾çš„æŒ‡é’ˆï¼Œä¹‹ååˆ†é…å¯¹è±¡ä¼š**ä»è¿™ä¸ªæŒ‡é’ˆå¼€å§‹æ£€ç´¢åˆ†é…**ã€‚è¿™ä¸ªæœºåˆ¶å«åš **bump-the-pointer** ï¼ˆæ’é’ˆï¼‰ã€‚

å¯¹äºå¤šçº¿ç¨‹åº”ç”¨æ¥è¯´ï¼Œå†…å­˜åˆ†é…éœ€è¦è€ƒè™‘çº¿ç¨‹å®‰å…¨ã€‚æœ€ç›´æ¥çš„æƒ³æ³•å°±æ˜¯é€šè¿‡å…¨å±€é”ï¼Œä½†æ˜¯è¿™ä¸ªæ€§èƒ½ä¼šå¾ˆå·®ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ä¸ªæ€§èƒ½ï¼Œæˆ‘ä»¬è€ƒè™‘å¯ä»¥æ¯ä¸ªçº¿ç¨‹åˆ†é…ä¸€ä¸ªçº¿ç¨‹æœ¬åœ°ç§æœ‰çš„å†…å­˜æ± ï¼Œç„¶åé‡‡ç”¨ bump-the-pointer æœºåˆ¶è¿›è¡Œå†…å­˜åˆ†é…ã€‚è¿™ä¸ªçº¿ç¨‹æœ¬åœ°ç§æœ‰çš„å†…å­˜æ± ï¼Œå°±æ˜¯ TLABã€‚åªæœ‰ TLAB æ»¡äº†ï¼Œå†å»ç”³è¯·å†…å­˜çš„æ—¶å€™ï¼Œéœ€è¦æ‰©å…… TLAB æˆ–è€…ä½¿ç”¨æ–°çš„ TLABï¼Œè¿™æ—¶å€™æ‰éœ€è¦é”ã€‚è¿™æ ·å¤§å¤§å‡å°‘äº†é”ä½¿ç”¨ã€‚


æ•… TLAB çš„ç›®çš„æ˜¯åœ¨ä¸ºæ–°å¯¹è±¡åˆ†é…å†…å­˜ç©ºé—´æ—¶ï¼Œè®©æ¯ä¸ª Java åº”ç”¨çº¿ç¨‹èƒ½åœ¨ä½¿ç”¨è‡ªå·±ä¸“å±çš„åˆ†é…æŒ‡é’ˆæ¥åˆ†é…ç©ºé—´ï¼Œå‡æ‘Šå¯¹GC å †ï¼ˆedenåŒºï¼‰é‡Œå…±äº«çš„åˆ†é…æŒ‡é’ˆåšæ›´æ–°è€Œå¸¦æ¥çš„åŒæ­¥å¼€é”€ã€‚

TLABåªæ˜¯è®©æ¯ä¸ªçº¿ç¨‹æœ‰ç§æœ‰çš„åˆ†é…æŒ‡é’ˆï¼Œä½†åº•ä¸‹å­˜å¯¹è±¡çš„å†…å­˜ç©ºé—´è¿˜æ˜¯ç»™æ‰€æœ‰çº¿ç¨‹è®¿é—®çš„ï¼Œåªæ˜¯å…¶å®ƒçº¿ç¨‹æ— æ³•åœ¨è¿™ä¸ªåŒºåŸŸåˆ†é…è€Œå·²ã€‚å½“ä¸€ä¸ªTLABç”¨æ»¡ï¼ˆåˆ†é…æŒ‡é’ˆtopæ’ä¸Šåˆ†é…æé™endäº†ï¼‰ï¼Œå°±æ–°ç”³è¯·ä¸€ä¸ªTLABï¼Œè€Œåœ¨è€TLABé‡Œçš„å¯¹è±¡è¿˜ç•™åœ¨åŸåœ°ä»€ä¹ˆéƒ½ä¸ç”¨ç®¡â€”â€”å®ƒä»¬æ— æ³•æ„ŸçŸ¥è‡ªå·±æ˜¯å¦æ˜¯æ›¾ç»ä»TLABåˆ†é…å‡ºæ¥çš„ï¼Œè€Œåªå…³å¿ƒè‡ªå·±æ˜¯åœ¨ eden é‡Œåˆ†é…çš„ã€‚

æ‰€ä»¥è¯´ï¼Œå› ä¸ºæœ‰äº† TLAB æŠ€æœ¯ï¼Œ**å †å†…å­˜å¹¶ä¸æ˜¯å®Œå®Œå…¨å…¨çš„çº¿ç¨‹å…±äº«ï¼Œå…¶ eden åŒºåŸŸä¸­è¿˜æ˜¯æœ‰ä¸€éƒ¨åˆ†ç©ºé—´æ˜¯åˆ†é…ç»™çº¿ç¨‹ç‹¬äº«çš„ã€‚**

##### TLAB ç”Ÿå‘½å‘¨æœŸ

åœ¨ TLAB å·²ç»æ»¡äº†æˆ–è€…æ¥è¿‘äºæ»¡äº†çš„æ—¶å€™ï¼ŒTLAB å¯èƒ½ä¼šè¢«é‡Šæ”¾å› Edenã€‚GC æ‰«æå¯¹è±¡å‘ç”Ÿæ—¶ï¼ŒTLAB ä¼šè¢«é‡Šæ”¾å› Edenã€‚TLAB çš„ç”Ÿå‘½å‘¨æœŸæœŸæœ›åªå­˜åœ¨äºä¸€ä¸ª GC æ‰«æå‘¨æœŸå†…ã€‚åœ¨ JVM ä¸­ï¼Œä¸€ä¸ª GC æ‰«æå‘¨æœŸï¼Œå°±æ˜¯ä¸€ä¸ªepochã€‚é‚£ä¹ˆï¼Œå¯ä»¥çŸ¥é“ï¼ŒTLAB å†…åˆ†é…å†…å­˜ä¸€å®šæ˜¯çº¿æ€§åˆ†é…çš„ã€‚

##### TLAB çš„ dummy å¡«å……

ç”±äº TLAB ä»…çº¿ç¨‹å†…çŸ¥é“å“ªäº›è¢«åˆ†é…äº†ï¼Œåœ¨ GC æ‰«æå‘ç”Ÿæ—¶è¿”å› Eden åŒºï¼Œå¦‚æœä¸å¡«å……çš„è¯ï¼Œå¤–éƒ¨å¹¶ä¸çŸ¥é“å“ªä¸€éƒ¨åˆ†è¢«ä½¿ç”¨å“ªä¸€éƒ¨åˆ†æ²¡æœ‰ï¼Œéœ€è¦åšé¢å¤–çš„æ£€æŸ¥ï¼Œå¦‚æœå¡«å……å·²ç»ç¡®è®¤ä¼šè¢«å›æ”¶çš„å¯¹è±¡ï¼Œä¹Ÿå°±æ˜¯ dummy objectï¼Œ GC ä¼šç›´æ¥æ ‡è®°ä¹‹åè·³è¿‡è¿™å—å†…å­˜ï¼Œæé«˜æ‰«ææ•ˆç‡ã€‚åæ­£è¿™å—å†…å­˜å·²ç»å±äº TLABï¼Œå…¶ä»–çº¿ç¨‹åœ¨ä¸‹æ¬¡æ‰«æç»“æŸå‰æ˜¯æ— æ³•ä½¿ç”¨çš„ã€‚è¿™ä¸ª dummy object å°±æ˜¯ int æ•°ç»„ã€‚

##### TLAB çš„ä¸¤ç§ç­–ç•¥

å½“éœ€è¦åˆ†é…çš„å†…å­˜å¯¹è±¡å¤§äº tlab çš„å‰©ä½™ç©ºé—´æ—¶ï¼Œæœ‰ä¸¤ç§ç­–ç•¥

> 1. å°†æœ¬å— TLAB æ”¾å› Edenï¼Œç”³è¯·å—æ–°çš„æ¥ç”¨  ( refill æ–¹æ¡ˆ)
>
> 2. å°†æœ¬æ¬¡éœ€è¦åˆ†é…çš„å¯¹è±¡ç›´æ¥æ”¾åœ¨ Edenï¼Œä¸‹æ¬¡ç»§ç»­ç”¨æœ¬å— tlab

é‚£ä¹ˆä½¿ç”¨å“ªç§ç­–ç•¥å‘¢ï¼Ÿæ˜¾ç„¶ç†æƒ³å›ç­”æ˜¯ï¼šå“ªä¸ªæ–¹æ¡ˆèƒ½ä½¿ TLAB æµªè´¹çš„ç©ºé—´å°½å¯èƒ½å°å°±é€‰å“ªç§ã€‚

> æ•…å¯è®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼šTLAB æµªè´¹ç©ºé—´ > é˜ˆå€¼ï¼Œåˆ™æµªè´¹å¤ªå¤šäº†ï¼Œç»§ç»­ç”¨ï¼Œæœ¬æ¬¡ç”³è¯·å¯¹è±¡ç›´æ¥æ”¾åœ¨ Eden åŒºã€‚å¦åˆ™é‡‡å–æ–¹æ¡ˆä¸€ã€‚

#### ğŸ—‘âœ¨CollectedHeap

æ¯ä¸ªåƒåœ¾å›æ”¶å™¨éƒ½ä¼šæŠ½è±¡å‡ºè‡ªå·±çš„å †ç»“æ„ï¼ŒåŒ…å«æœ€é‡è¦çš„å¯¹è±¡åˆ†é…å’Œåƒåœ¾å›æ”¶æ¥å£ã€‚**CollectedHeap** è¡¨ç¤ºå¯ç”¨äºåƒåœ¾å›æ”¶çš„ Java å †ï¼Œæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œæ˜¯åƒåœ¾å›æ”¶å™¨çš„**å…±åŒåŸºç±»**ï¼Œéƒ¨åˆ†å£°æ˜ä»£ç å¦‚ä¸‹ï¼š

```c++
class CollectedHeap : public CHeapObj<mtGC> {
  friend class VMStructs;
  friend class JVMCIVMStructs;
  friend class IsGCActiveMark; // Block structured external access to _is_gc_active
  friend class DisableIsGCActiveMark; // Disable current IsGCActiveMark
  friend class MemAllocator;
  friend class ParallelObjectIterator;
  
 protected:
    // Create a new tlab. All TLAB allocations must go through this.
  // To allow more flexible TLAB allocations min_size specifies
  // the minimum size needed, while requested_size is the requested
  // size based on ergonomics. The actually allocated size will be
  // returned in actual_size.
  virtual HeapWord* allocate_new_tlab(size_t min_size,
                                      size_t requested_size,
                                      size_t* actual_size) = 0;

  // Reinitialize tlabs before resuming mutators.
  virtual void resize_all_tlabs();

  // Raw memory allocation facilities
  // The obj and array allocate methods are covers for these methods.
  // mem_allocate() should never be
  // called to allocate TLABs, only individual objects.
  virtual HeapWord* mem_allocate(size_t size,
                                 bool* gc_overhead_limit_was_exceeded) = 0;
    
  virtual void trace_heap(GCWhen::Type when, const GCTracer* tracer);
    
 public:
  // ç›®å‰æ”¯æŒçš„ GC ç®—æ³•ç±»å‹
  enum Name {
    None,
    Serial,
    Parallel,
    G1,
    Epsilon,
    Z,
    Shenandoah
  };

 protected:
  // Get a pointer to the derived heap object.  Used to implement
  // derived class heap() functions rather than being called directly.
  template<typename T>
  static T* named_heap(Name kind) {
    CollectedHeap* heap = Universe::heap();
    assert(heap != nullptr, "Uninitialized heap");
    assert(kind == heap->kind(), "Heap kind %u should be %u",
           static_cast<uint>(heap->kind()), static_cast<uint>(kind));
    return static_cast<T*>(heap);
  }
  // ...
    
  // Perform a collection of the heap; intended for use in implementing
  // "System.gc".  This probably implies as full a collection as the
  // "CollectedHeap" supports.
  virtual void collect(GCCause::Cause cause) = 0;

  // Perform a full collection
  virtual void do_full_collection(bool clear_all_soft_refs) = 0;

  // Workers used in non-GC safepoints for parallel safepoint cleanup. If this
  // method returns null, cleanup tasks are done serially in the VMThread. See
  // `SafepointSynchronize::do_cleanup_tasks` for details.
  // GCs using a GC worker thread pool inside GC safepoints may opt to share
  // that pool with non-GC safepoints, avoiding creating extraneous threads.
  // Such sharing is safe, because GC safepoints and non-GC safepoints never
  // overlap. For example, `G1CollectedHeap::workers()` (for GC safepoints) and
  // `G1CollectedHeap::safepoint_workers()` (for non-GC safepoints) return the
  // same thread-pool.
  virtual WorkerThreads* safepoint_workers() { return nullptr; }
   // ...
}
```

#### ğŸ—‘EpsilonHeap

```c++
class EpsilonHeap : public CollectedHeap {
  friend class VMStructs;
private:
  EpsilonMonitoringSupport* _monitoring_support;
  MemoryPool* _pool;
  GCMemoryManager _memory_manager;
  ContiguousSpace* _space;
  VirtualSpace _virtual_space;
  size_t _max_tlab_size;
  size_t _step_counter_update;
  size_t _step_heap_print;
  int64_t _decay_time_ns;
  volatile size_t _last_counter_update;
  volatile size_t _last_heap_print;
    
public:
  static EpsilonHeap* heap();

  EpsilonHeap() :
          _memory_manager("Epsilon Heap"),
          _space(nullptr) {};

  Name kind() const override {
    return CollectedHeap::Epsilon;
  }

  const char* name() const override {
    return "Epsilon";
  }

  jint initialize() override;
  void initialize_serviceability() override;

  GrowableArray<GCMemoryManager*> memory_managers() override;
  GrowableArray<MemoryPool*> memory_pools() override;

  size_t max_capacity() const override { return _virtual_space.reserved_size();  }
  size_t capacity()     const override { return _virtual_space.committed_size(); }
  size_t used()         const override { return _space->used(); }

  bool is_in(const void* p) const override {
    return _space->is_in(p);
  }

  bool requires_barriers(stackChunkOop obj) const override { return false; }

  bool is_maximal_no_gc() const override {
    // No GC is going to happen. Return "we are at max", when we are about to fail.
    return used() == capacity();
  }
    
  // ...
}
```

##### ğŸ¸å†…å­˜åˆ†é…

###### 1ï¸âƒ£tlab æ…¢åˆ†é…

å…ˆå°è¯• tlab åˆ†é…

```c++
HeapWord* EpsilonHeap::allocate_new_tlab(size_t min_size,
                                         size_t requested_size,
                                         size_t* actual_size) {
  Thread* thread = Thread::current();

  // Defaults in case elastic paths are not taken
  bool fits = true;
  size_t size = requested_size;
  size_t ergo_tlab = requested_size;
  int64_t time = 0;

  if (EpsilonElasticTLAB) {
    ergo_tlab = EpsilonThreadLocalData::ergo_tlab_size(thread);

    if (EpsilonElasticTLABDecay) {
      int64_t last_time = EpsilonThreadLocalData::last_tlab_time(thread);
      time = (int64_t) os::javaTimeNanos();

      assert(last_time <= time, "time should be monotonic");

      // If the thread had not allocated recently, retract the ergonomic size.
      // This conserves memory when the thread had initial burst of allocations,
      // and then started allocating only sporadically.
      if (last_time != 0 && (time - last_time > _decay_time_ns)) {
        ergo_tlab = 0;
        EpsilonThreadLocalData::set_ergo_tlab_size(thread, 0);
      }
    }

    // If we can fit the allocation under current TLAB size, do so.
    // Otherwise, we want to elastically increase the TLAB size.
    fits = (requested_size <= ergo_tlab);
    if (!fits) {
      size = (size_t) (ergo_tlab * EpsilonTLABElasticity);
    }
  }

  // Always honor boundaries
  size = clamp(size, min_size, _max_tlab_size);

  // Always honor alignment
  size = align_up(size, MinObjAlignment);

  // Check that adjustments did not break local and global invariants
  assert(is_object_aligned(size),
         "Size honors object alignment: " SIZE_FORMAT, size);
  assert(min_size <= size,
         "Size honors min size: "  SIZE_FORMAT " <= " SIZE_FORMAT, min_size, size);
  assert(size <= _max_tlab_size,
         "Size honors max size: "  SIZE_FORMAT " <= " SIZE_FORMAT, size, _max_tlab_size);
  assert(size <= CollectedHeap::max_tlab_size(),
         "Size honors global max size: "  SIZE_FORMAT " <= " SIZE_FORMAT, size, CollectedHeap::max_tlab_size());

  if (log_is_enabled(Trace, gc)) {
    ResourceMark rm;
    log_trace(gc)("TLAB size for \"%s\" (Requested: " SIZE_FORMAT "K, Min: " SIZE_FORMAT
                          "K, Max: " SIZE_FORMAT "K, Ergo: " SIZE_FORMAT "K) -> " SIZE_FORMAT "K",
                  thread->name(),
                  requested_size * HeapWordSize / K,
                  min_size * HeapWordSize / K,
                  _max_tlab_size * HeapWordSize / K,
                  ergo_tlab * HeapWordSize / K,
                  size * HeapWordSize / K);
  }

  // All prepared, let's do it!
  HeapWord* res = allocate_work(size);

  if (res != nullptr) {
    // Allocation successful
    *actual_size = size;
    if (EpsilonElasticTLABDecay) {
      EpsilonThreadLocalData::set_last_tlab_time(thread, time);
    }
    if (EpsilonElasticTLAB && !fits) {
      // If we requested expansion, this is our new ergonomic TLAB size
      EpsilonThreadLocalData::set_ergo_tlab_size(thread, size);
    }
  } else {
    // Allocation failed, reset ergonomics to try and fit smaller TLABs
    if (EpsilonElasticTLAB) {
      EpsilonThreadLocalData::set_ergo_tlab_size(thread, 0);
    }
  }

  return res;
}
```

ä»ä»£ç å¯ä»¥çœ‹å‡ºï¼Œåˆ†é…ä¸ä»…ä»…æ˜¯ç®€å•çš„å¯¹è±¡å†…å­˜åˆ†é…ï¼ŒåŒæ—¶æ¶‰åŠ tlab çš„è°ƒæ•´ (è¾¹ç•Œæ‰©å±• ã€è¾¹ç•Œç¼©å‡)ã€‚

###### 2ï¸âƒ£ç›´æ¥ä½¿ç”¨å † Eden åŒº

å¦ä¸€ç§ç­–ç•¥æ˜¯ç›´æ¥åœ¨å †ä¸Š Eden åŒºåˆ†é…ã€‚

```c++
HeapWord* EpsilonHeap::mem_allocate(size_t size, bool *gc_overhead_limit_was_exceeded) {
  *gc_overhead_limit_was_exceeded = false;
  return allocate_work(size);
}
```

###### 3ï¸âƒ£åœ¨å¯ç”¨ç©ºé—´ä¸Šåˆ†é…å†…å­˜å¯¹è±¡

æ— è®ºæ˜¯å¦ä½¿ç”¨ tlab æœºåˆ¶ï¼Œæœ€åçš„åˆ†é…æ ¸å¿ƒå‡½æ•° ( åœ¨å¯ç”¨ç©ºé—´ä¸Šåˆ†é…å†…å­˜å¯¹è±¡ )éƒ½æ˜¯å¦‚ä¸‹ï¼š

```c++
HeapWord* EpsilonHeap::allocate_work(size_t size, bool verbose) {
  assert(is_object_aligned(size), "Allocation size should be aligned: " SIZE_FORMAT, size);

  HeapWord* res = nullptr;
  while (true) {
    // Try to allocate, assume space is available
    res = _space->par_allocate(size);
    if (res != nullptr) {
      break;
    }

    // Allocation failed, attempt expansion, and retry:
    {
      MutexLocker ml(Heap_lock);

      // Try to allocate under the lock, assume another thread was able to expand
      res = _space->par_allocate(size);
      if (res != nullptr) {
        break;
      }

      // Expand and loop back if space is available
      size_t size_in_bytes = size * HeapWordSize;
      size_t uncommitted_space = max_capacity() - capacity();
      size_t unused_space = max_capacity() - used();
      size_t want_space = MAX2(size_in_bytes, EpsilonMinHeapExpand);
      assert(unused_space >= uncommitted_space,
             "Unused (" SIZE_FORMAT ") >= uncommitted (" SIZE_FORMAT ")",
             unused_space, uncommitted_space);

      if (want_space < uncommitted_space) {
        // Enough space to expand in bulk:
        bool expand = _virtual_space.expand_by(want_space);
        assert(expand, "Should be able to expand");
      } else if (size_in_bytes < unused_space) {
        // No space to expand in bulk, and this allocation is still possible,
        // take all the remaining space:
        bool expand = _virtual_space.expand_by(uncommitted_space);
        assert(expand, "Should be able to expand");
      } else {
        // No space left:
        return nullptr;
      }

      _space->set_end((HeapWord *) _virtual_space.high());
    }
  }

  size_t used = _space->used();

  // Allocation successful, update counters
  if (verbose) {
    size_t last = _last_counter_update;
    if ((used - last >= _step_counter_update) && Atomic::cmpxchg(&_last_counter_update, last, used) == last) {
      _monitoring_support->update_counters();
    }
  }

  // ...and print the occupancy line, if needed
  if (verbose) {
    size_t last = _last_heap_print;
    if ((used - last >= _step_heap_print) && Atomic::cmpxchg(&_last_heap_print, last, used) == last) {
      print_heap_info(used);
      print_metaspace_info();
    }
  }

  assert(is_object_aligned(res), "Object should be aligned: " PTR_FORMAT, p2i(res));
  return res;
}

// Lock-free.
HeapWord* ContiguousSpace::par_allocate(size_t size) {
  return par_allocate_impl(size);
}

// This version is lock-free.
inline HeapWord* ContiguousSpace::par_allocate_impl(size_t size) {
  do {
    HeapWord* obj = top();
    if (pointer_delta(end(), obj) >= size) {
      HeapWord* new_top = obj + size;
      HeapWord* result = Atomic::cmpxchg(top_addr(), obj, new_top);
      // å‡½æ•°åŠŸèƒ½æ˜¯ï¼šå°† obj ä¸ top_addr æ¯”è¾ƒï¼Œè‹¥ç›¸åŒï¼Œå°† new_top å†™å…¥ top_addr å¹¶è¿”å› old å€¼ï¼›è‹¥ä¸åŒè¿”å› top_addr çš„å€¼ (Java CASâ€”â€”Compare and swap æœºåˆ¶ä¹Ÿä¾èµ–æ­¤å®ç°)
      // result can be one of two:
      //  the old top value: the exchange succeeded
      //  otherwise: the new value of the top is returned.
      if (result == obj) {
        assert(is_object_aligned(obj) && is_object_aligned(new_top), "checking alignment");
        return obj;
      }
    } else {
      return nullptr;
    }
  } while (true);
}
```

å…¶ä¸­ HeapWordImpl å¯ç†è§£ä¸ºä¸€ä¸ªå †æŒ‡é’ˆï¼š

```c++
// An opaque type, so that HeapWord* can be a generic pointer into the heap.
// We require that object sizes be measured in units of heap words (e.g.
// pointer-sized values), so that given HeapWord* hw,
//   hw += oop(hw)->foo();
// works, where foo is a method (like size or scavenge) that returns the
// object size.
class HeapWordImpl;             // Opaque, never defined.
typedef HeapWordImpl* HeapWord;
```

###### å…³äº CAS ( Compare and swap )

å…¶ä¸­ *Atomic::cmpxchg* æ˜¯ Java CAS æœºåˆ¶çš„é‡è¦æ”¯æ’‘ï¼Œå…¶é€šè¿‡æ±‡ç¼–å®ç°ï¼š

```c++
template<typename D, typename U, typename T>
inline D Atomic::cmpxchg(D volatile* dest,
                         U compare_value,
                         T exchange_value,
                         atomic_memory_order order) {
  return CmpxchgImpl<D, U, T>()(dest, compare_value, exchange_value, order);
}
```

ä»¥ä¸‹æ˜¯ x86 ä¸‹ Linux çš„ä»£ç æ ¸å¿ƒå®ç°

```c++
template<>
template<typename T>
inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,
                                                T compare_value,
                                                T exchange_value,
                                                atomic_memory_order /* order */) const {
  STATIC_ASSERT(4 == sizeof(T));
  __asm__ volatile ("lock cmpxchgl %1,(%3)"
                    : "=a" (exchange_value)
                    : "r" (exchange_value), "a" (compare_value), "r" (dest)
                    : "cc", "memory");
  return exchange_value;
}
```

> æ±‡ç¼–æŒ‡ä»¤çš„è§£é‡Šå¦‚ä¸‹ï¼š
>
> 1. å‰ä¸‰è¡Œåˆ†åˆ«è¡¨ç¤ºï¼š**æ±‡ç¼–æŒ‡ä»¤**ã€**è¾“å‡ºåˆ—è¡¨**ã€**è¾“å…¥åˆ—è¡¨**
> 2. è¾“å‡ºåˆ—è¡¨è¡¨ç¤ºæŒ‡ä»¤ç»“æŸå EAX çš„å€¼ é€šè¿‡ exchange_value è¿”å›ã€‚
> 3. "**r**" è¡¨ç¤ºå¯„å­˜å™¨ï¼Œä»£è¡¨éšæœºä¸€ä¸ªå¯ç”¨å¯„å­˜å™¨æ¥å­˜å‚¨å€¼ï¼Œ"**a**" è¡¨ç¤º EAX å¯„å­˜å™¨ã€‚
> 4. æ±‡ç¼–æŒ‡ä»¤çš„ **%n** è¡¨ç¤ºè¾“å…¥åˆ—è¡¨ä¸­çš„ç¬¬ n ä¸ªæ“ä½œæ•° (ä» **1** å¼€å§‹)ã€‚

ä»ç”³è¯·è¿‡ç¨‹å¯ä»¥çœ‹å‡ºï¼Œå¦‚æœç”³è¯·å¤±è´¥ï¼Œå°†å°è¯•æ‰©å®¹å†ç”³è¯· ( æ‰©å®¹è¿‡ç¨‹å°†ç»™å †åŠ é”ï¼Œè¿™ä¸ªè¿‡ç¨‹è¦è€ƒè™‘åŒæ­¥é—®é¢˜ï¼Œå³è·å¾—é”åè¦å†æ¬¡éªŒè¯å…¶ä»–çº¿ç¨‹æ˜¯å¦å·²ç»å®Œæˆæ‰©å®¹ )ï¼Œæ‰©å®¹åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼š

> 1. åˆ†é…æƒ³è¦çš„å¤§å°ã€‚
> 2. æƒ³è¦çš„å¤§å°å¤§äºå‰©ä½™å¯ç”¨ç©ºé—´ï¼Œå°†å‰©ä½™çš„æ‰€æœ‰å¯ç”¨ç©ºé—´è¿›è¡Œåˆ†é…ã€‚

æ‰©å®¹æˆåŠŸåˆ™è®¾ç½® å † çš„ **end** æŒ‡é’ˆã€‚

ç”³è¯·æˆåŠŸåå°†æŒ‰éœ€è¿›è¡Œæ—¥å¿—è¾“å‡ºã€‚

##### ğŸ¸åƒåœ¾å›æ”¶

```c++
void EpsilonHeap::collect(GCCause::Cause cause) {
  switch (cause) {
    case GCCause::_metadata_GC_threshold:
    case GCCause::_metadata_GC_clear_soft_refs:
      // Receiving these causes means the VM itself entered the safepoint for metadata collection.
      // While Epsilon does not do GC, it has to perform sizing adjustments, otherwise we would
      // re-enter the safepoint again very soon.

      assert(SafepointSynchronize::is_at_safepoint(), "Expected at safepoint");
      log_info(gc)("GC request for \"%s\" is handled", GCCause::to_string(cause));
      MetaspaceGC::compute_new_size();
      print_metaspace_info();
      break;
    default:
      log_info(gc)("GC request for \"%s\" is ignored", GCCause::to_string(cause));
  }
  _monitoring_support->update_counters();
}

void EpsilonHeap::do_full_collection(bool clear_all_soft_refs) {
  collect(gc_cause());
}
```

ä»æºç ä¸­å¯ä»¥çœ‹å‡ºï¼Œè¯¥ç®—æ³•**å¹¶ä¸å›æ”¶åƒåœ¾**ï¼Œä»…æ˜¯ç®€å•åœ°è®°å½•æ—¥å¿—ä¿¡æ¯ä¸è®¡ç®—æ–°çš„ gc æ°´å‡†çº¿ã€‚

#### ğŸ—‘SerialHeap

##### ğŸ¸åˆ†ä»£ç±»å‹

```c++
class SerialHeap : public CollectedHeap {
  friend class Generation;
  friend class DefNewGeneration;
  friend class TenuredGeneration;
  friend class GenMarkSweep;
  friend class VM_GenCollectForAllocation;
  friend class VM_GenCollectFull;
  friend class VM_GC_HeapInspection;
  friend class VM_HeapDumper;
  friend class HeapInspection;
  friend class GCCauseSetter;
  friend class VMStructs;
public:
  friend class VM_PopulateDumpSharedSpace;

  enum GenerationType {
    YoungGen,
    OldGen
  };

private:
  DefNewGeneration* _young_gen;
  TenuredGeneration* _old_gen;
  // ...
}
```

```c++
// DefNewGeneration is a young generation containing eden, from- and
// to-space.

class DefNewGeneration: public Generation {
  friend class VMStructs;
  // ...
}
```

```c++
// TenuredGeneration models the heap containing old (promoted/tenured) objects
// contained in a single contiguous space. This generation is covered by a card
// table, and uses a card-size block-offset array to implement block_start.
// Garbage collection is performed using mark-compact.

class TenuredGeneration: public Generation {
  friend class VMStructs;
  // Abstractly, this is a subtype that gets access to protected fields.
  friend class VM_PopulateDumpSharedSpace;

  MemRegion _prev_used_region;
  // ...
}
```

ä»æºç å¯ä»¥çœ‹å‡ºï¼Œæ”¯æŒå¹´è½»ä»£ä¸è€å¹´ä»£ã€‚

##### ğŸ¸å†…å­˜åˆ†é…

###### TLAB æ…¢åˆ†é…

å°è¯•åœ¨ Young åŒºåˆ†é… tlab

```c++
HeapWord* SerialHeap::allocate_new_tlab(size_t min_size,
                                        size_t requested_size,
                                        size_t* actual_size) {
  HeapWord* result = mem_allocate_work(requested_size /* size */,
                                       true /* is_tlab */);
  if (result != nullptr) {
    *actual_size = requested_size;
  }

  return result;
}
```

```c++
HeapWord* SerialHeap::mem_allocate_work(size_t size,
                                        bool is_tlab) {

  HeapWord* result = nullptr;

  // Loop until the allocation is satisfied, or unsatisfied after GC.
  for (uint try_count = 1, gclocker_stalled_count = 0; /* return or throw */; try_count += 1) {

    // First allocation attempt is lock-free.
    Generation *young = _young_gen;
    if (young->should_allocate(size, is_tlab)) {
      result = young->par_allocate(size, is_tlab);
      if (result != nullptr) {
        assert(is_in_reserved(result), "result not in heap");
        return result;
      }
    }
    uint gc_count_before;  // Read inside the Heap_lock locked region.
    {
      MutexLocker ml(Heap_lock);
      log_trace(gc, alloc)("SerialHeap::mem_allocate_work: attempting locked slow path allocation");
      // Note that only large objects get a shot at being
      // allocated in later generations.
      // ä»…å¤§å¯¹è±¡æœ‰æœºä¼šåœ¨ å¹´è€ä»£ åˆ†é…
      bool first_only = !should_try_older_generation_allocation(size);

      result = attempt_allocation(size, is_tlab, first_only);
      if (result != nullptr) {
        assert(is_in_reserved(result), "result not in heap");
        return result;
      }

      if (GCLocker::is_active_and_needs_gc()) {
        if (is_tlab) {
          return nullptr;  // Caller will retry allocating individual object.
        }
        if (!is_maximal_no_gc()) {
          // Try and expand heap to satisfy request.
          result = expand_heap_and_allocate(size, is_tlab);
          // Result could be null if we are out of space.
          if (result != nullptr) {
            return result;
          }
        }

        if (gclocker_stalled_count > GCLockerRetryAllocationCount) {
          return nullptr; // We didn't get to do a GC and we didn't get any memory.
        }

        // If this thread is not in a jni critical section, we stall
        // the requestor until the critical section has cleared and
        // GC allowed. When the critical section clears, a GC is
        // initiated by the last thread exiting the critical section; so
        // we retry the allocation sequence from the beginning of the loop,
        // rather than causing more, now probably unnecessary, GC attempts.
        JavaThread* jthr = JavaThread::current();
        if (!jthr->in_critical()) {
          MutexUnlocker mul(Heap_lock);
          // Wait for JNI critical section to be exited
          GCLocker::stall_until_clear();
          gclocker_stalled_count += 1;
          continue;
        } else {
          if (CheckJNICalls) {
            fatal("Possible deadlock due to allocating while"
                  " in jni critical section");
          }
          return nullptr;
        }
      }

      // Read the gc count while the heap lock is held.
      gc_count_before = total_collections();
    }

    VM_GenCollectForAllocation op(size, is_tlab, gc_count_before);
    VMThread::execute(&op);
    if (op.prologue_succeeded()) {
      result = op.result();
      if (op.gc_locked()) {
         assert(result == nullptr, "must be null if gc_locked() is true");
         continue;  // Retry and/or stall as necessary.
      }

      assert(result == nullptr || is_in_reserved(result),
             "result not in heap");
      return result;
    }

    // Give a warning if we seem to be looping forever.
    if ((QueuedAllocationWarningCount > 0) &&
        (try_count % QueuedAllocationWarningCount == 0)) {
          log_warning(gc, ergo)("SerialHeap::mem_allocate_work retries %d times,"
                                " size=" SIZE_FORMAT " %s", try_count, size, is_tlab ? "(TLAB)" : "");
    }
  }
}
```

åœ¨åˆ†é…è¿‡ç¨‹ä¸­ä¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨ Old åŒºè¿›è¡Œåˆ†é…

```c++
// Return true if any of the following is true:
// . the allocation won't fit into the current young gen heap
// . gc locker is occupied (jni critical section)
// . heap memory is tight -- the most recent previous collection
//   was a full collection because a partial collection (would
//   have) failed and is likely to fail again
bool SerialHeap::should_try_older_generation_allocation(size_t word_size) const {
  size_t young_capacity = _young_gen->capacity_before_gc();
  return    (word_size > heap_word_size(young_capacity))
         || GCLocker::is_active_and_needs_gc()
         || incremental_collection_failed();
}
```

```c++
HeapWord* SerialHeap::attempt_allocation(size_t size,
                                         bool is_tlab,
                                         bool first_only) {
  // first_only è¡¨æ˜æ˜¯å¦åªåœ¨ å¹´è½»ä»£ åˆ†é…
  HeapWord* res = nullptr;

  if (_young_gen->should_allocate(size, is_tlab)) {
    res = _young_gen->allocate(size, is_tlab);
    if (res != nullptr || first_only) {
      return res;
    }
  }

  if (_old_gen->should_allocate(size, is_tlab)) {
    res = _old_gen->allocate(size, is_tlab);
  }

  return res;
}
```

###### ç›´æ¥åœ¨ Young åŒºåˆ†é…

```c++
HeapWord* SerialHeap::mem_allocate(size_t size,
                                   bool* gc_overhead_limit_was_exceeded) {
  return mem_allocate_work(size,
                           false /* is_tlab */);
}
```

##### âœ¨Stop the World

```c++
class SafepointSynchronize : AllStatic {
 public:
  enum SynchronizeState {
      _not_synchronized = 0,                   // Threads not synchronized at a safepoint. Keep this value 0.
      _synchronizing    = 1,                   // Synchronizing in progress
      _synchronized     = 2                    // All Java threads are running in native, blocked in OS or stopped at safepoint.
                                               // VM thread and any NonJavaThread may be running.
  };
   // ...
}
```

```c++
// Roll all threads forward to a safepoint and suspend them all
void SafepointSynchronize::begin() {
  assert(Thread::current()->is_VM_thread(), "Only VM thread may execute a safepoint");

  EventSafepointBegin begin_event;
  SafepointTracing::begin(VMThread::vm_op_type());

  Universe::heap()->safepoint_synchronize_begin();

  // By getting the Threads_lock, we assure that no threads are about to start or
  // exit. It is released again in SafepointSynchronize::end().
  Threads_lock->lock();

  assert( _state == _not_synchronized, "trying to safepoint synchronize with wrong state");

  int nof_threads = Threads::number_of_threads();

  _nof_threads_hit_polling_page = 0;

  log_debug(safepoint)("Safepoint synchronization initiated using %s wait barrier. (%d threads)", _wait_barrier->description(), nof_threads);

  // Reset the count of active JNI critical threads
  _current_jni_active_count = 0;

  // Set number of threads to wait for
  _waiting_to_block = nof_threads;

  jlong safepoint_limit_time = 0;
  if (SafepointTimeout) {
    // Set the limit time, so that it can be compared to see if this has taken
    // too long to complete.
    safepoint_limit_time = SafepointTracing::start_of_safepoint() + (jlong)(SafepointTimeoutDelay * NANOSECS_PER_MILLISEC);
    timeout_error_printed = false;
  }

  EventSafepointStateSynchronization sync_event;
  int initial_running = 0;

  // Arms the safepoint, _current_jni_active_count and _waiting_to_block must be set before.
  arm_safepoint();

  // Will spin until all threads are safe.
  int iterations = synchronize_threads(safepoint_limit_time, nof_threads, &initial_running);
  assert(_waiting_to_block == 0, "No thread should be running");

#ifndef PRODUCT
  // Mark all threads
  if (VerifyCrossModifyFence) {
    JavaThreadIteratorWithHandle jtiwh;
    for (; JavaThread *cur = jtiwh.next(); ) {
      cur->set_requires_cross_modify_fence(true);
    }
  }

  if (safepoint_limit_time != 0) {
    jlong current_time = os::javaTimeNanos();
    if (safepoint_limit_time < current_time) {
      log_warning(safepoint)("# SafepointSynchronize: Finished after "
                    INT64_FORMAT_W(6) " ms",
                    (int64_t)(current_time - SafepointTracing::start_of_safepoint()) / (NANOUNITS / MILLIUNITS));
    }
  }
#endif

  assert(Threads_lock->owned_by_self(), "must hold Threads_lock");

  // Record state
  _state = _synchronized;

  OrderAccess::fence();

  // Set the new id
  ++_safepoint_id;

#ifdef ASSERT
  // Make sure all the threads were visited.
  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *cur = jtiwh.next(); ) {
    assert(cur->was_visited_for_critical_count(_safepoint_counter), "missed a thread");
  }
#endif // ASSERT

  // Update the count of active JNI critical regions
  GCLocker::set_jni_lock_count(_current_jni_active_count);

  post_safepoint_synchronize_event(sync_event,
                                   _safepoint_id,
                                   initial_running,
                                   _waiting_to_block, iterations);

  SafepointTracing::synchronized(nof_threads, initial_running, _nof_threads_hit_polling_page);

  // We do the safepoint cleanup first since a GC related safepoint
  // needs cleanup to be completed before running the GC op.
  EventSafepointCleanup cleanup_event;
  do_cleanup_tasks();
  post_safepoint_cleanup_event(cleanup_event, _safepoint_id);

  post_safepoint_begin_event(begin_event, _safepoint_id, nof_threads, _current_jni_active_count);
  SafepointTracing::cleanup();
}
```

```c++
// Wake up all threads, so they are ready to resume execution after the safepoint
// operation has been carried out
void SafepointSynchronize::end() {
  assert(Threads_lock->owned_by_self(), "must hold Threads_lock");
  EventSafepointEnd event;
  assert(Thread::current()->is_VM_thread(), "Only VM thread can execute a safepoint");

  disarm_safepoint();

  Universe::heap()->safepoint_synchronize_end();

  SafepointTracing::end();

  post_safepoint_end_event(event, safepoint_id());
}

void SafepointSynchronize::disarm_safepoint() {
  uint64_t active_safepoint_counter = _safepoint_counter;
  {
    JavaThreadIteratorWithHandle jtiwh;
#ifdef ASSERT
    // A pending_exception cannot be installed during a safepoint.  The threads
    // may install an async exception after they come back from a safepoint into
    // pending_exception after they unblock.  But that should happen later.
    for (; JavaThread *cur = jtiwh.next(); ) {
      assert (!(cur->has_pending_exception() &&
                cur->safepoint_state()->is_at_poll_safepoint()),
              "safepoint installed a pending exception");
    }
#endif // ASSERT

    OrderAccess::fence(); // keep read and write of _state from floating up
    assert(_state == _synchronized, "must be synchronized before ending safepoint synchronization");

    // Change state first to _not_synchronized.
    // No threads should see _synchronized when running.
    _state = _not_synchronized;

    // Set the next dormant (even) safepoint id.
    assert((_safepoint_counter & 0x1) == 1, "must be odd");
    Atomic::release_store(&_safepoint_counter, _safepoint_counter + 1);

    OrderAccess::fence(); // Keep the local state from floating up.

    jtiwh.rewind();
    for (; JavaThread *current = jtiwh.next(); ) {
      // Clear the visited flag to ensure that the critical counts are collected properly.
      DEBUG_ONLY(current->reset_visited_for_critical_count(active_safepoint_counter);)
      ThreadSafepointState* cur_state = current->safepoint_state();
      assert(!cur_state->is_running(), "Thread not suspended at safepoint");
      cur_state->restart(); // TSS _running
      assert(cur_state->is_running(), "safepoint state has not been reset");
    }
  } // ~JavaThreadIteratorWithHandle

  // Release threads lock, so threads can be created/destroyed again.
  Threads_lock->unlock();

  // Wake threads after local state is correctly set.
  _wait_barrier->disarm();
}
```

##### ğŸ¸åƒåœ¾å›æ”¶

###### â²è§¦å‘æ—¶æœº

```c++
HeapWord* SerialHeap::mem_allocate_work(size_t size,
                                        bool is_tlab) {

 	//...
    VM_GenCollectForAllocation op(size, is_tlab, gc_count_before);
    VMThread::execute(&op);
    // ...
}
```

```c++
void VM_GenCollectForAllocation::doit() {
  SvcGCMarker sgcm(SvcGCMarker::MINOR);

  SerialHeap* gch = SerialHeap::heap();
  GCCauseSetter gccs(gch, _gc_cause);
  _result = gch->satisfy_failed_allocation(_word_size, _tlab);
  assert(_result == nullptr || gch->is_in_reserved(_result), "result not in heap");

  if (_result == nullptr && GCLocker::is_active_and_needs_gc()) {
    set_gc_locked();
  }
}
```

```c++
// Callback from VM_GenCollectForAllocation operation.
// This function does everything necessary/possible to satisfy an
// allocation request that failed in the youngest generation that should
// have handled it (including collection, expansion, etc.)
HeapWord* SerialHeap::satisfy_failed_allocation(size_t size, bool is_tlab) {
  GCCauseSetter x(this, GCCause::_allocation_failure);
  HeapWord* result = nullptr;

  assert(size != 0, "Precondition violated");
  if (GCLocker::is_active_and_needs_gc()) {
    // GC locker is active; instead of a collection we will attempt
    // to expand the heap, if there's room for expansion.
    if (!is_maximal_no_gc()) {
      result = expand_heap_and_allocate(size, is_tlab);
    }
    return result;   // Could be null if we are out of space.
  } else if (!incremental_collection_will_fail(false /* don't consult_young */)) {
    // Do an incremental collection.
    do_collection(false,                     // full
                  false,                     // clear_all_soft_refs
                  size,                      // size
                  is_tlab,                   // is_tlab
                  SerialHeap::OldGen); // max_generation
  } else {
    log_trace(gc)(" :: Trying full because partial may fail :: ");
    // Try a full collection; see delta for bug id 6266275
    // for the original code and why this has been simplified
    // with from-space allocation criteria modified and
    // such allocation moved out of the safepoint path.
    do_collection(true,                      // full
                  false,                     // clear_all_soft_refs
                  size,                      // size
                  is_tlab,                   // is_tlab
                  SerialHeap::OldGen); // max_generation
  }

  result = attempt_allocation(size, is_tlab, false /*first_only*/);

  if (result != nullptr) {
    assert(is_in_reserved(result), "result not in heap");
    return result;
  }

  // OK, collection failed, try expansion.
  result = expand_heap_and_allocate(size, is_tlab);
  if (result != nullptr) {
    return result;
  }

  // If we reach this point, we're really out of memory. Try every trick
  // we can to reclaim memory. Force collection of soft references. Force
  // a complete compaction of the heap. Any additional methods for finding
  // free memory should be here, especially if they are expensive. If this
  // attempt fails, an OOM exception will be thrown.
  {
    UIntFlagSetting flag_change(MarkSweepAlwaysCompactCount, 1); // Make sure the heap is fully compacted

    do_collection(true,                      // full
                  true,                      // clear_all_soft_refs
                  size,                      // size
                  is_tlab,                   // is_tlab
                  SerialHeap::OldGen); // max_generation
  }

  result = attempt_allocation(size, is_tlab, false /* first_only */);
  if (result != nullptr) {
    assert(is_in_reserved(result), "result not in heap");
    return result;
  }

  assert(!soft_ref_policy()->should_clear_all_soft_refs(),
    "Flag should have been handled and cleared prior to this point");

  // What else?  We might try synchronous finalization later.  If the total
  // space available is large enough for the allocation, then a more
  // complete compaction phase than we've tried so far might be
  // appropriate.
  return nullptr;
}
```

###### ğŸ”¨System.gc()

```c++
// public collection interfaces
void SerialHeap::collect(GCCause::Cause cause) {
  // The caller doesn't have the Heap_lock
  assert(!Heap_lock->owned_by_self(), "this thread should not own the Heap_lock");

  unsigned int gc_count_before;
  unsigned int full_gc_count_before;

  {
    MutexLocker ml(Heap_lock);
    // Read the GC count while holding the Heap_lock
    gc_count_before      = total_collections();
    full_gc_count_before = total_full_collections();
  }

  if (GCLocker::should_discard(cause, gc_count_before)) {
    return;
  }

  bool should_run_young_gc =  (cause == GCCause::_wb_young_gc)
                           || (cause == GCCause::_gc_locker)
                DEBUG_ONLY(|| (cause == GCCause::_scavenge_alot));

  const GenerationType max_generation = should_run_young_gc
                                      ? YoungGen
                                      : OldGen;

  while (true) {
    VM_GenCollectFull op(gc_count_before, full_gc_count_before,
                         cause, max_generation);
    VMThread::execute(&op);

    if (!GCCause::is_explicit_full_gc(cause)) {
      return;
    }

    {
      MutexLocker ml(Heap_lock);
      // Read the GC count while holding the Heap_lock
      if (full_gc_count_before != total_full_collections()) {
        return;
      }
    }

    if (GCLocker::is_active_and_needs_gc()) {
      // If GCLocker is active, wait until clear before retrying.
      GCLocker::stall_until_clear();
    }
  }
}
```



```c++
void SerialHeap::do_full_collection(bool clear_all_soft_refs) {
   do_full_collection(clear_all_soft_refs, OldGen);
}

void SerialHeap::do_full_collection(bool clear_all_soft_refs,
                                    GenerationType last_generation) {
  do_collection(true,                   // full
                clear_all_soft_refs,    // clear_all_soft_refs
                0,                      // size
                false,                  // is_tlab
                last_generation);       // last_generation
  // Hack XXX FIX ME !!!
  // A scavenge may not have been attempted, or may have
  // been attempted and failed, because the old gen was too full
  if (gc_cause() == GCCause::_gc_locker && incremental_collection_failed()) {
    log_debug(gc, jni)("GC locker: Trying a full collection because scavenge failed");
    // This time allow the old gen to be collected as well
    do_collection(true,                // full
                  clear_all_soft_refs, // clear_all_soft_refs
                  0,                   // size
                  false,               // is_tlab
                  OldGen);             // last_generation
  }
}
```

```c++
void SerialHeap::do_collection(bool full,
                               bool clear_all_soft_refs,
                               size_t size,
                               bool is_tlab,
                               GenerationType max_generation) {
  ResourceMark rm;
  DEBUG_ONLY(Thread* my_thread = Thread::current();)

  assert(SafepointSynchronize::is_at_safepoint(), "should be at safepoint");
  assert(my_thread->is_VM_thread(), "only VM thread");
  assert(Heap_lock->is_locked(),
         "the requesting thread should have the Heap_lock");
  guarantee(!is_gc_active(), "collection is not reentrant");

  if (GCLocker::check_active_before_gc()) {
    return; // GC is disabled (e.g. JNI GetXXXCritical operation)
  }

  const bool do_clear_all_soft_refs = clear_all_soft_refs ||
                          soft_ref_policy()->should_clear_all_soft_refs();

  ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());

  AutoModifyRestore<bool> temporarily(_is_gc_active, true);

  bool complete = full && (max_generation == OldGen);
  bool old_collects_young = complete && !ScavengeBeforeFullGC;
  bool do_young_collection = !old_collects_young && _young_gen->should_collect(full, size, is_tlab);

  const PreGenGCValues pre_gc_values = get_pre_gc_values();

  bool run_verification = total_collections() >= VerifyGCStartAt;
  bool prepared_for_verification = false;
  bool do_full_collection = false;

  if (do_young_collection) {
    GCIdMark gc_id_mark;
    GCTraceCPUTime tcpu(((DefNewGeneration*)_young_gen)->gc_tracer());
    GCTraceTime(Info, gc) t("Pause Young", nullptr, gc_cause(), true);

    print_heap_before_gc();

    if (run_verification && VerifyBeforeGC) {
      prepare_for_verify();
      prepared_for_verification = true;
    }

    gc_prologue(complete);
    increment_total_collections(complete);

    collect_generation(_young_gen,
                       full,
                       size,
                       is_tlab,
                       run_verification,
                       do_clear_all_soft_refs);

    if (size > 0 && (!is_tlab || _young_gen->supports_tlab_allocation()) &&
        size * HeapWordSize <= _young_gen->unsafe_max_alloc_nogc()) {
      // Allocation request was met by young GC.
      size = 0;
    }

    // Ask if young collection is enough. If so, do the final steps for young collection,
    // and fallthrough to the end.
    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);
    if (!do_full_collection) {
      // Adjust generation sizes.
      _young_gen->compute_new_size();

      print_heap_change(pre_gc_values);

      // Track memory usage and detect low memory after GC finishes
      MemoryService::track_memory_usage();

      gc_epilogue(complete);
    }

    print_heap_after_gc();

  } else {
    // No young collection, ask if we need to perform Full collection.
    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);
  }

  if (do_full_collection) {
    GCIdMark gc_id_mark;
    GCTraceCPUTime tcpu(GenMarkSweep::gc_tracer());
    GCTraceTime(Info, gc) t("Pause Full", nullptr, gc_cause(), true);

    print_heap_before_gc();

    if (!prepared_for_verification && run_verification && VerifyBeforeGC) {
      prepare_for_verify();
    }

    if (!do_young_collection) {
      gc_prologue(complete);
      increment_total_collections(complete);
    }

    // Accounting quirk: total full collections would be incremented when "complete"
    // is set, by calling increment_total_collections above. However, we also need to
    // account Full collections that had "complete" unset.
    if (!complete) {
      increment_total_full_collections();
    }

    CodeCache::on_gc_marking_cycle_start();

    ClassUnloadingContext ctx(1 /* num_nmethod_unlink_workers */,
                              false /* unregister_nmethods_during_purge */,
                              false /* lock_codeblob_free_separately */);

    collect_generation(_old_gen,
                       full,
                       size,
                       is_tlab,
                       run_verification,
                       do_clear_all_soft_refs);

    CodeCache::on_gc_marking_cycle_finish();
    CodeCache::arm_all_nmethods();

    // Adjust generation sizes.
    _old_gen->compute_new_size();
    _young_gen->compute_new_size();

    // Delete metaspaces for unloaded class loaders and clean up loader_data graph
    ClassLoaderDataGraph::purge(/*at_safepoint*/true);
    DEBUG_ONLY(MetaspaceUtils::verify();)

    // Need to clear claim bits for the next mark.
    ClassLoaderDataGraph::clear_claimed_marks();

    // Resize the metaspace capacity after full collections
    MetaspaceGC::compute_new_size();

    print_heap_change(pre_gc_values);

    // Track memory usage and detect low memory after GC finishes
    MemoryService::track_memory_usage();

    // Need to tell the epilogue code we are done with Full GC, regardless what was
    // the initial value for "complete" flag.
    gc_epilogue(true);

    print_heap_after_gc();
  }
}
```

###### â†”åˆ†ä»£å›æ”¶

```c++
void SerialHeap::collect_generation(Generation* gen, bool full, size_t size,
                                    bool is_tlab, bool run_verification, bool clear_soft_refs) {
  FormatBuffer<> title("Collect gen: %s", gen->short_name());
  GCTraceTime(Trace, gc, phases) t1(title);
  TraceCollectorStats tcs(gen->counters());
  TraceMemoryManagerStats tmms(gen->gc_manager(), gc_cause(), heap()->is_young_gen(gen) ? "end of minor GC" : "end of major GC");

  gen->stat_record()->invocations++;
  gen->stat_record()->accumulated_time.start();

  // Must be done anew before each collection because
  // a previous collection will do mangling and will
  // change top of some spaces.
  record_gen_tops_before_GC();

  log_trace(gc)("%s invoke=%d size=" SIZE_FORMAT, heap()->is_young_gen(gen) ? "Young" : "Old", gen->stat_record()->invocations, size * HeapWordSize);

  if (run_verification && VerifyBeforeGC) {
    Universe::verify("Before GC");
  }
  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());

  // Do collection work
  {
    save_marks();   // save marks for all gens

    gen->collect(full, clear_soft_refs, size, is_tlab);
  }

  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());

  gen->stat_record()->accumulated_time.stop();

  update_gc_stats(gen, full);

  if (run_verification && VerifyAfterGC) {
    Universe::verify("After GC");
  }
}
```

å…¶ä¸­ 

> collect 

æ˜¯ä¸€ä¸ªè™šå‡½æ•°ï¼Œç”¨äºæ¯ä¸ªä»£å®ç°ä¸åŒçš„å›æ”¶æ–¹æ¡ˆã€‚

```c++
class Generation: public CHeapObj<mtGC> {
  friend class VMStructs;
 private:
  GCMemoryManager* _gc_manager;
// Perform a garbage collection.
  // If full is true attempt a full garbage collection of this generation.
  // Otherwise, attempting to (at least) free enough space to support an
  // allocation of the given "word_size".
  virtual void collect(bool   full,
                       bool   clear_all_soft_refs,
                       size_t word_size,
                       bool   is_tlab) = 0;
  // ...
}
```

###### â¤µå¹´è½»ä»£

å¹´è½»ä»£é‡‡ç”¨ å¤åˆ¶ç®—æ³•

```c++
void DefNewGeneration::collect(bool   full,
                               bool   clear_all_soft_refs,
                               size_t size,
                               bool   is_tlab) {
  assert(full || size > 0, "otherwise we don't want to collect");

  SerialHeap* heap = SerialHeap::heap();

  // If the next generation is too full to accommodate promotion
  // from this generation, pass on collection; let the next generation
  // do it.
  if (!collection_attempt_is_safe()) {
    log_trace(gc)(":: Collection attempt not safe ::");
    heap->set_incremental_collection_failed(); // Slight lie: we did not even attempt one
    return;
  }
  assert(to()->is_empty(), "Else not collection_attempt_is_safe");
  _gc_timer->register_gc_start();
  _gc_tracer->report_gc_start(heap->gc_cause(), _gc_timer->gc_start());
  _ref_processor->start_discovery(clear_all_soft_refs);

  _old_gen = heap->old_gen();

  init_assuming_no_promotion_failure();

  GCTraceTime(Trace, gc, phases) tm("DefNew", nullptr, heap->gc_cause());

  heap->trace_heap_before_gc(_gc_tracer);

  // These can be shared for all code paths
  IsAliveClosure is_alive(this);

  age_table()->clear();
  to()->clear(SpaceDecorator::Mangle);
  // The preserved marks should be empty at the start of the GC.
  _preserved_marks_set.init(1);

  assert(heap->no_allocs_since_save_marks(),
         "save marks have not been newly set.");

  YoungGenScanClosure young_gen_cl(this);
  OldGenScanClosure   old_gen_cl(this);

  FastEvacuateFollowersClosure evacuate_followers(heap,
                                                  &young_gen_cl,
                                                  &old_gen_cl);

  assert(heap->no_allocs_since_save_marks(),
         "save marks have not been newly set.");

  {
    StrongRootsScope srs(0);
    RootScanClosure root_cl{this};
    CLDScanClosure cld_cl{this};

    MarkingCodeBlobClosure code_cl(&root_cl,
                                   CodeBlobToOopClosure::FixRelocations,
                                   false /* keepalive_nmethods */);

    heap->process_roots(SerialHeap::SO_ScavengeCodeCache,
                        &root_cl,
                        &cld_cl,
                        &cld_cl,
                        &code_cl);

    _old_gen->scan_old_to_young_refs();
  }

  // "evacuate followers".
  evacuate_followers.do_void();

  {
    // Reference processing
    KeepAliveClosure keep_alive(this);
    ReferenceProcessor* rp = ref_processor();
    ReferenceProcessorPhaseTimes pt(_gc_timer, rp->max_num_queues());
    SerialGCRefProcProxyTask task(is_alive, keep_alive, evacuate_followers);
    const ReferenceProcessorStats& stats = rp->process_discovered_references(task, pt);
    _gc_tracer->report_gc_reference_stats(stats);
    _gc_tracer->report_tenuring_threshold(tenuring_threshold());
    pt.print_all_references();
  }
  assert(heap->no_allocs_since_save_marks(), "save marks have not been newly set.");

  {
    AdjustWeakRootClosure cl{this};
    WeakProcessor::weak_oops_do(&is_alive, &cl);
  }

  // Verify that the usage of keep_alive didn't copy any objects.
  assert(heap->no_allocs_since_save_marks(), "save marks have not been newly set.");

  _string_dedup_requests.flush();

  if (!_promotion_failed) {
    // Swap the survivor spaces.
    eden()->clear(SpaceDecorator::Mangle);
    from()->clear(SpaceDecorator::Mangle);
    if (ZapUnusedHeapArea) {
      // This is now done here because of the piece-meal mangling which
      // can check for valid mangling at intermediate points in the
      // collection(s).  When a young collection fails to collect
      // sufficient space resizing of the young generation can occur
      // an redistribute the spaces in the young generation.  Mangle
      // here so that unzapped regions don't get distributed to
      // other spaces.
      to()->mangle_unused_area();
    }
    swap_spaces();

    assert(to()->is_empty(), "to space should be empty now");

    adjust_desired_tenuring_threshold();

    assert(!heap->incremental_collection_failed(), "Should be clear");
  } else {
    assert(_promo_failure_scan_stack.is_empty(), "post condition");
    _promo_failure_scan_stack.clear(true); // Clear cached segments.

    remove_forwarding_pointers();
    log_info(gc, promotion)("Promotion failed");
    // Add to-space to the list of space to compact
    // when a promotion failure has occurred.  In that
    // case there can be live objects in to-space
    // as a result of a partial evacuation of eden
    // and from-space.
    swap_spaces();   // For uniformity wrt ParNewGeneration.
    from()->set_next_compaction_space(to());
    heap->set_incremental_collection_failed();

    _gc_tracer->report_promotion_failed(_promotion_failed_info);

    // Reset the PromotionFailureALot counters.
    NOT_PRODUCT(heap->reset_promotion_should_fail();)
  }
  // We should have processed and cleared all the preserved marks.
  _preserved_marks_set.reclaim();

  heap->trace_heap_after_gc(_gc_tracer);

  _gc_timer->register_gc_end();

  _gc_tracer->report_gc_end(_gc_timer->gc_end(), _gc_timer->time_partitions());
}
```

###### â¤µè€å¹´ä»£

```c++
void TenuredGeneration::collect(bool   full,
                                bool   clear_all_soft_refs,
                                size_t size,
                                bool   is_tlab) {
  SerialHeap* gch = SerialHeap::heap();

  STWGCTimer* gc_timer = GenMarkSweep::gc_timer();
  gc_timer->register_gc_start();

  SerialOldTracer* gc_tracer = GenMarkSweep::gc_tracer();
  gc_tracer->report_gc_start(gch->gc_cause(), gc_timer->gc_start());

  gch->pre_full_gc_dump(gc_timer);

  // ä½¿ç”¨ mark-compact æ ‡è®°-æ•´ç†(å‹ç¼©)æ³• æ”¶é›†åƒåœ¾
  GenMarkSweep::invoke_at_safepoint(clear_all_soft_refs);

  gch->post_full_gc_dump(gc_timer);

  gc_timer->register_gc_end();

  gc_tracer->report_gc_end(gc_timer->gc_end(), gc_timer->time_partitions());
}
```

```c++
void GenMarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {
  assert(SafepointSynchronize::is_at_safepoint(), "must be at a safepoint");

  SerialHeap* gch = SerialHeap::heap();
#ifdef ASSERT
  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {
    assert(clear_all_softrefs, "Policy should have been checked earlier");
  }
#endif

  gch->trace_heap_before_gc(_gc_tracer);

  // Increment the invocation count
  _total_invocations++;

  // Capture used regions for old-gen to reestablish old-to-young invariant
  // after full-gc.
  gch->old_gen()->save_used_region();

  allocate_stacks();

  phase1_mark(clear_all_softrefs);

  // æ•´ç†æ“ä½œå°è£…åœ¨ Compacter ç±»ä¸­
  Compacter compacter{gch};

  {
    // Now all live objects are marked, compute the new object addresses.
    GCTraceTime(Info, gc, phases) tm("Phase 2: Compute new object addresses", _gc_timer);

    compacter.phase2_calculate_new_addr();
  }

  // Don't add any more derived pointers during phase3
#if COMPILER2_OR_JVMCI
  assert(DerivedPointerTable::is_active(), "Sanity");
  DerivedPointerTable::set_active(false);
#endif

  {
    // Adjust the pointers to reflect the new locations
    GCTraceTime(Info, gc, phases) tm("Phase 3: Adjust pointers", gc_timer());

    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);

    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);
    gch->process_roots(SerialHeap::SO_AllCodeCache,
                       &adjust_pointer_closure,
                       &adjust_cld_closure,
                       &adjust_cld_closure,
                       &code_closure);

    WeakProcessor::oops_do(&adjust_pointer_closure);

    adjust_marks();
    compacter.phase3_adjust_pointers();
  }

  {
    // All pointers are now adjusted, move objects accordingly
    GCTraceTime(Info, gc, phases) tm("Phase 4: Move objects", _gc_timer);

    compacter.phase4_compact();
  }

  restore_marks();

  // Set saved marks for allocation profiler (and other things? -- dld)
  // (Should this be in general part?)
  gch->save_marks();

  deallocate_stacks();

  MarkSweep::_string_dedup_requests->flush();

  bool is_young_gen_empty = (gch->young_gen()->used() == 0);
  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);

  gch->prune_scavengable_nmethods();

  // Update heap occupancy information which is used as
  // input to soft ref clearing policy at the next gc.
  Universe::heap()->update_capacity_and_used_at_gc();

  // Signal that we have completed a visit to all live objects.
  Universe::heap()->record_whole_heap_examined_timestamp();

  gch->trace_heap_after_gc(_gc_tracer);
}
```

#### ğŸ—‘ParallelScavengeHeap

ParallelScavenge åˆ©ç”¨å„ç§é€‚åº”æ€§ç­–ç•¥ä»¥æé«˜ååé‡ã€‚

##### ğŸ¸å†…å­˜åˆ†é…

```c++
// ParallelScavengeHeap is the implementation of CollectedHeap for Parallel GC.
//
// The heap is reserved up-front in a single contiguous block, split into two
// parts, the old and young generation. The old generation resides at lower
// addresses, the young generation at higher addresses. The boundary address
// between the generations is fixed. Within a generation, committed memory
// grows towards higher addresses.
//
//
// low                                                                high
//
//                          +-- generation boundary (fixed after startup)
//                          |
// |<- old gen (reserved) ->|<-       young gen (reserved)             ->|
// +---------------+--------+-----------------+--------+--------+--------+
// |      old      |        |       eden      |  from  |   to   |        |
// |               |        |                 |  (to)  | (from) |        |
// +---------------+--------+-----------------+--------+--------+--------+
// |<- committed ->|        |<-          committed            ->|
//
class ParallelScavengeHeap : public CollectedHeap {
  friend class VMStructs;
 private:
  static PSYoungGen* _young_gen;
  static PSOldGen*   _old_gen;

  // Sizing policy for entire heap
  static PSAdaptiveSizePolicy*       _size_policy;
  static PSGCAdaptivePolicyCounters* _gc_policy_counters;
  // ...
}
```

```c++
void PSYoungGen::initialize_work() {

  _reserved = MemRegion((HeapWord*)virtual_space()->low_boundary(),
                        (HeapWord*)virtual_space()->high_boundary());
  assert(_reserved.byte_size() == max_gen_size(), "invariant");

  MemRegion cmr((HeapWord*)virtual_space()->low(),
                (HeapWord*)virtual_space()->high());
  ParallelScavengeHeap::heap()->card_table()->resize_covered_region(cmr);

  if (ZapUnusedHeapArea) {
    // Mangle newly committed space immediately because it
    // can be done here more simply that after the new
    // spaces have been computed.
    SpaceMangler::mangle_region(cmr);
  }

  if (UseNUMA) {
    _eden_space = new MutableNUMASpace(virtual_space()->alignment());
  } else {
    _eden_space = new MutableSpace(virtual_space()->alignment());
  }
  _from_space = new MutableSpace(virtual_space()->alignment());
  _to_space   = new MutableSpace(virtual_space()->alignment());

  // Generation Counters - generation 0, 3 subspaces
  _gen_counters = new PSGenerationCounters("new", 0, 3, min_gen_size(),
                                           max_gen_size(), virtual_space());

  // Compute maximum space sizes for performance counters
  size_t alignment = SpaceAlignment;
  size_t size = virtual_space()->reserved_size();

  size_t max_survivor_size;
  size_t max_eden_size;

  if (UseAdaptiveSizePolicy) {
    max_survivor_size = size / MinSurvivorRatio;

    // round the survivor space size down to the nearest alignment
    // and make sure its size is greater than 0.
    max_survivor_size = align_down(max_survivor_size, alignment);
    max_survivor_size = MAX2(max_survivor_size, alignment);

    // set the maximum size of eden to be the size of the young gen
    // less two times the minimum survivor size. The minimum survivor
    // size for UseAdaptiveSizePolicy is one alignment.
    max_eden_size = size - 2 * alignment;
  } else {
    max_survivor_size = size / InitialSurvivorRatio;

    // round the survivor space size down to the nearest alignment
    // and make sure its size is greater than 0.
    max_survivor_size = align_down(max_survivor_size, alignment);
    max_survivor_size = MAX2(max_survivor_size, alignment);

    // set the maximum size of eden to be the size of the young gen
    // less two times the survivor size when the generation is 100%
    // committed. The minimum survivor size for -UseAdaptiveSizePolicy
    // is dependent on the committed portion (current capacity) of the
    // generation - the less space committed, the smaller the survivor
    // space, possibly as small as an alignment. However, we are interested
    // in the case where the young generation is 100% committed, as this
    // is the point where eden reaches its maximum size. At this point,
    // the size of a survivor space is max_survivor_size.
    max_eden_size = size - 2 * max_survivor_size;
  }

  _eden_counters = new SpaceCounters("eden", 0, max_eden_size, _eden_space,
                                     _gen_counters);
  _from_counters = new SpaceCounters("s0", 1, max_survivor_size, _from_space,
                                     _gen_counters);
  _to_counters = new SpaceCounters("s1", 2, max_survivor_size, _to_space,
                                   _gen_counters);

  compute_initial_space_boundaries();
}
```

##### ğŸ¸åƒåœ¾å›æ”¶

```C++
void ParallelScavengeHeap::collect(GCCause::Cause cause) {
  assert(!Heap_lock->owned_by_self(),
    "this thread should not own the Heap_lock");

  uint gc_count      = 0;
  uint full_gc_count = 0;
  {
    MutexLocker ml(Heap_lock);
    // This value is guarded by the Heap_lock
    gc_count      = total_collections();
    full_gc_count = total_full_collections();
  }

  if (GCLocker::should_discard(cause, gc_count)) {
    return;
  }

  while (true) {
    VM_ParallelGCSystemGC op(gc_count, full_gc_count, cause);
    VMThread::execute(&op);

    if (!GCCause::is_explicit_full_gc(cause) || op.full_gc_succeeded()) {
      return;
    }

    {
      MutexLocker ml(Heap_lock);
      if (full_gc_count != total_full_collections()) {
        return;
      }
    }

    if (GCLocker::is_active_and_needs_gc()) {
      // If GCLocker is active, wait until clear before retrying.
      GCLocker::stall_until_clear();
    }
  }
}
```

```C++
void VM_ParallelGCSystemGC::doit() {
  SvcGCMarker sgcm(SvcGCMarker::FULL);

  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();

  GCCauseSetter gccs(heap, _gc_cause);
  if (!_full) {
    // If (and only if) the scavenge fails, this will invoke a full gc.
    _full_gc_succeeded = heap->invoke_scavenge();
  } else {
    _full_gc_succeeded = PSParallelCompact::invoke(false);
  }
}

inline bool ParallelScavengeHeap::invoke_scavenge() {
  return PSScavenge::invoke();
}
```

```c++
void VMThread::execute(VM_Operation* op) {
  Thread* t = Thread::current();

  if (t->is_VM_thread()) {
    op->set_calling_thread(t);
    ((VMThread*)t)->inner_execute(op);
    return;
  }
  // ...
}

void VMThread::inner_execute(VM_Operation* op) {
  assert(Thread::current()->is_VM_thread(), "Must be the VM thread");

  VM_Operation* prev_vm_operation = nullptr;
  if (_cur_vm_operation != nullptr) {
    // Check that the VM operation allows nested VM operation.
    // This is normally not the case, e.g., the compiler
    // does not allow nested scavenges or compiles.
    if (!_cur_vm_operation->allow_nested_vm_operations()) {
      fatal("Unexpected nested VM operation %s requested by operation %s",
            op->name(), _cur_vm_operation->name());
    }
    op->set_calling_thread(_cur_vm_operation->calling_thread());
    prev_vm_operation = _cur_vm_operation;
  }

  _cur_vm_operation = op;

  HandleMark hm(VMThread::vm_thread());

  const char* const cause = op->cause();
  EventMarkVMOperation em("Executing %sVM operation: %s%s%s%s",
      prev_vm_operation != nullptr ? "nested " : "",
      op->name(),
      cause != nullptr ? " (" : "",
      cause != nullptr ? cause : "",
      cause != nullptr ? ")" : "");

  log_debug(vmthread)("Evaluating %s %s VM operation: %s",
                       prev_vm_operation != nullptr ? "nested" : "",
                      _cur_vm_operation->evaluate_at_safepoint() ? "safepoint" : "non-safepoint",
                      _cur_vm_operation->name());

  bool end_safepoint = false;
  bool has_timeout_task = (_timeout_task != nullptr);
  if (_cur_vm_operation->evaluate_at_safepoint() &&
      !SafepointSynchronize::is_at_safepoint()) {
    SafepointSynchronize::begin();
    if (has_timeout_task) {
      _timeout_task->arm(_cur_vm_operation->name());
    }
    end_safepoint = true;
  }

  evaluate_operation(_cur_vm_operation);

  if (end_safepoint) {
    if (has_timeout_task) {
      _timeout_task->disarm();
    }
    SafepointSynchronize::end();
  }

  _cur_vm_operation = prev_vm_operation;
}
```

```c++
// This method contains all heap specific policy for invoking scavenge.
// PSScavenge::invoke_no_policy() will do nothing but attempt to
// scavenge. It will not clean up after failed promotions, bail out if
// we've exceeded policy time limits, or any other special behavior.
// All such policy should be placed here.
//
// Note that this method should only be called from the vm_thread while
// at a safepoint!
bool PSScavenge::invoke() {
  assert(SafepointSynchronize::is_at_safepoint(), "should be at safepoint");
  assert(Thread::current() == (Thread*)VMThread::vm_thread(), "should be in vm thread");
  assert(!ParallelScavengeHeap::heap()->is_gc_active(), "not reentrant");

  ParallelScavengeHeap* const heap = ParallelScavengeHeap::heap();
  PSAdaptiveSizePolicy* policy = heap->size_policy();
  IsGCActiveMark mark;

  const bool scavenge_done = PSScavenge::invoke_no_policy();
  const bool need_full_gc = !scavenge_done;
  bool full_gc_done = false;

  if (UsePerfData) {
    PSGCAdaptivePolicyCounters* const counters = heap->gc_policy_counters();
    const int ffs_val = need_full_gc ? full_follows_scavenge : not_skipped;
    counters->update_full_follows_scavenge(ffs_val);
  }

  if (need_full_gc) {
    GCCauseSetter gccs(heap, GCCause::_adaptive_size_policy);
    SoftRefPolicy* srp = heap->soft_ref_policy();
    const bool clear_all_softrefs = srp->should_clear_all_soft_refs();

    full_gc_done = PSParallelCompact::invoke_no_policy(clear_all_softrefs);
  }

  return full_gc_done;
}
```

#### ğŸ—‘G1CollectedHeap

##### ğŸ¸å†…å­˜åˆ†é…

###### TLAB æ…¢åˆ†é…

```c++
HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,
                                             size_t requested_size,
                                             size_t* actual_size) {
  assert_heap_not_locked_and_not_at_safepoint();
  assert(!is_humongous(requested_size), "we do not allow humongous TLABs");

  return attempt_allocation(min_size, requested_size, actual_size);
}

inline HeapWord* G1CollectedHeap::attempt_allocation(size_t min_word_size,
                                                     size_t desired_word_size,
                                                     size_t* actual_word_size) {
  assert_heap_not_locked_and_not_at_safepoint();
  assert(!is_humongous(desired_word_size), "attempt_allocation() should not "
         "be called for humongous allocation requests");

  HeapWord* result = _allocator->attempt_allocation(min_word_size, desired_word_size, actual_word_size);

  if (result == nullptr) {
    *actual_word_size = desired_word_size;
    result = attempt_allocation_slow(desired_word_size);
  }

  assert_heap_not_locked();
  if (result != nullptr) {
    assert(*actual_word_size != 0, "Actual size must have been set here");
    dirty_young_block(result, *actual_word_size);
  } else {
    *actual_word_size = 0;
  }

  return result;
}

inline HeapWord* G1Allocator::attempt_allocation(size_t min_word_size,
                                                 size_t desired_word_size,
                                                 size_t* actual_word_size) {
  uint node_index = current_node_index();

  // åœ¨ Region ä¸Šåˆ†é…
  HeapWord* result = mutator_alloc_region(node_index)->attempt_retained_allocation(min_word_size, desired_word_size, actual_word_size);
  if (result != nullptr) {
    return result;
  }

  return mutator_alloc_region(node_index)->attempt_allocation(min_word_size, desired_word_size, actual_word_size);
}
```

###### ç›´æ¥åœ¨å †ä¸Šåˆ†é…

```c++
HeapWord*
G1CollectedHeap::mem_allocate(size_t word_size,
                              bool*  gc_overhead_limit_was_exceeded) {
  assert_heap_not_locked_and_not_at_safepoint();

  if (is_humongous(word_size)) {
    return attempt_allocation_humongous(word_size);
  }
  size_t dummy = 0;
  return attempt_allocation(word_size, word_size, &dummy);
}
```

##### ğŸ¸åƒåœ¾å›æ”¶

###### RSet

```c++
// A G1RemSet in which each heap region has a rem set that records the
// external heap references into it.  Uses a mod ref bs to track updates,
// so that they can be used to update the individual region remsets.
class G1RemSet: public CHeapObj<mtGC> {
public:
  typedef CardTable::CardValue CardValue;

private:
  G1RemSetScanState* _scan_state;

  G1RemSetSummary _prev_period_summary;

  G1CollectedHeap* _g1h;

  G1CardTable*           _ct;
  G1Policy*              _g1p;

  void print_merge_heap_roots_stats();

  void assert_scan_top_is_null(uint hrm_index) NOT_DEBUG_RETURN;

  void enqueue_for_reprocessing(CardValue* card_ptr);

public:
  // Initialize data that depends on the heap size being known.
  void initialize(uint max_reserved_regions);

  G1RemSet(G1CollectedHeap* g1h, G1CardTable* ct);
  ~G1RemSet();

  // Scan all cards in the non-collection set regions that potentially contain
  // references into the current whole collection set.
  void scan_heap_roots(G1ParScanThreadState* pss,
                       uint worker_id,
                       G1GCPhaseTimes::GCParPhases scan_phase,
                       G1GCPhaseTimes::GCParPhases objcopy_phase,
                       bool remember_already_scanned_cards);

  // Merge cards from various sources (remembered sets, log buffers)
  // and calculate the cards that need to be scanned later (via scan_heap_roots()).
  // If initial_evacuation is set, this is called during the initial evacuation.
  void merge_heap_roots(bool initial_evacuation);

  void complete_evac_phase(bool has_more_than_one_evacuation_phase);
  // Prepare for and cleanup after scanning the heap roots. Must be called
  // once before and after in sequential code.
  void prepare_for_scan_heap_roots();

  // Print coarsening stats.
  void print_coarsen_stats();
  // Creates a task for cleaining up temporary data structures and the
  // card table, removing temporary duplicate detection information.
  G1AbstractSubTask* create_cleanup_after_scan_heap_roots_task();
  // Excludes the given region from heap root scanning.
  void exclude_region_from_scan(uint region_idx);
  // Creates a snapshot of the current _top values at the start of collection to
  // filter out card marks that we do not want to scan.
  void prepare_region_for_scan(HeapRegion* region);

  // Do work for regions in the current increment of the collection set, scanning
  // non-card based (heap) roots.
  void scan_collection_set_regions(G1ParScanThreadState* pss,
                                   uint worker_id,
                                   G1GCPhaseTimes::GCParPhases scan_phase,
                                   G1GCPhaseTimes::GCParPhases coderoots_phase,
                                   G1GCPhaseTimes::GCParPhases objcopy_phase);

  // Two methods for concurrent refinement support, executed concurrently to
  // the mutator:
  // Cleans the card at "*card_ptr_addr" before refinement, returns true iff the
  // card needs later refinement.
  bool clean_card_before_refine(CardValue** const card_ptr_addr);
  // Refine the region corresponding to "card_ptr". Must be called after
  // being filtered by clean_card_before_refine(), and after proper
  // fence/synchronization.
  void refine_card_concurrently(CardValue* const card_ptr,
                                const uint worker_id);

  // Print accumulated summary info from the start of the VM.
  void print_summary_info();

  // Print accumulated summary info from the last time called.
  void print_periodic_summary_info(const char* header, uint period_count, bool show_thread_times);
};
```

###### CSet

```c++
class G1CollectionSet {
  G1CollectedHeap* _g1h;
  G1Policy* _policy;

  // All old gen collection set candidate regions.
  G1CollectionSetCandidates _candidates;

  // The actual collection set as a set of region indices.
  // All entries in _collection_set_regions below _collection_set_cur_length are
  // assumed to be part of the collection set.
  // We assume that at any time there is at most only one writer and (one or more)
  // concurrent readers. This means we are good with using storestore and loadload
  // barriers on the writer and reader respectively only.
  uint* _collection_set_regions;
  volatile uint _collection_set_cur_length;
  uint _collection_set_max_length;

  uint _eden_region_length;
  uint _survivor_region_length;
  uint _initial_old_region_length;

  // When doing mixed collections we can add old regions to the collection set, which
  // will be collected only if there is enough time. We call these optional (old) regions.
  G1CollectionCandidateRegionList _optional_old_regions;

  enum CSetBuildType {
    Active,             // We are actively building the collection set
    Inactive            // We are not actively building the collection set
  };

  CSetBuildType _inc_build_state;
  size_t _inc_part_start;

  G1CollectorState* collector_state() const;
  G1GCPhaseTimes* phase_times();

  void verify_young_cset_indices() const NOT_DEBUG_RETURN;

  // Update the incremental collection set information when adding a region.
  void add_young_region_common(HeapRegion* hr);

  // Add the given old region to the head of the current collection set.
  void add_old_region(HeapRegion* hr);

  void move_candidates_to_collection_set(G1CollectionCandidateRegionList* regions);
  // Prepares old regions in the given set for optional collection later. Does not
  // add the region to collection set yet.
  void prepare_optional_regions(G1CollectionCandidateRegionList* regions);
  // Moves given old regions from the marking candidates to the retained candidates.
  // This makes sure that marking candidates will not remain there to unnecessarily
  // prolong the mixed phase.
  void move_pinned_marking_to_retained(G1CollectionCandidateRegionList* regions);
  // Removes the given list of regions from the retained candidates.
  void drop_pinned_retained_regions(G1CollectionCandidateRegionList* regions);

  // Finalize the young part of the initial collection set. Relabel survivor regions
  // as Eden and calculate a prediction on how long the evacuation of all young regions
  // will take.
  double finalize_young_part(double target_pause_time_ms, G1SurvivorRegions* survivors);
  // Perform any final calculations on the incremental collection set fields before we
  // can use them.
  void finalize_incremental_building();

  // Select the regions comprising the initial and optional collection set from marking
  // and retained collection set candidates.
  void finalize_old_part(double time_remaining_ms);

  // Iterate the part of the collection set given by the offset and length applying the given
  // HeapRegionClosure. The worker_id will determine where in the part to start the iteration
  // to allow for more efficient parallel iteration.
  void iterate_part_from(HeapRegionClosure* cl,
                         HeapRegionClaimer* hr_claimer,
                         size_t offset,
                         size_t length,
                         uint worker_id) const;
public:
  G1CollectionSet(G1CollectedHeap* g1h, G1Policy* policy);
  ~G1CollectionSet();

  // Initializes the collection set giving the maximum possible length of the collection set.
  void initialize(uint max_region_length);

  void abandon_all_candidates();

  G1CollectionSetCandidates* candidates() { return &_candidates; }
  const G1CollectionSetCandidates* candidates() const { return &_candidates; }

  void init_region_lengths(uint eden_cset_region_length,
                           uint survivor_cset_region_length);

  uint region_length() const       { return young_region_length() +
                                            initial_old_region_length(); }
  uint young_region_length() const { return eden_region_length() +
                                            survivor_region_length(); }

  uint eden_region_length() const     { return _eden_region_length; }
  uint survivor_region_length() const { return _survivor_region_length; }
  uint initial_old_region_length() const      { return _initial_old_region_length; }
  uint optional_region_length() const { return _optional_old_regions.length(); }

  bool only_contains_young_regions() const { return (initial_old_region_length() + optional_region_length()) == 0; }

  // Reset the contents of the collection set.
  void clear();

  // Incremental collection set support

  // Initialize incremental collection set info.
  void start_incremental_building();
  // Start a new collection set increment.
  void update_incremental_marker() { _inc_build_state = Active; _inc_part_start = _collection_set_cur_length; }
  // Stop adding regions to the current collection set increment.
  void stop_incremental_building() { _inc_build_state = Inactive; }

  // Iterate over the current collection set increment applying the given HeapRegionClosure
  // from a starting position determined by the given worker id.
  void iterate_incremental_part_from(HeapRegionClosure* cl, HeapRegionClaimer* hr_claimer, uint worker_id) const;

  // Returns the length of the current increment in number of regions.
  size_t increment_length() const { return _collection_set_cur_length - _inc_part_start; }
  // Returns the length of the whole current collection set in number of regions
  size_t cur_length() const { return _collection_set_cur_length; }

  // Iterate over the entire collection set (all increments calculated so far), applying
  // the given HeapRegionClosure on all of them.
  void iterate(HeapRegionClosure* cl) const;
  void par_iterate(HeapRegionClosure* cl,
                   HeapRegionClaimer* hr_claimer,
                   uint worker_id) const;

  void iterate_optional(HeapRegionClosure* cl) const;

  // Finalize the initial collection set consisting of all young regions potentially a
  // few old gen regions.
  void finalize_initial_collection_set(double target_pause_time_ms, G1SurvivorRegions* survivor);
  // Finalize the next collection set from the set of available optional old gen regions.
  bool finalize_optional_for_evacuation(double remaining_pause_time);
  // Abandon (clean up) optional collection set regions that were not evacuated in this
  // pause.
  void abandon_optional_collection_set(G1ParScanThreadStateSet* pss);

  // Add eden region to the collection set.
  void add_eden_region(HeapRegion* hr);

  // Add survivor region to the collection set.
  void add_survivor_regions(HeapRegion* hr);

#ifndef PRODUCT
  bool verify_young_ages();

  void print(outputStream* st);
#endif // !PRODUCT
};
```

###### å¡è¡¨

g1 å°† region åˆ†ä¸ºå¤šä¸ªå¡ï¼Œå¹¶ç»´æŠ¤ä¸€ä¸ªå…¨å±€å¡è¡¨ï¼Œç”¨äºæé«˜é‡æ–°æ ‡è®°æ•ˆç‡ã€‚

###### âœ¨è¯»å†™å±éšœ

```c++
jint G1CollectedHeap::initialize() {
    // ...
	// Create the barrier set for the entire reserved region.
  G1CardTable* ct = new G1CardTable(heap_rs.region());
  G1BarrierSet* bs = new G1BarrierSet(ct);
  bs->initialize();
  assert(bs->is_a(BarrierSet::G1BarrierSet), "sanity");
  BarrierSet::set_barrier_set(bs);
  //...
}
```

###### âœ¨ä¸‰è‰²æ ‡è®°æ³•

> 1. å¼€å§‹æ—¶æ‰€æœ‰å¯¹è±¡ä¸ºç™½è‰²ï¼Œè¡¨ç¤ºæœªæ‰«ææˆ–åƒåœ¾çŠ¶æ€ï¼›ç°è‰²è¡¨ç¤ºéœ€è¦ç»§ç»­æ‰«æï¼Œé»‘è‰²è¡¨ç¤ºæ‰«æåœæ­¢ä¸”å­˜æ´»ã€‚
> 2. åˆå§‹æ ‡è®°é˜¶æ®µï¼ŒæŒ‡çš„æ˜¯æ ‡è®° GCRoots ç›´æ¥å¼•ç”¨çš„èŠ‚ç‚¹ï¼Œå°†å®ƒä»¬æ ‡è®°ä¸º**ç°è‰²**ï¼Œè¿™ä¸ªé˜¶æ®µéœ€è¦ ã€ŒStop the Worldã€ã€‚
> 3. å¹¶å‘æ ‡è®°é˜¶æ®µï¼ŒæŒ‡çš„æ˜¯ä»ç°è‰²èŠ‚ç‚¹å¼€å§‹ï¼Œå»æ‰«ææ•´ä¸ªå¼•ç”¨é“¾ï¼Œç„¶åå°†å®ƒä»¬æ ‡è®°ä¸º**é»‘è‰²**ï¼Œè¿™ä¸ªé˜¶æ®µä¸éœ€è¦ã€ŒStop the Worldã€ã€‚
> 4. é‡æ–°æ ‡è®°é˜¶æ®µï¼ŒæŒ‡çš„æ˜¯å»æ ¡æ­£å¹¶å‘æ ‡è®°é˜¶æ®µçš„é”™è¯¯ï¼Œè¿™ä¸ªé˜¶æ®µéœ€è¦ã€ŒStop the Worldã€ã€‚
> 5. å¹¶å‘æ¸…é™¤ï¼ŒæŒ‡çš„æ˜¯å°†å·²ç»ç¡®å®šä¸ºåƒåœ¾çš„å¯¹è±¡æ¸…é™¤æ‰ï¼Œè¿™ä¸ªé˜¶æ®µä¸éœ€è¦ã€ŒStop the Worldã€

ä¸Šè¿°æ­¥éª¤å¯èƒ½å‡ºç°é—®é¢˜æ˜¯æ­¥éª¤ 2ï¼Œå› ä¸ºè¯¥è¿‡ç¨‹å¯èƒ½å› ä¸ºç”¨æˆ·çº¿ç¨‹äº§ç”Ÿæˆ–åˆ é™¤æ–°çš„å¼•ç”¨å…³ç³»ï¼Œå¯èƒ½å‡ºç°å¤šæ ‡æˆ–**æ¼æ ‡**ï¼Œå…¶ä¸­æ¼æ ‡å°†å½±å“ç¨‹åºåŠŸèƒ½ï¼Œå¿…é¡»è¢«è§£å†³ã€‚

ä¸‹åˆ—ä¼ªä»£ç åæ˜ äº†æ¼æ ‡çš„åœºæ™¯ï¼š

```javascript
var G = objE.fieldG; // field_ è¡¨ç¤ºå¼•ç”¨çš„å¯¹è±¡ (è¯»)
objE.fieldG = null;  // ç°è‰² E æ–­å¼€å¼•ç”¨ ç™½è‰²G (å†™)
objD.fieldG = G;  // é»‘è‰²D å¼•ç”¨ ç™½è‰²G (å†™)
```

æ¼æ ‡çš„å……è¦æ¡ä»¶å¦‚ä¸‹ï¼š

> 1.  æœ‰è‡³å°‘ä¸€ä¸ªé»‘è‰²å¯¹è±¡åœ¨è‡ªå·±è¢«æ ‡è®°ä¹‹åæŒ‡å‘äº†è¿™ä¸ªç™½è‰²å¯¹è±¡ã€‚
> 2.  æ‰€æœ‰çš„ç°è‰²å¯¹è±¡åœ¨è‡ªå·±å¼•ç”¨æ‰«æå®Œæˆä¹‹å‰åˆ é™¤äº†å¯¹ç™½è‰²å¯¹è±¡çš„å¼•ç”¨ã€‚

åªè¦ç ´åå…¶ä¸­ä¸€ä¸ªæ¡ä»¶å³å¯ï¼Œæ•…å¯¹åº”ä¸¤ç§æ–¹å¼

> 1. å¢é‡æ›´æ–°
> 2. åŸå§‹å¿«ç…§

CMS é‡‡ç”¨æ–¹æ¡ˆ 1ï¼ŒG1  é‡‡ç”¨æ–¹æ¡ˆ 2ã€‚

> - ä¸ºäº†é˜²æ­¢ç°è‰²å¯¹è±¡è¢«æ‰«æå®Œæˆä¹‹å‰åˆ é™¤å¯¹ç™½è‰²å¯¹è±¡çš„å¼•ç”¨ï¼Œåœ¨æ­¤ä¹‹å‰ï¼Œå…ˆå°†ç°è‰²å¯¹è±¡æŒ‡å‘çš„ç™½è‰²å¯¹è±¡å¼•ç”¨å…³ç³»ä¿å­˜ä¸‹æ¥(å³æ ‡è®°è¯¥ç™½è‰²å¯¹è±¡ä¸ºç°è‰²)ï¼Œç„¶ååœ¨é‡æ–°æ ‡è®°é˜¶æ®µå°±è¡Œé‡æ–°æ‰«æã€‚
> - åœ¨ä¸Šè¿°ä¼ªä»£ç ç¬¬2è¡Œï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªå†™å±éšœï¼Œä»¥ä¾¿è®°å½•Gï¼Œå°†æ­¤æ ‡è®°ä¸ºç°è‰²ã€‚
> - ä¸ºäº†é¿å…é‡æ–°æ ‡è®°é˜¶æ®µå—åˆ°å¹²æ‰°ï¼Œæ­¤æ­¥éœ€è¦STWã€‚

###### âœ¨æ•ˆç›Šä¼˜å…ˆæ”¶é›†

æ ‡è®°å®Œæˆåï¼Œéœ€è¦æ”¶é›†çš„åƒåœ¾ä¿å­˜åœ¨CSetä¸­ï¼ŒG1æ”¶é›†å™¨é€šè¿‡è®¡ç®— Region çš„åƒåœ¾ç™¾åˆ†æ¯”é€‰å–æ•ˆç›Šè¾ƒé«˜çš„ Region è¿›è¡Œä¼˜å…ˆæ”¶é›†ã€‚æ­¤æ­¥**éœ€è¦ STW**ã€‚

```c++
void G1CollectedHeap::collect(GCCause::Cause cause) {
  try_collect(cause, collection_counters(this));
}

bool G1CollectedHeap::try_collect(GCCause::Cause cause,
                                  const G1GCCounters& counters_before) {
  if (should_do_concurrent_full_gc(cause)) {
    return try_collect_concurrently(cause,
                                    counters_before.total_collections(),
                                    counters_before.old_marking_cycles_started());
  } else if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc
             DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {

    // Schedule a standard evacuation pause. We're setting word_size
    // to 0 which means that we are not requesting a post-GC allocation.
    VM_G1CollectForAllocation op(0,     /* word_size */
                                 counters_before.total_collections(),
                                 cause);
    VMThread::execute(&op);
    return op.gc_succeeded();
  } else {
    // Schedule a Full GC.
    return try_collect_fullgc(cause, counters_before);
  }
}
```

```c++
void VM_G1CollectForAllocation::doit() {
  G1CollectedHeap* g1h = G1CollectedHeap::heap();

  GCCauseSetter x(g1h, _gc_cause);
  // Try a partial collection of some kind.
  _gc_succeeded = g1h->do_collection_pause_at_safepoint();
  assert(_gc_succeeded, "no reason to fail");

  if (_word_size > 0) {
    // An allocation had been requested. Do it, eventually trying a stronger
    // kind of GC.
    _result = g1h->satisfy_failed_allocation(_word_size, &_gc_succeeded);
  } else if (g1h->should_upgrade_to_full_gc()) {
    // There has been a request to perform a GC to free some space. We have no
    // information on how much memory has been asked for. In case there are
    // absolutely no regions left to allocate into, do a full compaction.
    _gc_succeeded = g1h->upgrade_to_full_collection();
  }
}
```

```c++
bool G1CollectedHeap::do_collection_pause_at_safepoint() {
  assert_at_safepoint_on_vm_thread();
  guarantee(!is_gc_active(), "collection is not reentrant");

  do_collection_pause_at_safepoint_helper();
  return true;
}

void G1CollectedHeap::do_collection_pause_at_safepoint_helper() {
  ResourceMark rm;

  IsGCActiveMark active_gc_mark;
  GCIdMark gc_id_mark;
  SvcGCMarker sgcm(SvcGCMarker::MINOR);

  GCTraceCPUTime tcpu(_gc_tracer_stw);

  _bytes_used_during_gc = 0;

  policy()->decide_on_concurrent_start_pause();
  // Record whether this pause may need to trigger a concurrent operation. Later,
  // when we signal the G1ConcurrentMarkThread, the collector state has already
  // been reset for the next pause.
  bool should_start_concurrent_mark_operation = collector_state()->in_concurrent_start_gc();

  // Perform the collection.
  G1YoungCollector collector(gc_cause());
  collector.collect();

  // It should now be safe to tell the concurrent mark thread to start
  // without its logging output interfering with the logging output
  // that came from the pause.
  if (should_start_concurrent_mark_operation) {
    verifier()->verify_bitmap_clear(true /* above_tams_only */);
    // CAUTION: after the start_concurrent_cycle() call below, the concurrent marking
    // thread(s) could be running concurrently with us. Make sure that anything
    // after this point does not assume that we are the only GC thread running.
    // Note: of course, the actual marking work will not start until the safepoint
    // itself is released in SuspendibleThreadSet::desynchronize().
    start_concurrent_cycle(collector.concurrent_operation_is_full_mark());
    ConcurrentGCBreakpoints::notify_idle_to_active();
  }
}
```

```c++
void G1YoungCollector::collect() {
  // Do timing/tracing/statistics/pre- and post-logging/verification work not
  // directly related to the collection. They should not be accounted for in
  // collection work timing.

  // The G1YoungGCTraceTime message depends on collector state, so must come after
  // determining collector state.
  G1YoungGCTraceTime tm(this, _gc_cause);

  // JFR
  G1YoungGCJFRTracerMark jtm(gc_timer_stw(), gc_tracer_stw(), _gc_cause);
  // JStat/MXBeans
  G1YoungGCMonitoringScope ms(monitoring_support(),
                              !collection_set()->candidates()->is_empty() /* all_memory_pools_affected */);
  // Create the heap printer before internal pause timing to have
  // heap information printed as last part of detailed GC log.
  G1HeapPrinterMark hpm(_g1h);
  // Young GC internal pause timing
  G1YoungGCNotifyPauseMark npm(this);

  // Verification may use the workers, so they must be set up before.
  // Individual parallel phases may override this.
  set_young_collection_default_active_worker_threads();

  // Wait for root region scan here to make sure that it is done before any
  // use of the STW workers to maximize cpu use (i.e. all cores are available
  // just to do that).
  wait_for_root_region_scanning();

  G1YoungGCVerifierMark vm(this);
  {
    // Actual collection work starts and is executed (only) in this scope.

    // Young GC internal collection timing. The elapsed time recorded in the
    // policy for the collection deliberately elides verification (and some
    // other trivial setup above).
    policy()->record_young_collection_start();

    pre_evacuate_collection_set(jtm.evacuation_info());

    G1ParScanThreadStateSet per_thread_states(_g1h,
                                              workers()->active_workers(),
                                              collection_set(),
                                              &_evac_failure_regions);

    bool may_do_optional_evacuation = collection_set()->optional_region_length() != 0;
    // Actually do the work...
    // è¿›è¡Œä¸‰è‰²æ ‡è®°
    evacuate_initial_collection_set(&per_thread_states, may_do_optional_evacuation);

    if (may_do_optional_evacuation) {
      evacuate_optional_collection_set(&per_thread_states);
    }
    post_evacuate_collection_set(jtm.evacuation_info(), &per_thread_states);

    // Refine the type of a concurrent mark operation now that we did the
    // evacuation, eventually aborting it.
    _concurrent_operation_is_full_mark = policy()->concurrent_operation_is_full_mark("Revise IHOP");

    // Need to report the collection pause now since record_collection_pause_end()
    // modifies it to the next state.
    jtm.report_pause_type(collector_state()->young_gc_pause_type(_concurrent_operation_is_full_mark));

    policy()->record_young_collection_end(_concurrent_operation_is_full_mark, evacuation_alloc_failed());
  }
  TASKQUEUE_STATS_ONLY(_g1h->task_queues()->print_and_reset_taskqueue_stats("Oop Queue");)
}
```

```c++
void G1YoungCollector::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states,
                                                      bool has_optional_evacuation_work) {
  G1GCPhaseTimes* p = phase_times();

  {
    Ticks start = Ticks::now();
    rem_set()->merge_heap_roots(true /* initial_evacuation */);
    p->record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
  }

  Tickspan task_time;
  const uint num_workers = workers()->active_workers();

  Ticks start_processing = Ticks::now();
  {
    G1RootProcessor root_processor(_g1h, num_workers);
    G1EvacuateRegionsTask g1_par_task(_g1h,
                                      per_thread_states,
                                      task_queues(),
                                      &root_processor,
                                      num_workers,
                                      has_optional_evacuation_work);
    task_time = run_task_timed(&g1_par_task);
    // Closing the inner scope will execute the destructor for the
    // G1RootProcessor object. By subtracting the WorkerThreads task from the total
    // time of this scope, we get the "NMethod List Cleanup" time. This list is
    // constructed during "STW two-phase nmethod root processing", see more in
    // nmethod.hpp
  }
  Tickspan total_processing = Ticks::now() - start_processing;

  p->record_initial_evac_time(task_time.seconds() * 1000.0);
  p->record_or_add_nmethod_list_cleanup_time((total_processing - task_time).seconds() * 1000.0);

  rem_set()->complete_evac_phase(has_optional_evacuation_work);
}

Tickspan G1YoungCollector::run_task_timed(WorkerTask* task) {
  Ticks start = Ticks::now();
  workers()->run_task(task);
  return Ticks::now() - start;
}
```

#### ğŸ—‘ShenandoahHeap

æœ¬ç®—æ³•å¯ä»¥å®ç°å¹¶å‘å›æ”¶(é€šè¿‡ **è½¬å‘æŒ‡é’ˆ** & **è¯»å±éšœ** )ã€‚ä½†ç”±äºè½¬å‘æŒ‡é’ˆçš„åŠ å…¥éœ€è¦è¦†ç›–æ‰€æœ‰å¯¹è±¡è®¿é—®çš„åœºæ™¯ï¼ŒåŒ…æ‹¬è¯»ã€å†™ã€åŠ é”ç­‰ç­‰ï¼Œæ‰€ä»¥éœ€è¦**åŒæ—¶è®¾ç½®è¯»å±éšœå’Œå†™å±éšœ**ã€‚

ç®—æ³•é‡‡ç”¨ **è¿æ¥çŸ©é˜µï¼ˆConnection Matrixï¼‰** æ›¿æ¢ g1 ä¸­çš„å¡è¡¨ã€‚

> 1. Init Mark å¹¶å‘æ ‡è®°çš„åˆå§‹åŒ–é˜¶æ®µï¼Œå®ƒä¸ºå¹¶å‘æ ‡è®°å‡†å¤‡å †å’Œåº”ç”¨çº¿ç¨‹ï¼Œç„¶åæ‰«ærooté›†åˆã€‚è¿™æ˜¯æ•´ä¸ªGCç”Ÿå‘½å‘¨æœŸç¬¬ä¸€æ¬¡åœé¡¿ï¼Œè¿™ä¸ªé˜¶æ®µä¸»è¦å·¥ä½œæ˜¯rooté›†åˆæ‰«æï¼Œæ‰€ä»¥åœé¡¿æ—¶é—´ä¸»è¦å–å†³äºrooté›†åˆå¤§å°ã€‚
> 2. Concurrent Marking è´¯ç©¿æ•´ä¸ªå †ï¼Œä»¥rooté›†åˆä¸ºèµ·ç‚¹ï¼Œè·Ÿè¸ªå¯è¾¾çš„æ‰€æœ‰å¯¹è±¡ã€‚ è¿™ä¸ªé˜¶æ®µå’Œåº”ç”¨ç¨‹åºä¸€èµ·è¿è¡Œï¼Œå³å¹¶å‘ï¼ˆconcurrentï¼‰ã€‚è¿™ä¸ªé˜¶æ®µçš„æŒç»­æ—¶é—´ä¸»è¦å–å†³äºå­˜æ´»å¯¹è±¡çš„æ•°é‡ï¼Œä»¥åŠå †ä¸­å¯¹è±¡å›¾çš„ç»“æ„ã€‚ç”±äºè¿™ä¸ªé˜¶æ®µï¼Œåº”ç”¨ä¾ç„¶å¯ä»¥åˆ†é…æ–°çš„æ•°æ®ï¼Œæ‰€ä»¥åœ¨å¹¶å‘æ ‡è®°é˜¶æ®µï¼Œå †å ç”¨ç‡ä¼šä¸Šå‡ã€‚
> 3. Final Mark æ¸…ç©ºæ‰€æœ‰å¾…å¤„ç†çš„æ ‡è®°/æ›´æ–°é˜Ÿåˆ—ï¼Œé‡æ–°æ‰«ærooté›†åˆï¼Œç»“æŸå¹¶å‘æ ‡è®°ã€‚è¿™ä¸ªé˜¶æ®µè¿˜ä¼šææ˜ç™½éœ€è¦è¢«æ¸…ç†ï¼ˆevacuatedï¼‰çš„regionï¼ˆå³åƒåœ¾æ”¶é›†é›†åˆï¼‰ï¼Œå¹¶ä¸”é€šå¸¸ä¸ºä¸‹ä¸€é˜¶æ®µåšå‡†å¤‡ã€‚æœ€ç»ˆæ ‡è®°æ˜¯æ•´ä¸ªGCå‘¨æœŸçš„ç¬¬äºŒä¸ªåœé¡¿é˜¶æ®µï¼Œè¿™ä¸ªé˜¶æ®µçš„éƒ¨åˆ†å·¥ä½œèƒ½åœ¨å¹¶å‘é¢„æ¸…ç†é˜¶æ®µå®Œæˆï¼Œè¿™ä¸ªé˜¶æ®µæœ€è€—æ—¶çš„è¿˜æ˜¯æ¸…ç©ºé˜Ÿåˆ—å’Œæ‰«ærooté›†åˆã€‚
> 4. Concurrent Cleanup å›æ”¶å³æ—¶åƒåœ¾åŒºåŸŸ -- è¿™äº›åŒºåŸŸæ˜¯æŒ‡å¹¶å‘æ ‡è®°åï¼Œæ¢æµ‹ä¸åˆ°ä»»ä½•å­˜æ´»çš„å¯¹è±¡ã€‚
> 5. Concurrent Evacuation ä»åƒåœ¾æ”¶é›†é›†åˆä¸­æ‹·è´å­˜æ´»çš„å¯¹åˆ°å…¶ä»–çš„regionä¸­ï¼Œè¿™æ˜¯æœ‰åˆ«äºOpenJDKå…¶ä»–GCä¸»è¦çš„ä¸åŒç‚¹ã€‚è¿™ä¸ªé˜¶æ®µèƒ½å†æ¬¡å’Œåº”ç”¨ä¸€èµ·è¿è¡Œï¼Œæ‰€ä»¥åº”ç”¨ä¾ç„¶å¯ä»¥ç»§ç»­åˆ†é…å†…å­˜ï¼Œè¿™ä¸ªé˜¶æ®µæŒç»­æ—¶é—´ä¸»è¦å–å†³äºé€‰ä¸­çš„åƒåœ¾æ”¶é›†é›†åˆå¤§å°ï¼ˆæ¯”å¦‚æ•´ä¸ªå †åˆ’åˆ†128ä¸ªregionï¼Œå¦‚æœæœ‰16ä¸ªregionè¢«é€‰ä¸­ï¼Œå…¶è€—æ—¶è‚¯å®šè¶…è¿‡8ä¸ªregionè¢«é€‰ä¸­ï¼‰ã€‚
> 6. Init Update Refs åˆå§‹åŒ–æ›´æ–°å¼•ç”¨é˜¶æ®µï¼Œå®ƒé™¤äº†ç¡®ä¿æ‰€æœ‰GCçº¿ç¨‹å’Œåº”ç”¨çº¿ç¨‹å·²ç»å®Œæˆå¹¶å‘Evacuationé˜¶æ®µï¼Œä»¥åŠä¸ºä¸‹ä¸€é˜¶æ®µGCåšå‡†å¤‡ä»¥å¤–ï¼Œå…¶ä»–ä»€ä¹ˆéƒ½æ²¡æœ‰åšã€‚è¿™æ˜¯æ•´ä¸ªGCå‘¨æœŸä¸­ï¼Œç¬¬ä¸‰æ¬¡åœé¡¿ï¼Œä¹Ÿæ˜¯æ—¶é—´æœ€çŸ­çš„ä¸€æ¬¡ã€‚
> 7. Concurrent Update References å†æ¬¡éå†æ•´ä¸ªå †ï¼Œæ›´æ–°é‚£äº›åœ¨å¹¶å‘evacuationé˜¶æ®µè¢«ç§»åŠ¨çš„å¯¹è±¡çš„å¼•ç”¨ã€‚è¿™ä¹Ÿæ˜¯æœ‰åˆ«äºOpenJDKå…¶ä»–GCä¸»è¦çš„ä¸åŒï¼Œè¿™ä¸ªé˜¶æ®µæŒç»­æ—¶é—´ä¸»è¦å–å†³äºå †ä¸­å¯¹è±¡çš„æ•°é‡ï¼Œå’Œå¯¹è±¡å›¾ç»“æ„æ— å…³ï¼Œå› ä¸ºè¿™ä¸ªè¿‡ç¨‹æ˜¯çº¿æ€§æ‰«æå †ã€‚è¿™ä¸ªé˜¶æ®µæ˜¯å’Œåº”ç”¨ä¸€èµ·å¹¶å‘è¿è¡Œçš„ã€‚
> 8. Final Update Refs é€šè¿‡å†æ¬¡æ›´æ–°ç°æœ‰çš„rooté›†åˆå®Œæˆæ›´æ–°å¼•ç”¨é˜¶æ®µï¼Œå®ƒä¹Ÿä¼šå›æ”¶æ”¶é›†é›†åˆä¸­çš„regionï¼Œå› ä¸ºç°åœ¨çš„å †å·²ç»æ²¡æœ‰å¯¹è¿™äº›regionä¸­çš„å¯¹è±¡çš„å¼•ç”¨ã€‚

#### ğŸ—‘ZCollectedHeap

##### Region å¸ƒå±€

> 1. å°å‹Regionå®¹é‡å›ºå®šä¸º2MBï¼Œç”¨äºå­˜æ”¾å°äº256KBçš„å¯¹è±¡ã€‚
> 2. ä¸­å‹Regionå®¹é‡å›ºå®šä¸º32MBï¼Œç”¨äºå­˜æ”¾å¤§äºç­‰äº256KBä½†ä¸è¶³4MBçš„å¯¹è±¡ã€‚
> 3. å¤§å‹Regionå®¹é‡ä¸º2MBçš„æ•´æ•°å€ï¼Œå­˜æ”¾4MBåŠä»¥ä¸Šå¤§å°çš„å¯¹è±¡ï¼Œè€Œä¸”æ¯ä¸ªå¤§å‹Regionä¸­åªå­˜æ”¾ä¸€ä¸ªå¤§å¯¹è±¡ã€‚ç”±äºå¤§å¯¹è±¡ç§»åŠ¨ä»£ä»·è¿‡å¤§ï¼Œæ‰€ä»¥è¯¥å¯¹è±¡ä¸ä¼šè¢«é‡åˆ†é…ã€‚

##### âœ¨è¯»å±éšœ

å¯¹æ¯” g1 æ”¶é›†å™¨ï¼ŒZGC é‡‡ç”¨çš„æ˜¯è¯»å±éšœï¼Œè¿™ä½¿å¾—æœ‰æœºä¼šæ‹¦æˆª Java ä»£ç ä¸­å¯¹å¯¹è±¡çš„è®¿é—®ï¼Œå¦‚æœè®¿é—®çš„å¯¹è±¡å†…å­˜åœ°å€å› ä¸º gc è¢«ç§»åŠ¨ï¼Œé‚£ä¹ˆå¯ä»¥åœ¨æ‹¦æˆªçš„æ—¶å€™æ³¨å…¥æŒ‡é’ˆä¿®å¤æ“ä½œï¼Œå¦‚æ­¤å°±ä¸éœ€è¦é€šè¿‡ STW å®Œæˆç§»åŠ¨äº†ã€‚

##### âœ¨æŒ‡é’ˆç€è‰²æŠ€æœ¯

æŒ‡é’ˆç€è‰²çš„æ„æ€æ˜¯å€Ÿç”¨ 64 ä½æŒ‡é’ˆçš„ä¸€éƒ¨åˆ†**é«˜ä½**ç”¨äºæ ‡è®° åœ°å€å±äºå“ªå— Regionï¼ŒåŒ…å«ä¸‹åˆ—ä¸‰ç§çŠ¶æ€ï¼š

> - Mark 0
> - Mark 1
> - Remapped  (æ–°äº§ç”Ÿ/è¢«é‡åˆ†é…çš„å¯¹è±¡ä¸ºæ­¤åˆå§‹ç±»å‹)

åœ¨**å¹¶å‘æ ‡è®°**è¿‡ç¨‹ä¸­ï¼ŒZGC ç›´æ¥åœ¨æŒ‡é’ˆä¸Šåšæ ‡è®°ï¼Œè€Œéå¦‚ g1 ä¸€æ ·åœ¨å¯¹è±¡å¤´éƒ¨é™„åŠ ä¿¡æ¯å¤„åšæ ‡è®°ï¼Œå¹¶æ ‡è®°ä¸ºM0/M1(ä¸ from/to å¤åˆ¶ç§»åŠ¨ä¸€å®šç¨‹åº¦ä¸Šç›¸ä¼¼)ã€‚

å›æ”¶è¿‡ç¨‹å°†å‘ç”Ÿé‡åˆ†é…ï¼Œå³å¤åˆ¶å­˜æ´»å¯¹è±¡åˆ°æ–° regionï¼ˆæ­¤åæ ‡è®°ä¸º remappedï¼‰ï¼Œå¹¶ä¸”ä¸ºæ¯ä¸ª region ç»´æŠ¤ä¸€ä¸ªè½¬å‘è¡¨ï¼Œè®°å½•ä»æ—§å¯¹è±¡åˆ°æ–°å¯¹è±¡çš„è½¬å‘å…³ç³»ã€‚å¦‚æœæ­¤æ—¶ç”¨æˆ·çº¿ç¨‹è®¿é—®äº†é‡åˆ†é…é›†(M0/M1)çš„å¯¹è±¡ï¼Œå³å¯æ ¹æ®è½¬å‘è¡¨åœ¨è¯»å±éšœä¸‹ç›´æ¥ä¿®å¤æŒ‡é’ˆã€‚

ä¸€æ—¦ region æ‰€æœ‰å­˜æ´»å¯¹è±¡ä¸€å®šå®Œæˆï¼Œå³å¯ç›´æ¥é‡Šæ”¾ regionï¼Œä½†æš‚æ—¶ä¿ç•™è½¬å‘è¡¨ã€‚

æ¥ä¸‹æ¥è¿›è¡Œé‡æ˜ å°„ï¼Œä¿®æ­£æ•´ä¸ªå †ä¸­æŒ‡å‘é‡åˆ†é…é›†ä¸­æ—§å¯¹è±¡çš„æ‰€æœ‰å¼•ç”¨ï¼Œå¹¶é€æ­¥é‡Šæ”¾åŸæ¥çš„è½¬å‘è¡¨ã€‚

åœ¨é‡æ˜ å°„è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å‘ç”Ÿç¬¬äºŒæ¬¡å¹¶å‘æ ‡è®°ï¼Œæ­¤æ¬¡æ ‡è®°ä¸º M1/M0ï¼Œä»¥æ­¤ä¸ä¸Šæ¬¡å­˜æ´»å¯¹è±¡ä½œåŒºåˆ†ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼š**M0ã€M1 åœ¨æ¯è½®æ ‡è®°ä¸­åªæœ‰ä¸€ç§æ˜¯æ´»è·ƒçš„ï¼Œä¸”æ­¤æ—¶ç”¨æˆ·çº¿ç¨‹åˆ›å»ºçš„æ–°å˜é‡ä¸ºM0/M1å…¶ä¸­ä¸€ç§å½“è½®çš„æ´»è·ƒçŠ¶æ€ã€‚**



> 1. åˆå§‹æ ‡è®°(Mark Start)ï¼šå…ˆSTWï¼Œå¹¶è®°å½•ä¸‹gc rootsç›´æ¥å¼•ç”¨çš„å¯¹è±¡ã€‚
> 2. å¹¶å‘æ ‡è®°ï¼ˆConcurrent Markï¼‰ï¼šä¸G1ä¸€æ ·ï¼Œå¹¶å‘æ ‡è®°æ˜¯éå†å¯¹è±¡å›¾å¯è¾¾æ€§åˆ†æçš„é˜¶æ®µï¼Œå®ƒçš„åˆå§‹åŒ–æ ‡è®°ï¼ˆMark Startï¼‰å’Œæœ€ç»ˆæ ‡è®°ï¼ˆMark Endï¼‰ä¹Ÿä¼šå‡ºç°çŸ­æš‚çš„åœé¡¿ï¼Œä¸G1ä¸åŒçš„æ˜¯ï¼ŒZGCçš„æ ‡è®°æ˜¯åœ¨æŒ‡é’ˆä¸Šè€Œä¸æ˜¯åœ¨å¯¹è±¡ä¸Šè¿›è¡Œçš„ï¼Œæ ‡è®°é˜¶æ®µä¼šæ›´æ–°é¢œè‰²æŒ‡é’ˆï¼ˆè§ä¸‹é¢è¯¦è§£ï¼‰ä¸­çš„ Marked0ã€Marked1æ ‡å¿—ä½ã€‚è®°å½•åœ¨æŒ‡é’ˆçš„å¥½å¤„å°±æ˜¯å¯¹è±¡å›æ”¶ä¹‹åï¼Œè¿™å—å†…å­˜å°±å¯ä»¥ç«‹å³ä½¿ç”¨ã€‚å­˜åœ¨å¯¹è±¡ä¸Šçš„æ—¶å€™å°±ä¸èƒ½é©¬ä¸Šä½¿ç”¨ï¼Œå› ä¸ºå®ƒä¸Šé¢è¿˜å­˜æ”¾ç€ä¸€äº›åƒåœ¾å›æ”¶çš„ä¿¡æ¯ï¼Œéœ€è¦æ¸…ç†å®Œæˆä¹‹åæ‰èƒ½ä½¿ç”¨ã€‚
> 3. å†æ ‡è®°å’Œéå¼ºæ ¹å¹¶è¡Œæ ‡è®°ï¼Œåœ¨å¹¶å‘æ ‡è®°ç»“æŸåå°è¯•ç»ˆç»“æ ‡è®°åŠ¨ä½œï¼Œç†è®ºä¸Šå¹¶å‘æ ‡è®°ç»“æŸåæ‰€æœ‰å¾…æ ‡è®°çš„å¯¹è±¡ä¼šå…¨éƒ¨å®Œæˆï¼Œä½†æ˜¯å› ä¸ºGCå·¥ä½œçº¿ç¨‹å’Œåº”ç”¨ç¨‹åºçº¿ç¨‹æ˜¯å¹¶å‘è¿è¡Œï¼Œæ‰€ä»¥å¯èƒ½å­˜åœ¨GCå·¥ä½œçº¿ç¨‹æ‰§è¡Œç»“æŸæ ‡è®°æ—¶ï¼Œåº”ç”¨ç¨‹åºçº¿ç¨‹åˆæœ‰æ–°çš„å¼•ç”¨å…³ç³»å˜åŒ–å¯¼è‡´æ¼æ ‡è®°ï¼Œæ‰€ä»¥è¿™ä¸€æ­¥å…ˆåˆ¤æ–­æ˜¯å¦çœŸçš„ç»“æŸäº†å¯¹è±¡çš„æ ‡è®°ï¼Œå¦‚æœæ²¡æœ‰ç»“æŸå°±è¿˜ä¼šå¯åŠ¨å¹¶è¡Œæ ‡è®°ï¼Œæ‰€ä»¥è¿™ä¸€æ­¥éœ€è¦STWã€‚å¦å¤–ï¼Œåœ¨è¯¥æ­¥ä¸­ï¼Œè¿˜ä¼šå¯¹éå¼ºæ ¹ï¼ˆè½¯å¼•ç”¨ï¼Œè™šå¼•ç”¨ç­‰ï¼‰è¿›è¡Œå¹¶è¡Œæ ‡è®°ã€‚
> 4. å¹¶å‘é¢„å¤‡é‡åˆ†é…ï¼ˆConcurrent Prepare for Relocateï¼‰ï¼šè¿™ä¸ªé˜¶æ®µéœ€è¦æ ¹æ®ç‰¹å®šçš„æŸ¥è¯¢æ¡ä»¶ç»Ÿè®¡å¾—å‡ºæœ¬æ¬¡æ”¶é›†è¿‡ç¨‹è¦æ¸…ç†é‚£äº› Regionï¼Œå°†è¿™äº› Regionç»„æˆé‡åˆ†é…é›†ï¼ˆRelocation Setï¼‰ã€‚ZGC æ¯æ¬¡å›æ”¶éƒ½ä¼šæ‰«ææ‰€æœ‰çš„ Regionï¼Œç”¨èŒƒå›´æ›´å¤§çš„æ‰«ææˆæœ¬æ¢å–G1ä¸­è®°å¿†é›†å’Œç»´æŠ¤æˆæœ¬ã€‚
> 5. åˆå§‹è½¬ç§»ï¼šè½¬ç§»æ ¹å¯¹è±¡å¼•ç”¨çš„å¯¹è±¡ï¼Œè¯¥æ­¥éœ€è¦STWã€‚
> 6. å¹¶å‘é‡åˆ†é…ï¼ˆConcurrent Relocateï¼‰ï¼šé‡åˆ†é…æ˜¯ ZGCæ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ ¸å¿ƒé˜¶æ®µï¼Œè¿™ä¸ªè¿‡ç¨‹è¦æŠŠé‡åˆ†é…é›†ä¸­çš„å­˜æ´»å¯¹è±¡å¤åˆ¶åˆ°æ–°çš„ Regionä¸Šï¼Œå¹¶ä¸ºé‡åˆ†é…é›†ä¸­çš„æ¯ä¸ª Regionç»´æŠ¤äº†ä¸€ä¸ªè½¬å‘è¡¨ï¼ˆForward Tableï¼‰ï¼Œè®°å½•ä»æ—§å¯¹è±¡åˆ°æ–°å¯¹è±¡çš„è½¬æ¢å…³ç³»ã€‚ZGCæ”¶é›†å™¨èƒ½ä»…ä»å¼•ç”¨ä¸Šå°±æ˜ç¡®å¾—çŸ¥ä¸€ä¸ªå¯¹è±¡æ˜¯å¦å¤„äºé‡åˆ†é…é›†ä¸­ï¼Œå¦‚æœç”¨æˆ·çº¿ç¨‹æ­¤æ—¶å¹¶å‘è®¿é—®äº†ä½äºé‡åˆ†é…é›†ä¸­çš„å¯¹è±¡ï¼Œè¿™æ¬¡è®¿é—®å°†ä¼šè¢«é¢„ç½®çš„å†…å­˜å±éšœæ‰€æˆªè·ï¼Œç„¶åç«‹å³æ ¹æ® Regionä¸Šçš„è½¬å‘è¡¨è®°å½•å°†è®¿é—®è½¬åˆ°æ–°å¤åˆ¶çš„å¯¹è±¡ä¸Šï¼Œå¹¶åŒæ—¶ä¿®æ­£æ›´æ–°è¯¥å¼•ç”¨çš„å€¼ï¼Œä½¿å…¶ç›´æ¥æŒ‡å‘æ–°å¯¹è±¡ï¼ŒZGCå°†è¿™ç§è¡Œä¸ºç§°ä¸ºæŒ‡é’ˆçš„â€œè‡ªæ„ˆâ€ï¼ˆSelf-Healingï¼‰èƒ½åŠ›ã€‚
> 7. å¹¶å‘é‡æ˜ å°„ï¼ˆConcurrent Remap)ï¼šé‡æ˜ å°„æ‰€åšçš„å°±æ˜¯ä¿®æ­£æ•´ä¸ªå †ä¸­æŒ‡å‘é‡åˆ†é…é›†ä¸­æ—§å¯¹è±¡çš„æ‰€æœ‰å¼•ç”¨ï¼Œä½†æ˜¯ZGCä¸­å¯¹è±¡å¼•ç”¨å­˜åœ¨â€œè‡ªæ„ˆâ€åŠŸèƒ½ï¼Œæ‰€ä»¥è¿™ä¸ªé‡æ˜ å°„æ“ä½œå¹¶ä¸æ˜¯å¾ˆè¿«åˆ‡ã€‚ZGCå¾ˆå·§å¦™åœ°æŠŠå¹¶å‘é‡æ˜ å°„é˜¶æ®µè¦åšçš„å·¥ä½œï¼Œåˆå¹¶åˆ°äº†ä¸‹ä¸€æ¬¡åƒåœ¾æ”¶é›†å¾ªç¯ä¸­çš„å¹¶å‘æ ‡è®°é˜¶æ®µé‡Œå»å®Œæˆï¼Œåæ­£ä»–ä»¬éƒ½æ˜¯è¦éå†æ‰€æœ‰å¯¹è±¡ï¼Œè¿™æ ·åˆå¹¶å°±èŠ‚çœäº†ä¸€æ¬¡éå†å¯¹è±¡å›¾çš„å¼€é”€ã€‚ä¸€æ—¦æ‰€æœ‰æŒ‡é’ˆéƒ½è¢«ä¿®æ­£ä¹‹åï¼ŒåŸæ¥è®°å½•æ–°æ—§å¯¹è±¡å…³ç³»çš„è½¬å‘è¡¨å°±å¯ä»¥é‡Šæ”¾æ‰äº†ã€‚

## ğŸ–Šç±»åŠ è½½å™¨

#### ç­‰çº§åŠ è½½æœºåˆ¶

ç±»åŠ è½½éµå¾ªä»¥ä¸‹è´£ä»»é¡ºåº ( **å§”æ´¾åŸåˆ™** )ï¼šExtClassLoader ( *PlatformClassLoader* ) -> AppClassLoader -> è‡ªå®šä¹‰ ç±»åŠ è½½å™¨

å¯¹äº JVM è‡ªèº«å·¥ä½œéœ€è¦çš„ç±»ï¼Œè¿™äº›åŠ è½½çš„ç±»ä¸€èˆ¬ä¸æš´éœ²ç»™ Java å¼€å‘è€…ï¼Œé€šè¿‡ Bootstrap ClassLoader è¿›è¡ŒåŠ è½½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ*Bootstrap ClassLoader* æ²¡æœ‰å­ç±»ï¼Œä¹Ÿä¸éµå¾ªå§”æ´¾åŸåˆ™ã€‚

#### å®ç°è‡ªå®šä¹‰ ClassLoader

ä»¥ä¸‹æƒ…å†µä¸€èˆ¬éœ€è¦å®ç°è‡ªå®šä¹‰çš„ ClassLoader:

1. åŠ è½½è‡ªå®šä¹‰è·¯å¾„ä¸‹çš„ class ç±»æ–‡ä»¶
2. å¯¹è¦åŠ è½½çš„ç±»åšç‰¹æ®Šå¤„ç†ï¼Œå¦‚ä¿è¯ç½‘ç»œä¼ è¾“çš„ç±»çš„å®‰å…¨æ€§ï¼Œå¯ä»¥åŠ å¯†åå†ä¼ è¾“ï¼Œåœ¨åŠ è½½åˆ° JVM å‰éœ€è¦å¯¹å­—èŠ‚ç è¿›è¡Œè§£å¯†
3. è‡ªå®šä¹‰ç±»çš„å®ç°æœºåˆ¶ã€‚

ä»¥ä¸‹ä¸º ClassLoader çš„åŠ è½½ç±»æ—¶å…³é”®å‡½æ•°çš„è°ƒç”¨é¡ºåºï¼š

> loadClass - > findClass -> defineClass ( final æ–¹æ³• )

å¯é€šè¿‡ä¿®æ”¹ findClass åŠ è½½è‡ªå®šä¹‰è·¯å¾„ä¸‹çš„ç±» (éœ€è¦éµå¾ªå§”æ´¾åŸåˆ™)ï¼Œæ‰‹åŠ¨è°ƒç”¨ **defineClass** æ ¹æ®å­—èŠ‚æµåˆ›å»ºä¸€ä¸ª Class å¯¹è±¡ã€‚

#### çº¿ç¨‹ä¸Šä¸‹æ–‡ç±»åŠ è½½å™¨

å¯é€šè¿‡

> Thread.*currentThread*().getContextClassLoader()
>
> Thread.*currentThread*().setContextClassLoader(ClassLoader)

æ¥è·å¾—/è®¾ç½®çº¿ç¨‹ä¸Šä¸‹æ–‡ç±»åŠ è½½å™¨ã€‚è‹¥æ— ç‰¹åˆ«æŒ‡å®šï¼Œé»˜è®¤å€¼ä¸º App ClassLoader

##### âœˆSPI ( Service Provider Interface )

åœ¨SPI çš„å®ç°ä¸­ï¼Œçº¿ç¨‹ä¸Šä¸‹æ–‡ç±»åŠ è½½å™¨ å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚

###### SPI è§„èŒƒ

> 1. å…ˆç¼–å†™å¥½æœåŠ¡æ¥å£çš„å®ç°ç±»ï¼Œå³ æœåŠ¡æä¾›ç±»ã€‚
> 2. ç„¶ååœ¨ classpath çš„ **META-INF/services** ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªä»¥ **æ¥å£å…¨é™å®šå** å‘½åçš„ UTF-8 æ–‡æœ¬æ–‡ä»¶ï¼Œå¹¶åœ¨è¯¥æ–‡ä»¶ä¸­å†™å…¥**å®ç°ç±»çš„å…¨é™å®šå**ï¼ˆå¦‚æœæœ‰å¤šä¸ªå®ç°ç±»ï¼Œä»¥æ¢è¡Œç¬¦åˆ†éš”ï¼‰
> 3. æœ€åè°ƒç”¨ JDK ä¸­çš„ java.util.ServiceLoader ç»„ä»¶ä¸­çš„ **load()** æ–¹æ³•ï¼Œå°±ä¼šæ ¹æ®ä¸Šè¿°æ–‡ä»¶æ¥å‘ç°å¹¶åŠ è½½å…·ä½“çš„æœåŠ¡å®ç°ã€‚

###### SPI å®ç°åŸç†

æ¥ä¸‹æ¥ä»¥ Mysql JDBC ä¸ºä¾‹ç®€ä»‹ SPI çš„å®ç°åŸç†ã€‚

ç”± 

> DriverManager èŠ‚

çŸ¥ **getConnection** å‡½æ•°å°†è°ƒç”¨ä¸‹åˆ—å‡½æ•°ï¼š

```java
/*
 * Load the initial JDBC drivers by checking the System property
 * jdbc.drivers and then use the {@code ServiceLoader} mechanism
 */
@SuppressWarnings("removal")
private static void ensureDriversInitialized() {
    if (driversInitialized) {
        return;
    }

    synchronized (lockForInitDrivers) {
        if (driversInitialized) {
            return;
        }
        String drivers;
        try {
            drivers = AccessController.doPrivileged(new PrivilegedAction<String>() {
                public String run() {
                    return System.getProperty(JDBC_DRIVERS_PROPERTY);
                }
            });
        } catch (Exception ex) {
            drivers = null;
        }
        // If the driver is packaged as a Service Provider, load it.
        // Get all the drivers through the classloader
        // exposed as a java.sql.Driver.class service.
        // ServiceLoader.load() replaces the sun.misc.Providers()

        AccessController.doPrivileged(new PrivilegedAction<Void>() {
            public Void run() {

                ServiceLoader<Driver> loadedDrivers = ServiceLoader.load(Driver.class);
                Iterator<Driver> driversIterator = loadedDrivers.iterator();

                /* Load these drivers, so that they can be instantiated.
                 * It may be the case that the driver class may not be there
                 * i.e. there may be a packaged driver with the service class
                 * as implementation of java.sql.Driver but the actual class
                 * may be missing. In that case a java.util.ServiceConfigurationError
                 * will be thrown at runtime by the VM trying to locate
                 * and load the service.
                 *
                 * Adding a try catch block to catch those runtime errors
                 * if driver not available in classpath but it's
                 * packaged as service and that service is there in classpath.
                 */
                try {
                    while (driversIterator.hasNext()) {
                        driversIterator.next();
                    }
                } catch (Throwable t) {
                    // Do nothing
                }
                return null;
            }
        });

        println("DriverManager.initialize: jdbc.drivers = " + drivers);

        if (drivers != null && !drivers.isEmpty()) {
            String[] driversList = drivers.split(":");
            println("number of Drivers:" + driversList.length);
            for (String aDriver : driversList) {
                try {
                    println("DriverManager.Initialize: loading " + aDriver);
                    Class.forName(aDriver, true,
                            ClassLoader.getSystemClassLoader());
                } catch (Exception ex) {
                    println("DriverManager.Initialize: load failed: " + ex);
                }
            }
        }

        driversInitialized = true;
        println("JDBC DriverManager initialized");
    }
}
```
ä¸ SPI æœºåˆ¶ç›¸å…³çš„æ ¸å¿ƒä»£ç ä¸º

```java
ServiceLoader<Driver> loadedDrivers = ServiceLoader.load(Driver.class);
Iterator<Driver> driversIterator = loadedDrivers.iterator();

try {
    while (driversIterator.hasNext()) {
        driversIterator.next();
    }
} catch (Throwable t) {
    // Do nothing
}
```

å…¶ä¸­ load æ–¹æ³•è¿”å›ä¸€ä¸ª **ServiceLoader** å¯¹è±¡ï¼Œå…¶ä¸­å±æ€§å€¼ æŒ‡å®šäº† è¦å®ç°äº†æœåŠ¡æ¥å£ç±»ä¸ ç±»åŠ è½½å™¨(çº¿ç¨‹ä¸Šä¸‹æ–‡åŠ è½½å™¨)ã€‚

```java
/**
 * Creates a new service loader for the given service type, using the
 * current thread's {@linkplain java.lang.Thread#getContextClassLoader
 * context class loader}.
 *
 * <p> An invocation of this convenience method of the form
 * <pre>{@code
 *     ServiceLoader.load(service)
 * }</pre>
 *
 * is equivalent to
 *
 * <pre>{@code
 *     ServiceLoader.load(service, Thread.currentThread().getContextClassLoader())
 * }</pre>
 *
 * @apiNote Service loader objects obtained with this method should not be
 * cached VM-wide. For example, different applications in the same VM may
 * have different thread context class loaders. A lookup by one application
 * may locate a service provider that is only visible via its thread
 * context class loader and so is not suitable to be located by the other
 * application. Memory leaks can also arise. A thread local may be suited
 * to some applications.
 *
 * @param  <S> the class of the service type
 *
 * @param  service
 *         The interface or abstract class representing the service
 *
 * @return A new service loader
 *
 * @throws ServiceConfigurationError
 *         if the service type is not accessible to the caller or the
 *         caller is in an explicit module and its module descriptor does
 *         not declare that it uses {@code service}
 *
 * @revised 9
 */
@CallerSensitive
public static <S> ServiceLoader<S> load(Class<S> service) {
    ClassLoader cl = Thread.currentThread().getContextClassLoader();
    return new ServiceLoader<>(Reflection.getCallerClass(), service, cl);
}
```

```java
/**
 * Initializes a new instance of this class for locating service providers
 * via a class loader.
 *
 * @throws ServiceConfigurationError
 *         If {@code svc} is not accessible to {@code caller} or the caller
 *         module does not use the service type.
 */
@SuppressWarnings("removal")
private ServiceLoader(Class<?> caller, Class<S> svc, ClassLoader cl) {
    Objects.requireNonNull(svc);

    if (VM.isBooted()) {
        checkCaller(caller, svc);
        if (cl == null) {
            cl = ClassLoader.getSystemClassLoader();
        }
    } else {

        // if we get here then it means that ServiceLoader is being used
        // before the VM initialization has completed. At this point then
        // only code in the java.base should be executing.
        Module callerModule = caller.getModule();
        Module base = Object.class.getModule();
        Module svcModule = svc.getModule();
        if (callerModule != base || svcModule != base) {
            fail(svc, "not accessible to " + callerModule + " during VM init");
        }

        // restricted to boot loader during startup
        cl = null;
    }

    this.service = svc;
    this.serviceName = svc.getName();
    this.layer = null;
    this.loader = cl;
    this.acc = (System.getSecurityManager() != null)
            ? AccessController.getContext()
            : null;
}
```

æ¥ä¸‹æ¥çš„

> while (driversIterator.hasNext()) {
>         driversIterator.next();
> }

æ–¹æ³•å°†è¿›è¡Œå¤šæ¬¡è¿­ä»£ï¼Œå…¶ä¸­ è¿­ä»£æ–¹æ³• åœ¨ ServiceLoader ä¸­è¢«é‡å†™å¦‚ä¸‹ï¼š

```java
public Iterator<S> iterator() {

        // create lookup iterator if needed
        if (lookupIterator1 == null) {
            lookupIterator1 = newLookupIterator();
        }

        return new Iterator<S>() {

            // record reload count
            final int expectedReloadCount = ServiceLoader.this.reloadCount;

            // index into the cached providers list
            int index;

            /**
             * Throws ConcurrentModificationException if the list of cached
             * providers has been cleared by reload.
             */
            private void checkReloadCount() {
                if (ServiceLoader.this.reloadCount != expectedReloadCount)
                    throw new ConcurrentModificationException();
            }

            @Override
            public boolean hasNext() {
                checkReloadCount();
                if (index < instantiatedProviders.size())
                    return true;
                return lookupIterator1.hasNext();
            }

            @Override
            public S next() {
                checkReloadCount();
                S next;
                if (index < instantiatedProviders.size()) {
                    next = instantiatedProviders.get(index);
                } else {
                    next = lookupIterator1.next().get();
                    instantiatedProviders.add(next);
                }
                index++;
                return next;
            }

        };
    }
```

ä»ä¸Šè¿°ä»£ç å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå½“

> instantiatedProviders

æœªç¼“å­˜æ—¶ï¼Œå°†é€šè¿‡ 

> lookupIterator1

è·å– ServiceLoader å®ä¾‹ï¼Œå®ä¾‹çš„ get æ–¹æ³•å¯åˆ›å»º *æœåŠ¡ç±»å®ä¾‹*

```java
// The lazy-lookup iterator for iterator operations
private Iterator<Provider<S>> lookupIterator1;
private final List<S> instantiatedProviders = new ArrayList<>();
```

```java
ProviderImpl(Class<S> service,
             Class<? extends S> type,
             Constructor<? extends S> ctor,
             @SuppressWarnings("removal") AccessControlContext acc) {
    this.service = service;
    this.type = type;
    this.factoryMethod = null;
    this.ctor = ctor;
    this.acc = acc;
}

@Override
public Class<? extends S> type() {
    return type;
}

@Override
public S get() {
    if (factoryMethod != null) {
        return invokeFactoryMethod();
    } else {
        return newInstance();
    }
}

/**
 * Invokes the provider's "provider" method to instantiate a provider.
 * When running with a security manager then the method runs with
 * permissions that are restricted by the security context of whatever
 * created this loader.
 */
@SuppressWarnings("removal")
private S invokeFactoryMethod() {
    Object result = null;
    Throwable exc = null;
    if (acc == null) {
        try {
            result = factoryMethod.invoke(null);
        } catch (Throwable x) {
            exc = x;
        }
    } else {
        PrivilegedExceptionAction<?> pa = new PrivilegedExceptionAction<>() {
            @Override
            public Object run() throws Exception {
                return factoryMethod.invoke(null);
            }
        };
        // invoke factory method with permissions restricted by acc
        try {
            result = AccessController.doPrivileged(pa, acc);
        } catch (Throwable x) {
            if (x instanceof PrivilegedActionException)
                x = x.getCause();
            exc = x;
        }
    }
    if (exc != null) {
        if (exc instanceof InvocationTargetException)
            exc = exc.getCause();
        fail(service, factoryMethod + " failed", exc);
    }
    if (result == null) {
        fail(service, factoryMethod + " returned null");
    }
    @SuppressWarnings("unchecked")
    S p = (S) result;
    return p;
}

/**
 * Invokes Constructor::newInstance to instantiate a provider. When running
 * with a security manager then the constructor runs with permissions that
 * are restricted by the security context of whatever created this loader.
 */
@SuppressWarnings("removal")
private S newInstance() {
    S p = null;
    Throwable exc = null;
    if (acc == null) {
        try {
            p = ctor.newInstance();
        } catch (Throwable x) {
            exc = x;
        }
    } else {
        PrivilegedExceptionAction<S> pa = new PrivilegedExceptionAction<>() {
            @Override
            public S run() throws Exception {
                return ctor.newInstance();
            }
        };
        // invoke constructor with permissions restricted by acc
        try {
            p = AccessController.doPrivileged(pa, acc);
        } catch (Throwable x) {
            if (x instanceof PrivilegedActionException)
                x = x.getCause();
            exc = x;
        }
    }
    if (exc != null) {
        if (exc instanceof InvocationTargetException)
            exc = exc.getCause();
        String cn = ctor.getDeclaringClass().getName();
        fail(service,
             "Provider " + cn + " could not be instantiated", exc);
    }
    return p;
}
```

å½“ç„¶ï¼Œç”±äºæ‰€æœ‰é©±åŠ¨éƒ½ç”± DriverManager ç®¡ç†ï¼Œæ•…æ­¤ load æ–¹æ³•è°ƒç”¨å°†éå†æ‰€æœ‰å®ç°äº†

> java.sql.Driver

çš„ç¬¬ä¸‰æ–¹æ‰©å±•ç±»ã€‚

## ğŸ–Šç½‘ç»œ

### å¥—æ¥å­— â€”â€” ä¼ è¾“å±‚

| åè®® |            å¥—æ¥å­—            |
| :--: | :--------------------------: |
| TCP  |            Socket            |
| UDP  |        DatagramSocket        |
|  IP  | Java æ ‡å‡†åº“ä¸æ”¯æŒ åŸå§‹å¥—æ¥å­— |

### Http åè®®åº“ â€”â€” åº”ç”¨å±‚

|   ç«¯   |   å®ç°ç±»   |
| :----: | :--------: |
| å®¢æˆ·ç«¯ | HttpClient |
| æœåŠ¡ç«¯ | HttpServer |

å½“ç„¶ï¼Œä¸€èˆ¬ Web ç¨‹åºé‡‡å– Servlet è§„èŒƒï¼Œæ•…å¯åŸºäº Servlet å®¹å™¨æ¡†æ¶ (å¦‚ *Tomcatã€Jetty* ç­‰) è¿›è¡Œå¼€å‘ã€‚

## ğŸ–Šæ•°æ®åº“

### JDBC

JDBC å…¨ç§° Java Database Connectivityï¼Œæ˜¯æä¾›ç»™ Java ç¨‹åº çš„æ•°æ®åº“é©±åŠ¨ï¼Œå¯é‡‡ç”¨å…¶ä»–è¯­è¨€ç¼–å†™ã€‚åŸºäº JDBCï¼Œå¯ä»¥è®© Java ç¨‹åºå¼€å‘ ä¸éœ€è¦è¿‡åˆ†å…³æ³¨æ•°æ®åº“åº•å±‚ç»†èŠ‚ã€‚

æ¥ä¸‹æ¥ä»¥ *MySQL* çš„ JDBC é©±åŠ¨ä¸ºä¾‹ï¼Œåˆ†æ JDBC å¯¹æ•°æ®åº“è¿æ¥çš„ç®¡ç†ã€‚

#### åˆå§‹åŒ–

å½“ç±»è¢«åŠ è½½ (å¦‚é€šè¿‡ *åå°„æœºåˆ¶* )æ—¶ï¼Œé©±åŠ¨ç±»çš„é™æ€æ–¹æ³•å°†è¢«è°ƒç”¨ä»¥å®Œæˆé©±åŠ¨çš„åˆå§‹åŒ–ã€‚

å½“ç„¶ï¼Œåœ¨ç›®å‰æ–°ç‰ˆ JDBC ä¸­ï¼Œé€šè¿‡ SPI æœºåˆ¶å®Œæˆäº†åˆå§‹åŒ–ï¼Œå·²ç»ä¸éœ€è¦é€šè¿‡åå°„æ‰‹åŠ¨åˆå§‹åŒ–äº†ã€‚

```java
public class TestJDBC {
    @Test
    public void test() {
        String JDBC_Driver = "com.mysql.jdbc.Driver";
        try {
            Class.forName(JDBC_Driver);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

```java
public class Driver extends com.mysql.cj.jdbc.Driver {
    public Driver() throws SQLException {
    }

    static {
        System.err.println("Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.");
    }
}

public class Driver extends NonRegisteringDriver implements java.sql.Driver {
    public Driver() throws SQLException {
    }

    static {
        try {
            DriverManager.registerDriver(new Driver());
        } catch (SQLException var1) {
            throw new RuntimeException("Can't register driver!");
        }
    }
}
```

```java
/**
 * Registers the given driver with the {@code DriverManager}.
 * A newly-loaded driver class should call
 * the method {@code registerDriver} to make itself
 * known to the {@code DriverManager}. If the driver is currently
 * registered, no action is taken.
 *
 * @param driver the new JDBC Driver that is to be registered with the
 *               {@code DriverManager}
 * @throws SQLException if a database access error occurs
 * @throws NullPointerException if {@code driver} is null
 */
public static void registerDriver(java.sql.Driver driver)
    throws SQLException {

    registerDriver(driver, null);
}

/**
 * Registers the given driver with the {@code DriverManager}.
 * A newly-loaded driver class should call
 * the method {@code registerDriver} to make itself
 * known to the {@code DriverManager}. If the driver is currently
 * registered, no action is taken.
 *
 * @param driver the new JDBC Driver that is to be registered with the
 *               {@code DriverManager}
 * @param da     the {@code DriverAction} implementation to be used when
 *               {@code DriverManager#deregisterDriver} is called
 * @throws SQLException if a database access error occurs
 * @throws NullPointerException if {@code driver} is null
 * @since 1.8
 */
public static void registerDriver(java.sql.Driver driver,
        DriverAction da)
    throws SQLException {

    /* Register the driver if it has not already been added to our list */
    if (driver != null) {
        registeredDrivers.addIfAbsent(new DriverInfo(driver, da));
    } else {
        // This is for compatibility with the original DriverManager
        throw new NullPointerException();
    }

    println("registerDriver: " + driver);

}
```

ä»ä¸Šè¿°ä»£ç å¯ä»¥çœ‹å‡ºï¼Œåˆå§‹åŒ–çš„è¿‡ç¨‹æ³¨å†Œäº†ç›¸å…³é©±åŠ¨ï¼Œå¹¶ä¿å­˜åœ¨ **DriverManager** ä¸­ï¼ŒDriverManager å®šä¹‰äº†ä¸€ç³»åˆ—é™æ€æ–¹æ³•ï¼Œç”¨äº**ç®¡ç†è¿æ¥**ã€‚

#### DriverManager

```java
/**
 * The basic service for managing a set of JDBC drivers.
 * <p>
 * <strong>NOTE:</strong> The {@link javax.sql.DataSource} interface, provides
 * another way to connect to a data source.
 * The use of a {@code DataSource} object is the preferred means of
 * connecting to a data source.
 * <P>
 * As part of its initialization, the {@code DriverManager} class will
 * attempt to load available JDBC drivers by using:
 * <ul>
 * <li>The {@code jdbc.drivers} system property which contains a
 * colon separated list of fully qualified class names of JDBC drivers. Each
 * driver is loaded using the {@linkplain ClassLoader#getSystemClassLoader
 * system class loader}:
 * <ul>
 * <li>{@code jdbc.drivers=foo.bah.Driver:wombat.sql.Driver:bad.taste.ourDriver}
 * </ul>
 *
 * <li>Service providers of the {@code java.sql.Driver} class, that are loaded
 * via the {@linkplain ServiceLoader#load service-provider loading} mechanism.
 *</ul>
 *
 * @implNote
 * {@code DriverManager} initialization is done lazily and looks up service
 * providers using the thread context class loader.  The drivers loaded and
 * available to an application will depend on the thread context class loader of
 * the thread that triggers driver initialization by {@code DriverManager}.
 *
 * <P>When the method {@code getConnection} is called,
 * the {@code DriverManager} will attempt to
 * locate a suitable driver from amongst those loaded at
 * initialization and those loaded explicitly using the same class loader
 * as the current application.
 *
 * @see Driver
 * @see Connection
 * @since 1.1
 */
public class DriverManager {


    // List of registered JDBC drivers
    private static final CopyOnWriteArrayList<DriverInfo> registeredDrivers = new CopyOnWriteArrayList<>();
    private static volatile int loginTimeout = 0;
    private static volatile java.io.PrintWriter logWriter = null;
    private static volatile java.io.PrintStream logStream = null;
    // Used in println() to synchronize logWriter
    private static final Object logSync = new Object();
    // Used in ensureDriversInitialized() to synchronize driversInitialized
    private static final Object lockForInitDrivers = new Object();
    private static volatile boolean driversInitialized;
    private static final String JDBC_DRIVERS_PROPERTY = "jdbc.drivers";

    /* Prevent the DriverManager class from being instantiated. */
    private DriverManager(){}
    
    /**
     * The {@code SQLPermission} constant that allows the
     * setting of the logging stream.
     * @since 1.3
     */
    static final SQLPermission SET_LOG_PERMISSION =
        new SQLPermission("setLog");

    /**
     * The {@code SQLPermission} constant that allows the
     * un-register a registered JDBC driver.
     * @since 1.8
     */
    static final SQLPermission DEREGISTER_DRIVER_PERMISSION =
        new SQLPermission("deregisterDriver");
```

é€šè¿‡ DriverManager å¯ä»¥è·å– Connectionï¼Œæ ¸å¿ƒä»£ç å¦‚ä¸‹ï¼š

```java
//  Worker method called by the public getConnection() methods.
@CallerSensitiveAdapter
private static Connection getConnection(
    String url, java.util.Properties info, Class<?> caller) throws SQLException {
    /*
     * When callerCl is null, we should check the application's
     * (which is invoking this class indirectly)
     * classloader, so that the JDBC driver class outside rt.jar
     * can be loaded from here.
     */
    ClassLoader callerCL = caller != null ? caller.getClassLoader() : null;
    if (callerCL == null || callerCL == ClassLoader.getPlatformClassLoader()) {
        callerCL = Thread.currentThread().getContextClassLoader();
    }

    if (url == null) {
        throw new SQLException("The url cannot be null", "08001");
    }

    println("DriverManager.getConnection(\"" + url + "\")");

    ensureDriversInitialized();

    // Walk through the loaded registeredDrivers attempting to make a connection.
    // Remember the first exception that gets raised so we can reraise it.
    SQLException reason = null;

    for (DriverInfo aDriver : registeredDrivers) {
        // If the caller does not have permission to load the driver then
        // skip it.
        if (isDriverAllowed(aDriver.driver, callerCL)) {
            try {
                println("    trying " + aDriver.driver.getClass().getName());
                Connection con = aDriver.driver.connect(url, info);
                if (con != null) {
                    // Success!
                    println("getConnection returning " + aDriver.driver.getClass().getName());
                    return (con);
                }
            } catch (SQLException ex) {
                if (reason == null) {
                    reason = ex;
                }
            }

        } else {
            println("    skipping: " + aDriver.driver.getClass().getName());
        }

    }

    // if we got here nobody could connect.
    if (reason != null)    {
        println("getConnection failed: " + reason);
        throw reason;
    }

    println("getConnection: no suitable driver found for "+ url);
    throw new SQLException("No suitable driver found for "+ url, "08001");
}
```

å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä¸Šè¿°ä»£ç åˆ©ç”¨äº†åå°„æœºåˆ¶åŒ¹é…åˆé€‚çš„æ•°æ®åº“é©±åŠ¨ï¼ŒåŒæ—¶ä½¿ç”¨äº†çº¿ç¨‹ä¸Šä¸‹æ–‡ ClassLoaderï¼Œè¿™æ˜¯å®ç° **SPI(Service Provider Interface)** æœºåˆ¶çš„é‡è¦æ‰‹æ®µï¼Œè¿™æ˜¯å› ä¸º æ•°æ®åº“å¼•å¯¼ç±» (å¦‚ **Connection**) æ˜¯å®šä¹‰åœ¨æ ‡å‡†åº“çš„ï¼Œè€Œæ¥å£çš„å®ç°ç”±ç¬¬ä¸‰æ–¹å®ç°ï¼Œå¦‚æœä¸ä½¿ç”¨çº¿ç¨‹ä¸Šä¸‹æ–‡åŠ è½½å™¨ï¼Œæ ‡å‡†åº“å¼€å‘è€…æ— æ³•ç¡®å®šç¬¬ä¸‰æ–¹å®ç°è€…ä½¿ç”¨ä»€ä¹ˆç±»åŠ è½½å™¨ï¼Œè€Œé¢å‘å¯¹è±¡è®¾è®¡å¯èƒ½æ„å‘³ç€æ ‡å‡†åº“å¼€å‘è€…å®šä¹‰çš„æ¨¡æ¿æ–¹æ³•ä¸­éœ€è¦å¼•ç”¨å®ç°ç±»çš„å®ä¾‹å¯¹è±¡ï¼Œæ­¤æ—¶å¦‚æœæ ‡å‡†åº“ä»£ç ä¸ç¬¬ä¸‰æ–¹åº“ç›¸å…³ä»£ç ä½¿ç”¨ç±»åŠ è½½å™¨ä¸åŒå°†ä¼šäº§ç”Ÿé—®é¢˜ã€‚

æ‰€ä»¥ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡åŠ è½½å™¨å°±æ˜¯ä¸ºäº†ä½¿ç”¨åŒä¸€ä¸ªåŠ è½½å™¨æ¥åŠ è½½ä¸¤è€…ã€‚ç¬¬ä¸‰æ–¹å®ç°è€…å¯ä»¥å°†éœ€è¦ä½¿ç”¨çš„åŠ è½½å™¨ä¿å­˜åˆ°çº¿ç¨‹ä¸Šä¸‹æ–‡åŠ è½½å™¨ï¼Œè®©æ ‡å‡†åº“å¼€å‘è€…ä½¿ç”¨è¯¥åŠ è½½å™¨æ¥åŠ è½½å¼•å¯¼ç±»ã€‚

#### æ•°æ®åº“æ“ä½œ

è·å–äº†ä¸€ä¸ª

> Connection

å¯¹è±¡åï¼Œä¾¿å¯åˆ©ç”¨è¯¥æ¥å£æ–¹æ³•æ“ä½œæ•°æ®åº“äº†ã€‚

### äº‹åŠ¡

### Sql ä¼˜åŒ–